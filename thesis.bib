% This file was created with Citavi 6.14.0.281

@incollection{Aggarwal.,
 author = {Aggarwal, Charu C. and Hinneburg, Alexander and Keim, Daniel A.},
 title = {On the Surprising Behavior of Distance Metrics in High Dimensional Space},
 pages = {420--434},
 bookpagination = {page},
 volume = {1973},
 abstract = {},
 doi = {10.1007/3-540-44503-X_27},
 origdate = {2001},
 file = {On the Surprising Behavior of Distance Metrics in High D:Attachments/On the Surprising Behavior of Distance Metrics in High D.pdf:application/pdf}
}


@incollection{Rifai.2011,
 author = {Rifai, Salah and Vincent, Pascal and Muller, Xavier and Glorot, Xavier and Bengio, Yoshua},
 title = {Contractive Auto-Encoders: Explicit Invariance during Feature Extraction},
 pages = {833--840},
 bookpagination = {page},
 isbn = {9781450306195},
 booktitle = {Proceedings of the Twenty-Eighth International Conference on Machine Learning},
 year = {2011},
 abstract = {We present in this paper a novel approach for training deterministic auto-encoders. We show that by adding a well chosen penalty term to the classical reconstruction cost function, we can achieve results that equal or surpass those attained by other regularized auto-encoders as well as denoising auto-encoders on a range of datasets. This penalty term corresponds to the Frobenius norm of the Jacobian matrix of the encoder activations with respect to the input. We show that this penalty term results in a localized space contraction which in turn yields robust features on the activation layer. Furthermore, we show how this penalty term is related to both regularized auto-encoders and denoising auto-encoders and how it can be seen as a link between deterministic and non-deterministic auto-encoders. We find empirically that this penalty helps to carve a representation that better captures the local directions of variation dictated by the data, corresponding to a lower-dimensional non-linear manifold, while being more invariant to the vast majority of directions orthogonal to the manifold. Finally, we show that by using the learned features to initialize a MLP, we achieve state of the art classification error on a range of datasets, surpassing other methods of pretraining.},
 location = {Madison},
 booksubtitle = {Bellevue, Washington, USA, June 28 - July 2,2011}
}


@article{Bac.2021,
 author = {Bac, Jonathan and Mirkes, Evgeny M. and Gorban, Alexander N. and Tyukin, Ivan and Zinovyev, Andrei},
 year = {2021},
 title = {Scikit-Dimension: A Python Package for Intrinsic Dimension Estimation},
 volume = {23},
 journaltitle = {Entropy (Basel, Switzerland)},
 language = {eng},
 doi = {10.3390/e23101368},
 number = {10},
 abstract = {Dealing with uncertainty in applications of machine learning to real-life data critically depends on the knowledge of intrinsic dimensionality (ID). A number of methods have been suggested for the purpose of estimating ID, but no standard package to easily apply them one by one or all at once has been implemented in Python. This technical note introduces scikit-dimension, an open-source Python package for intrinsic dimension estimation. The scikit-dimension package provides a uniform implementation of most of the known ID estimators based on the scikit-learn application programming interface to evaluate the global and local intrinsic dimension, as well as generators of synthetic toy and benchmark datasets widespread in the literature. The package is developed with tools assessing the code quality, coverage, unit testing and continuous integration. We briefly describe the package and demonstrate its use in a large-scale (more than 500 datasets) benchmarking of methods for ID estimation for real-life and synthetic data.},
 file = {Scikit-Dimension A Python Package for Intrinsic Dimensio:Attachments/Scikit-Dimension A Python Package for Intrinsic Dimensio.pdf:application/pdf},
 note = {Journal Article},
 eprint = {34682092}
}


@misc{Bank.2020,
 author = {Bank, Dor and Koenigstein, Noam and Giryes, Raja},
 year = {2020},
 title = {Autoencoders},
 url = {https://arxiv.org/pdf/2003.05991},
 keywords = {Computer Vision and Pattern Recognition (cs.CV);Machine Learning (cs.LG);Machine Learning (stat.ML)},
 abstract = {An autoencoder is a specific type of a neural network, which is mainly designed to encode the input into a compressed and meaningful representation, and then decode it back such that the reconstructed input is similar as possible to the original one. This chapter surveys the different types of autoencoders that are mainly used today. It also describes various applications and use-cases of autoencoders.},
 file = {Autoencoders:Attachments/Autoencoders.pdf:application/pdf}
}


@article{Bennett.1969,
 author = {Bennett, R.},
 year = {1969},
 title = {The intrinsic dimensionality of signal collections},
 pages = {517--525},
 pagination = {page},
 volume = {15},
 issn = {0018-9448},
 journaltitle = {IEEE Transactions on Information Theory},
 shortjournal = {IEEE Trans. Inform. Theory},
 doi = {10.1109/TIT.1969.1054365},
 number = {5},
 abstract = {}
}


@book{Bishop.2006,
 author = {Bishop, Christopher M.},
 year = {2006},
 title = {Pattern recognition and machine learning},
 url = {https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf},
 keywords = {Artificial intelligence;Artificial Intelligence (incl. Robotics);Computer Science;Image Processing and Computer Vision;Machine learning;Maschinelles Lernen;Mustererkennung;Optical data processing;Pattern Recognition;Statistics;Statistics for Engineering, Physics, Computer Science, Chemistry {\&} Geosciences},
 publisher = {{Springer Science+Business Media LLC}},
 language = {eng},
 location = {New York, NY},
 series = {Information Science and Statistics},
 abstract = {Probability Distributions -- Linear Models for Regression -- Linear Models for Classification -- Neural Networks -- Kernel Methods -- Sparse Kernel Machines -- Graphical Models -- Mixture Models and EM -- Approximate Inference -- Sampling Methods -- Continuous Latent Variables -- Sequential Data -- Combining Models.



The dramatic growth in practical applications for machine learning over the last ten years has been accompanied by many important developments in the underlying algorithms and techniques. For example, Bayesian methods have grown from a specialist niche to become mainstream, while graphical models have emerged as a general framework for describing and applying probabilistic techniques. The practical applicability of Bayesian methods has been greatly enhanced by the development of a range of approximate inference algorithms such as variational Bayes and expectation propagation, while new models based on kernels have had a significant impact on both algorithms and applications. This completely new textbook reflects these recent developments while providing a comprehensive introduction to the fields of pattern recognition and machine learning. It is aimed at advanced undergraduates or first-year PhD students, as well as researchers and practitioners. No previous knowledge of pattern recognition or machine learning concepts is assumed. Familiarity with multivariate calculus and basic linear algebra is required, and some experience in the use of probabilities would be helpful though not essential as the book includes a self-contained introduction to basic probability theory. The book is suitable for courses on machine learning, statistics, computer science, signal processing, computer vision, data mining, and bioinformatics. Extensive support is provided for course instructors, including more than 400 exercises, graded according to difficulty. Example solutions for a subset of the exercises are available from the book web site, while solutions for the remainder can be obtained by instructors from the publisher. The book is supported by a great deal of additional material, and the reader is encouraged to visit the book web site for the latest information. Christopher M. Bishop is Deputy Director of Microsoft Research Cambridge, and holds a Chair in Computer Science at the University of Edinburgh. He is a Fellow of Darwin College Cambridge, a Fellow of the Royal Academy of Engineering, and a Fellow of the Royal Society of Edinburgh. His previous textbook {\textquotedbl}Neural Networks for Pattern Recognition{\textquotedbl} has been widely adopted. Coming soon: *For students, worked solutions to a subset of exercises available on a public web site (for exercises marked {\textquotedbl}www{\textquotedbl} in the text) *For instructors, worked solutions to remaining exercises from the Springer web site *Lecture slides to accompany each chapter *Data sets available for download.},
 pagetotal = {758},
 note = {Bishop, Christopher M. (VerfasserIn)}
}


@article{Blum.1997,
 author = {Blum, Avrim L. and Langley, Pat},
 year = {1997},
 title = {Selection of relevant features and examples in machine learning},
 url = {https://www.sciencedirect.com/science/article/pii/S0004370297000635},
 keywords = {Machine learning;Relevant examples;Relevant features},
 pages = {245--271},
 pagination = {page},
 volume = {97},
 issn = {00043702},
 journaltitle = {Artificial Intelligence},
 doi = {10.1016/S0004-3702(97)00063-5},
 number = {1-2},
 abstract = {In this survey, we review work in machine learning on methods for handling data sets containing large amounts of irrelevant information. We focus on two key issues: the problem of selecting relevant features, and the problem of selecting relevant examples. We describe the advances that have been made on these topics in both empirical and theoretical work in machine learning, and we present a general framework that we use to compare different methods. We close with some challenges for future work in this area.},
 file = {Selection of relevant features and examples in machine l:Attachments/Selection of relevant features and examples in machine l.pdf:application/pdf},
 note = {PII:  S0004370297000635}
}


@article{Boser.1992,
 author = {Boser, Bernhard E. and Guyon, Isabelle M. and Vapnik, Vladimir N.},
 year = {1992},
 title = {A training algorithm for optimal margin classifiers},
 pages = {144--152},
 pagination = {page},
 journaltitle = {Proceedings of the fifth annual workshop on Computational learning theory},
 doi = {10.1145/130385.130401},
 abstract = {},
 file = {A training algorithm for optimal margin classifiers:Attachments/A training algorithm for optimal margin classifiers.pdf:application/pdf}
}


@incollection{Burges.2009,
 author = {Burges, Christopher J.C.},
 title = {Geometric Methods for Feature Extraction and Dimensional Reduction - A Guided Tour},
 url = {https://link.springer.com/chapter/10.1007/978-0-387-09823-4_4},
 pages = {53--82},
 bookpagination = {page},
 publisher = {{Springer, Boston, MA}},
 editor = {Maimon, Oded and Rokach, Lior},
 booktitle = {Data Mining and Knowledge Discovery Handbook},
 year = {2009},
 abstract = {We give a tutorial overview of several geometric methods for feature extractionand dimensional reduction. We divide the methods into projective methods and methods thatmodel the manifold on which the data lies. For projective methods, we review projectionpursuit,...},
 doi = {10.1007/978-0-387-09823-4_4},
 language = {en}
}


@article{Camastra.2002,
 author = {Camastra, F. and Vinciarelli, A.},
 year = {2002},
 title = {Estimating the intrinsic dimension of data with a fractal-based method},
 pages = {1404--1407},
 pagination = {page},
 volume = {24},
 issn = {01628828},
 journaltitle = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
 shortjournal = {IEEE Trans. Pattern Anal. Machine Intell.},
 doi = {10.1109/TPAMI.2002.1039212},
 number = {10},
 abstract = {},
 file = {Estimating the intrinsic dimension of data with a fracta:Attachments/Estimating the intrinsic dimension of data with a fracta.pdf:application/pdf}
}


@article{Campadelli.2015,
 author = {Campadelli, P. and Casiraghi, E. and Ceruti, C. and Rozza, A.},
 year = {2015},
 title = {Intrinsic Dimension Estimation: Relevant Techniques and a Benchmark Framework},
 pages = {1--21},
 pagination = {page},
 volume = {2015},
 issn = {1024-123X},
 journaltitle = {Mathematical Problems in Engineering},
 doi = {10.1155/2015/759567},
 abstract = {},
 file = {Intrinsic Dimension Estimation Relevant Techniques and a:Attachments/Intrinsic Dimension Estimation Relevant Techniques and a.pdf:application/pdf},
 note = {PII:  759567}
}


@article{CarreiraPerpinan.1997,
 author = {Carreira-Perpi{\~n}{\'a}n, Miguel {\'A}.},
 year = {1997},
 title = {A Review of Dimension Reduction Techniques},
 url = {http://www.pca.narod.ru/DimensionReductionBrifReview.pdf},
 abstract = {A classification of dimension reduction problems is proposed.},
 pagetotal = {69},
 file = {A Review of Dimension Reduction Techniques:Attachments/A Review of Dimension Reduction Techniques.pdf:application/pdf}
}


@book{Casella.2008,
 author = {Casella, G. and Fienberg, S. and Olkin, I. and Izenman, Alan J.},
 year = {2008},
 title = {Modern Multivariate Statistical Techniques},
 publisher = {{Springer New York}},
 isbn = {978-0-387-78188-4},
 location = {New York, NY},
 abstract = {},
 doi = {10.1007/978-0-387-78189-1},
 file = {Modern Multivariate Statistical Techniques:Attachments/Modern Multivariate Statistical Techniques.pdf:application/pdf}
}


@article{Cayton.2005,
 author = {Cayton, Lawrence},
 year = {2005},
 title = {Algorithms for manifold learning},
 url = {https://www.lcayton.com/resexam.pdf},
 abstract = {},
 pagetotal = {17},
 file = {resexampdf:Attachments/resexampdf.pdf:application/pdf}
}


@article{Charte.2018,
 author = {Charte, David and Charte, Francisco and Garc{\'i}a, Salvador and {Del Jesus}, Mar{\'i}a J. and Herrera, Francisco},
 year = {2018},
 title = {A practical tutorial on autoencoders for nonlinear feature fusion: Taxonomy, models, software and guidelines},
 url = {https://arxiv.org/pdf/1801.01586},
 keywords = {Machine Learning (cs.LG);Neural and Evolutionary Computing (cs.NE)},
 pages = {78--96},
 pagination = {page},
 volume = {44},
 issn = {15662535},
 journaltitle = {Information Fusion},
 doi = {10.1016/j.inffus.2017.12.007},
 abstract = {Many of the existing machine learning algorithms, both supervised and unsupervised, depend on the quality of the input characteristics to generate a good model. The amount of these variables is also important, since performance tends to decline as the input dimensionality increases, hence the interest in using feature fusion techniques, able to produce feature sets that are more compact and higher level. A plethora of procedures to fuse original variables for producing new ones has been developed in the past decades. The most basic ones use linear combinations of the original variables, such as PCA (Principal Component Analysis) and LDA (Linear Discriminant Analysis), while others find manifold embeddings of lower dimensionality based on non-linear combinations, such as Isomap or LLE (Linear Locally Embedding) techniques.

More recently, autoencoders (AEs) have emerged as an alternative to manifold learning for conducting nonlinear feature fusion. Dozens of AE models have been proposed lately, each with its own specific traits. Although many of them can be used to generate reduced feature sets through the fusion of the original ones, there also AEs designed with other applications in mind.

The goal of this paper is to provide the reader with a broad view of what an AE is, how they are used for feature fusion, a taxonomy gathering a broad range of models, and how they relate to other classical techniques. In addition, a set of didactic guidelines on how to choose the proper AE for a given task is supplied, together with a discussion of the software tools available. Finally, two case studies illustrate the usage of AEs with datasets of handwritten digits and breast cancer.},
 file = {A practical tutorial on autoencoders for nonlinear featu:Attachments/A practical tutorial on autoencoders for nonlinear featu.pdf:application/pdf},
 note = {PII:  S1566253517307844}
}


@proceedings{Dorffner.2001,
 year = {2001},
 title = {Artificial Neural Networks --- ICANN 2001},
 publisher = {{Springer Berlin Heidelberg}},
 isbn = {978-3-540-44668-2},
 editor = {Dorffner, Georg and Bischof, Horst and Hornik, Kurt},
 abstract = {},
 location = {Berlin, Heidelberg}
}


@proceedings{Dzeroski.2005,
 year = {2005},
 title = {Proceedings of the 22nd international conference on Machine learning - ICML '05},
 publisher = {{ACM Press}},
 isbn = {1595931805},
 editor = {Dzeroski, Saso},
 abstract = {},
 doi = {10.1145/1102351},
 venue = {Bonn, Germany},
 location = {New York, New York, USA},
 eventdate = {8/7/2005 - 8/11/2005},
 eventtitle = {the 22nd international conference}
}


@book{Garzon.2022,
 author = {Garzon, Max and Yang, Ching-Chi and Venugopal, Deepak and Kumar, Nirman and Jana, Kalidas and Deng, Lih-Yuan},
 year = {2022},
 title = {Dimensionality Reduction in Data Science},
 publisher = {{Springer International Publishing}},
 isbn = {978-3-031-05370-2},
 location = {Cham},
 abstract = {},
 doi = {10.1007/978-3-031-05371-9},
 file = {Dimensionality Reduction in Data Science:Attachments/Dimensionality Reduction in Data Science.pdf:application/pdf}
}


@book{Goodfellow.2016,
 author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
 year = {2016},
 title = {Deep learning},
 publisher = {{The MIT Press}},
 isbn = {9780262035613},
 language = {eng},
 location = {Cambridge, Massachusetts and London, England},
 series = {Adaptive computation and machine learning},
 abstract = {},
 pagetotal = {xxii, 775},
 note = {Goodfellow, Ian (VerfasserIn)

Bengio, Yoshua (VerfasserIn)

Courville, Aaron (VerfasserIn)},
 organization = {{MIT Press}}
}


@article{Gracia.2014,
 author = {Gracia, Antonio and Gonz{\'a}lez, Santiago and Robles, Victor and Menasalvas, Ernestina},
 year = {2014},
 title = {A methodology to compare Dimensionality Reduction algorithms in terms of loss of quality},
 pages = {1--27},
 pagination = {page},
 volume = {270},
 issn = {00200255},
 journaltitle = {Information Sciences},
 doi = {10.1016/j.ins.2014.02.068},
 abstract = {},
 file = {A methodology to compare Dimensionality Reduction algori:Attachments/A methodology to compare Dimensionality Reduction algori.pdf:application/pdf},
 note = {PII:  S0020025514001741}
}


@collection{Guyon.2006,
 year = {2006},
 title = {Feature Extraction: Foundations and Applications},
 publisher = {{Springer Berlin Heidelberg}},
 isbn = {978-3-540-35488-8},
 editor = {Guyon, Isabelle and Nikravesh, Masoud and Gunn, Steve and Zadeh, Lotfi A.},
 abstract = {},
 location = {Berlin, Heidelberg}
}


@incollection{Guyon.2006b,
 author = {Guyon, Isabelle and Elisseeff, Andr{\'e}},
 title = {An Introduction to Feature Extraction},
 pages = {1--25},
 bookpagination = {page},
 publisher = {{Springer Berlin Heidelberg}},
 isbn = {978-3-540-35488-8},
 editor = {Guyon, Isabelle and Nikravesh, Masoud and Gunn, Steve and Zadeh, Lotfi A.},
 booktitle = {Feature Extraction: Foundations and Applications},
 year = {2006},
 abstract = {This chapter introduces the reader to the various aspects of feature extraction covered in this book. Section 1 reviews definitions and notations and proposes a unified view of the feature extraction problem. Section 2 is an overview of the methods and results presented in the book, emphasizing novel contributions. Section 3 provides the reader with an entry point in the field of feature extraction by showing small revealing examples and describing simple but effective algorithms. Finally, Section 4 introduces a more theoretical formalism and points to directions of research and open problems.},
 doi = {10.1007/978-3-540-35488-8_1},
 location = {Berlin, Heidelberg}
}


@inproceedings{Hein.2005b,
 author = {Hein, Matthias and Audibert, Jean-Yves},
 title = {Intrinsic dimensionality estimation of submanifolds in R d},
 pages = {289--296},
 bookpagination = {page},
 publisher = {{ACM Press}},
 isbn = {1595931805},
 editor = {Dzeroski, Saso},
 booktitle = {Proceedings of the 22nd international conference on Machine learning - ICML '05},
 year = {2005},
 abstract = {},
 doi = {10.1145/1102351.1102388},
 location = {New York, New York, USA},
 eventtitle = {the 22nd international conference},
 venue = {Bonn, Germany},
 eventdate = {8/7/2005 - 8/11/2005}
}


@article{Hotelling.1933,
 author = {Hotelling, H.},
 year = {1933},
 title = {Analysis of a complex of statistical variables into principal components},
 pages = {417--441},
 pagination = {page},
 volume = {24},
 issn = {0022-0663},
 journaltitle = {Journal of Educational Psychology},
 doi = {10.1037/h0071325},
 number = {6},
 abstract = {}
}


@collection{InternationalMachineLearningSociety.2011,
 year = {2011},
 title = {Proceedings of the Twenty-Eighth International Conference on Machine Learning},
 isbn = {9781450306195},
 abstract = {},
 pagetotal = {1216},
 language = {eng},
 subtitle = {Bellevue, Washington, USA, June 28 - July 2,2011},
 location = {Madison},
 organization = {{International Machine Learning Society}}
}


@book{Jolliffe.2002,
 author = {Jolliffe, I. T.},
 year = {2002},
 title = {Principal Component Analysis},
 url = {https://ebookcentral.proquest.com/lib/kxp/detail.action?docID=6485318},
 keywords = {Electronic books;Principal components analysis},
 edition = {2nd ed.},
 publisher = {{Springer New York}},
 isbn = {9780387224404},
 language = {eng},
 location = {New York, NY},
 series = {Springer Series in Statistics Ser},
 abstract = {},
 pagetotal = {513},
 note = {Jolliffe, I. T. (VerfasserIn)}
}


@article{Kohonen.1990,
 author = {Kohonen, T.},
 year = {1990},
 title = {The self-organizing map},
 pages = {1464--1480},
 pagination = {page},
 volume = {78},
 issn = {00189219},
 journaltitle = {Proceedings of the IEEE},
 shortjournal = {Proc. IEEE},
 doi = {10.1109/5.58325},
 number = {9},
 abstract = {}
}


@article{Kramer.1991,
 author = {Kramer, Mark A.},
 year = {1991},
 title = {Nonlinear principal component analysis using autoassociative neural networks},
 url = {https://aiche.onlinelibrary.wiley.com/doi/10.1002/aic.690370209},
 pages = {233--243},
 pagination = {page},
 volume = {37},
 issn = {0001-1541},
 journaltitle = {AIChE Journal},
 shortjournal = {AIChE J.},
 language = {en},
 doi = {10.1002/aic.690370209},
 number = {2},
 abstract = {Abstract Nonlinear principal component analysis is a novel technique for multivariate data analysis, similar to the well-known method of principal component analysis. NLPCA, like PCA, is used to id...}
}


@proceedings{L.Saul.2004,
 year = {2004},
 title = {Advances in Neural Information Processing Systems},
 publisher = {{MIT Press}},
 editor = {{L. Saul} and {Y. Weiss} and {L. Bottou}},
 abstract = {}
}


@misc{Ladjal.2019,
 author = {Ladjal, Sa{\"i}d and Newson, Alasdair and Pham, Chi-Hieu},
 year = {2019},
 title = {A PCA-like Autoencoder},
 url = {https://arxiv.org/pdf/1904.01277},
 keywords = {Computer Vision and Pattern Recognition (cs.CV);Machine Learning (cs.LG)},
 abstract = {An autoencoder is a neural network which data projects to and from a lower dimensional latent space, where this data is easier to understand and model. The autoencoder consists of two sub-networks, the encoder and the decoder, which carry out these transformations. The neural network is trained such that the output is as close to the input as possible, the data having gone through an information bottleneck : the latent space. This tool bears significant ressemblance to Principal Component Analysis (PCA), with two main differences. Firstly, the autoencoder is a non-linear transformation, contrary to PCA, which makes the autoencoder more flexible and powerful. Secondly, the axes found by a PCA are orthogonal, and are ordered in terms of the amount of variability which the data presents along these axes. This makes the interpretability of the PCA much greater than that of the autoencoder, which does not have these attributes. Ideally, then, we would like an autoencoder whose latent space consists of independent components, ordered by decreasing importance to the data. In this paper, we propose an algorithm to create such a network. We create an iterative algorithm which progressively increases the size of the latent space, learning a new dimension at each step. Secondly, we propose a covariance loss term to add to the standard autoencoder loss function, as well as a normalisation layer just before the latent space, which encourages the latent space components to be statistically independent. We demonstrate the results of this autoencoder on simple geometric shapes, and find that the algorithm indeed finds a meaningful representation in the latent space. This means that subsequent interpolation in the latent space has meaning with respect to the geometric properties of the images.},
 file = {A PCA-like Autoencoder:Attachments/A PCA-like Autoencoder.pdf:application/pdf}
}


@book{Lee.2007,
 author = {Lee, John A. and Verleysen, Michel},
 year = {2007},
 title = {Nonlinear Dimensionality Reduction},
 publisher = {{Springer New York}},
 isbn = {978-0-387-39350-6},
 location = {New York, NY},
 abstract = {},
 doi = {10.1007/978-0-387-39351-3}
}


@article{Lee.2009,
 author = {Lee, John A. and Verleysen, Michel},
 year = {2009},
 title = {Quality assessment of dimensionality reduction: Rank-based criteria},
 pages = {1431--1443},
 pagination = {page},
 volume = {72},
 issn = {09252312},
 journaltitle = {Neurocomputing},
 doi = {10.1016/j.neucom.2008.12.017},
 number = {7-9},
 abstract = {},
 note = {PII:  S0925231209000101}
}


@book{Lespinats.2022,
 author = {Lespinats, Sylvain and Colange, Benoit and Dutykh, Denys},
 year = {2022},
 title = {Nonlinear dimensionality reduction techniques},
 publisher = {Springer},
 isbn = {9783030810252},
 subtitle = {A data structure preservation approach},
 language = {eng},
 location = {Cham},
 abstract = {},
 pagetotal = {247},
 doi = {10.1007/978-3-030-81026-9},
 note = {Lespinats, Sylvain (VerfasserIn)

Colange, Benoit (VerfasserIn)

Dutykh, Denys (VerfasserIn)},
 file = {Nonlinear dimensionality reduction techniques:Attachments/Nonlinear dimensionality reduction techniques.pdf:application/pdf}
}


@inproceedings{Levina.2004,
 author = {Levina, Elizaveta and Bickel, Peter},
 title = {Maximum Likelihood Estimation of Intrinsic Dimension},
 url = {https://proceedings.neurips.cc/paper/2004/file/74934548253bcab8490ebd74afed7031-Paper.pdf},
 volume = {17},
 publisher = {{MIT Press}},
 editor = {{L. Saul} and {Y. Weiss} and {L. Bottou}},
 booktitle = {Advances in Neural Information Processing Systems},
 year = {2004},
 abstract = {}
}


@collection{OdedMaimon.2009,
 year = {2009},
 title = {Data Mining and Knowledge Discovery Handbook},
 publisher = {{Springer, Boston, MA}},
 editor = {Maimon, Oded and Rokach, Lior},
 abstract = {},
 file = {Data Mining and Knowledge Discovery Handbook:Attachments/Data Mining and Knowledge Discovery Handbook.pdf:application/pdf}
}


@article{Pearson.1901,
 author = {Pearson, Karl},
 year = {1901},
 title = {On lines and planes of closest fit to systems of points in space},
 url = {http://www.stats.org.uk/pca/Pearson1901.pdf},
 journaltitle = {Philosophical Magazine},
 abstract = {},
 pagetotal = {14},
 file = {Microsoft Word - pearson1901doc:Attachments/Microsoft Word - pearson1901doc.pdf:application/pdf}
}


@misc{Plaut.2018,
 author = {Plaut, Elad},
 year = {2018},
 title = {From Principal Subspaces to Principal Components with Linear Autoencoders},
 url = {https://arxiv.org/pdf/1804.10253},
 keywords = {Machine Learning (cs.LG);Machine Learning (stat.ML)},
 abstract = {The autoencoder is an effective unsupervised learning model which is widely used in deep learning. It is well known that an autoencoder with a single fully-connected hidden layer, a linear activation function and a squared error cost function trains weights that span the same subspace as the one spanned by the principal component loading vectors, but that they are not identical to the loading vectors. In this paper, we show how to recover the loading vectors from the autoencoder weights.},
 file = {From Principal Subspaces to Principal Components with Li:Attachments/From Principal Subspaces to Principal Components with Li.pdf:application/pdf}
}


@article{Sarveniazi.2014,
 author = {Sarveniazi, Alireza},
 year = {2014},
 title = {An Actual Survey of Dimensionality Reduction},
 pages = {55--72},
 pagination = {page},
 volume = {04},
 issn = {2161-1211},
 journaltitle = {American Journal of Computational Mathematics},
 doi = {10.4236/ajcm.2014.42006},
 number = {02},
 abstract = {},
 file = {An Actual Survey of Dimensionality Reduction:Attachments/An Actual Survey of Dimensionality Reduction.pdf:application/pdf}
}


@misc{Scholkopf.1997,
 author = {Sch{\"o}lkopf, Bernhard and Smola, Alexander and M{\"u}ller, Klaus-Robert},
 year = {1997},
 title = {Kernel principal component analysis},
 abstract = {},
 doi = {10.1007/BFb0020217},
 file = {Kernel principal component analysis:Attachments/Kernel principal component analysis.pdf:application/pdf}
}


@book{ShaweTaylor.2011,
 author = {Shawe-Taylor, John and Cristianini, Nello},
 year = {2011},
 title = {Kernel Methods for Pattern Analysis},
 publisher = {{Cambridge University Press}},
 isbn = {9780521813976},
 abstract = {},
 doi = {10.1017/CBO9780511809682}
}


@article{vanderMaaten.2008,
 author = {{van der Maaten}, Laurens and Hinton, Goeffrey},
 year = {2008},
 title = {Visualizing Data using t-SNE},
 url = {https://lvdmaaten.github.io/publications/papers/JMLR_2008.pdf},
 abstract = {},
 pagetotal = {27},
 file = {JMLR2008pdf:Attachments/JMLR2008pdf.pdf:application/pdf}
}


@article{vanderMaaten.2009,
 author = {{van der Maaten}, Laurens and Postma, Eric and {van den Herik}, Jaap},
 year = {2009},
 title = {Dimensionality Reduction: A comparative review},
 url = {https://lvdmaaten.github.io/publications/papers/TR_Dimensionality_Reduction_Review_2009.pdf},
 abstract = {},
 pagetotal = {36},
 file = {TR_Dimensiereductie:Attachments/TR_Dimensiereductie.pdf:application/pdf}
}


@inproceedings{Venna.2001,
 author = {Venna, Jarkko and Kaski, Samuel},
 title = {Neighborhood Preservation in Nonlinear Projection Methods: An Experimental Study},
 pages = {485--491},
 bookpagination = {page},
 publisher = {{Springer Berlin Heidelberg}},
 isbn = {978-3-540-44668-2},
 editor = {Dorffner, Georg and Bischof, Horst and Hornik, Kurt},
 booktitle = {Artificial Neural Networks --- ICANN 2001},
 year = {2001},
 abstract = {},
 location = {Berlin, Heidelberg}
}


@article{Venna.2006,
 author = {Venna, Jarkko and Kaski, Samuel},
 year = {2006},
 title = {Local multidimensional scaling},
 keywords = {Algorithms;Artificial intelligence;Cluster Analysis;Computer Simulation;Humans;Models, Statistical;Pattern Recognition, Automated;Principal Component Analysis},
 pages = {889--899},
 pagination = {page},
 volume = {19},
 issn = {0893-6080},
 journaltitle = {Neural networks : the official journal of the International Neural Network Society},
 language = {eng},
 doi = {10.1016/j.neunet.2006.05.014},
 number = {6-7},
 abstract = {In a visualization task, every nonlinear projection method needs to make a compromise between trustworthiness and continuity. In a trustworthy projection the visualized proximities hold in the original data as well, whereas a continuous projection visualizes all proximities of the original data. We show experimentally that one of the multidimensional scaling methods, curvilinear components analysis, is good at maximizing trustworthiness. We then extend it to focus on local proximities both in the input and output space, and to explicitly make a user-tunable parameterized compromise between trustworthiness and continuity. The new method compares favorably to alternative nonlinear projection methods.},
 note = {Comparative Study

Journal Article

Research Support, Non-U.S. Gov't},
 eprint = {16787737}
}


@article{Verveer.1995,
 author = {Verveer, P. J. and Duin, R.P.W.},
 year = {1995},
 title = {An evaluation of intrinsic dimensionality estimators},
 pages = {81--86},
 pagination = {page},
 volume = {17},
 issn = {01628828},
 journaltitle = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
 shortjournal = {IEEE Trans. Pattern Anal. Machine Intell.},
 doi = {10.1109/34.368147},
 number = {1},
 abstract = {},
 file = {An evaluation of intrinsic dimensionality estimators:Attachments/An evaluation of intrinsic dimensionality estimators.pdf:application/pdf}
}


@article{Wolpert.1997,
 author = {Wolpert, D. H. and Macready, W. G.},
 year = {1997},
 title = {No free lunch theorems for optimization},
 pages = {67--82},
 pagination = {page},
 volume = {1},
 issn = {1089778X},
 journaltitle = {IEEE Transactions on Evolutionary Computation},
 shortjournal = {IEEE Trans. Evol. Computat.},
 doi = {10.1109/4235.585893},
 number = {1},
 abstract = {},
 file = {No free lunch theorems for optimization:Attachments/No free lunch theorems for optimization.pdf:application/pdf}
}


@article{Zhang.2021,
 author = {Zhang, Yinsheng and Shang, Qian and Zhang, Guoming},
 year = {2021},
 title = {pyDRMetrics - A Python toolkit for dimensionality reduction quality assessment},
 pages = {e06199},
 pagination = {page},
 volume = {7},
 issn = {2405-8440},
 journaltitle = {Heliyon},
 language = {eng},
 doi = {10.1016/j.heliyon.2021.e06199},
 number = {2},
 abstract = {High-dimensional data are pervasive in this bigdata era. To avoid the curse of the dimensionality problem, various dimensionality reduction (DR) algorithms have been proposed. To facilitate systematic DR quality comparison and assessment, this paper reviews related metrics and develops an open-source Python package pyDRMetrics. Supported metrics include reconstruction error, distance matrix, residual variance, ranking matrix, co-ranking matrix, trustworthiness, continuity, co-k-nearest neighbor size, LCMC (local continuity meta criterion), and rank-based local/global properties. pyDRMetrics provides a native Python class and a web-oriented API. A case study of mass spectra is conducted to demonstrate the package functions. A web GUI wrapper is also published to support user-friendly B/S applications.},
 file = {pyDRMetrics - A Python toolkit for dimensionality reduct:Attachments/pyDRMetrics - A Python toolkit for dimensionality reduct.pdf:application/pdf},
 note = {Journal Article

The authors declare no conflict of interest.},
 eprint = {33644472}
}


