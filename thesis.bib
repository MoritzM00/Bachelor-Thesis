% This file was created with Citavi 6.14.0.281

@misc{Bank.2020,
 author = {Bank, Dor and Koenigstein, Noam and Giryes, Raja},
 year = {2020},
 title = {Autoencoders},
 url = {https://arxiv.org/pdf/2003.05991},
 keywords = {Computer Vision and Pattern Recognition (cs.CV);Machine Learning (cs.LG);Machine Learning (stat.ML)},
 abstract = {An autoencoder is a specific type of a neural network, which is mainly designed to encode the input into a compressed and meaningful representation, and then decode it back such that the reconstructed input is similar as possible to the original one. This chapter surveys the different types of autoencoders that are mainly used today. It also describes various applications and use-cases of autoencoders.},
 file = {Autoencoders:Attachments/Autoencoders.pdf:application/pdf}
}


@book{Goodfellow.2016,
 author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
 year = {2016},
 title = {Deep learning},
 publisher = {{The MIT Press}},
 isbn = {9780262035613},
 language = {eng},
 location = {Cambridge, Massachusetts and London, England},
 series = {Adaptive computation and machine learning},
 abstract = {},
 pagetotal = {xxii, 775},
 note={\url{http://www.deeplearningbook.org}},
}


@book{Jolliffe.2002,
 author = {Jolliffe, I. T.},
 year = {2002},
 title = {Principal Component Analysis},
 url = {https://ebookcentral.proquest.com/lib/kxp/detail.action?docID=6485318},
 keywords = {Electronic books;Principal components analysis},
 edition = {2nd ed.},
 publisher = {{Springer New York}},
 isbn = {9780387224404},
 language = {eng},
 location = {New York, NY},
 series = {Springer Series in Statistics Ser},
 abstract = {},
 pagetotal = {513},
 note = {Jolliffe, I. T. (VerfasserIn)}
}


@article{Kohonen.1990,
 author = {Kohonen, T.},
 year = {1990},
 title = {The self-organizing map},
 pages = {1464--1480},
 pagination = {page},
 volume = {78},
 issn = {00189219},
 journaltitle = {Proceedings of the IEEE},
 shortjournal = {Proc. IEEE},
 doi = {10.1109/5.58325},
 number = {9},
 abstract = {}
}


@misc{Ladjal.2019,
 author = {Ladjal, Sa{\"i}d and Newson, Alasdair and Pham, Chi-Hieu},
 year = {2019},
 title = {A PCA-like Autoencoder},
 url = {https://arxiv.org/pdf/1904.01277},
 keywords = {Computer Vision and Pattern Recognition (cs.CV);Machine Learning (cs.LG)},
 abstract = {An autoencoder is a neural network which data projects to and from a lower dimensional latent space, where this data is easier to understand and model. The autoencoder consists of two sub-networks, the encoder and the decoder, which carry out these transformations. The neural network is trained such that the output is as close to the input as possible, the data having gone through an information bottleneck : the latent space. This tool bears significant ressemblance to Principal Component Analysis (PCA), with two main differences. Firstly, the autoencoder is a non-linear transformation, contrary to PCA, which makes the autoencoder more flexible and powerful. Secondly, the axes found by a PCA are orthogonal, and are ordered in terms of the amount of variability which the data presents along these axes. This makes the interpretability of the PCA much greater than that of the autoencoder, which does not have these attributes. Ideally, then, we would like an autoencoder whose latent space consists of independent components, ordered by decreasing importance to the data. In this paper, we propose an algorithm to create such a network. We create an iterative algorithm which progressively increases the size of the latent space, learning a new dimension at each step. Secondly, we propose a covariance loss term to add to the standard autoencoder loss function, as well as a normalisation layer just before the latent space, which encourages the latent space components to be statistically independent. We demonstrate the results of this autoencoder on simple geometric shapes, and find that the algorithm indeed finds a meaningful representation in the latent space. This means that subsequent interpolation in the latent space has meaning with respect to the geometric properties of the images.},
 file = {A PCA-like Autoencoder:Attachments/A PCA-like Autoencoder.pdf:application/pdf}
}


@article{Maaten.2009,
 author = {{van der Maaten}, Laurens and Postma, Eric and {van den Herik}, Jaap},
 year = {2009},
 title = {Dimensionality Reduction: A comparative review},
 url = {https://lvdmaaten.github.io/publications/papers/TR_Dimensionality_Reduction_Review_2009.pdf},
 abstract = {},
 pagetotal = {36},
 file = {TR_Dimensiereductie:Attachments/TR_Dimensiereductie.pdf:application/pdf}
}


@article{Pearson.1901,
 author = {Pearson, Karl},
 year = {1901},
 title = {http://www.stats.org.uk/pca/Pearson1901.pdf},
 url = {http://www.stats.org.uk/pca/Pearson1901.pdf},
 journaltitle = {Philosophical Magazine},
 abstract = {},
 pagetotal = {14},
 file = {Microsoft Word - pearson1901doc:Attachments/Microsoft Word - pearson1901doc.pdf:application/pdf}
}


@misc{Scholkopf.2005,
 author = {Sch{\"o}lkopf, Bernhard and Smola, Alexander and M{\"u}ller, Klaus-Robert},
 year = {2005},
 title = {Kernel principal component analysis},
 abstract = {},
 doi = {10.1007/BFb0020217},
 origdate = {2005},
 file = {Kernel principal component analysis:Attachments/Kernel principal component analysis.pdf:application/pdf}
}


@article{vanderMaaten.2008,
 author = {{van der Maaten}, Laurens and Hinton, Goeffrey},
 year = {2008},
 title = {Visualizing Data using t-SNE},
 url = {https://lvdmaaten.github.io/publications/papers/JMLR_2008.pdf},
 abstract = {},
 pagetotal = {27},
 file = {JMLR2008pdf:Attachments/JMLR2008pdf.pdf:application/pdf}
}


