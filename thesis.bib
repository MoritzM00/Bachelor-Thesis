% This file was created with Citavi 6.14.0.281

@proceedings{.2010,
 year = {2010},
 title = {2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
 abstract = {}
}


@proceedings{.2010b,
 year = {2010},
 title = {2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
 isbn = {1063-6919},
 abstract = {},
 eventtitle = {2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition}
}


@proceedings{.2016,
 year = {2016},
 abstract = {}
}


@collection{.2003,
 year = {2003},
 title = {The Analysis of Gene Expression Data},
 publisher = {{Springer, New York, NY}},
 abstract = {}
}


@collection{.2008,
 year = {2008},
 title = {Handbook of Data Visualization},
 publisher = {{Springer Berlin Heidelberg}},
 isbn = {978-3-540-33037-0},
 abstract = {},
 location = {Berlin, Heidelberg}
}


@inproceedings{Aggarwal.2001,
 author = {Aggarwal, Charu C. and Hinneburg, Alexander and Keim, Daniel A.},
 title = {On the Surprising Behavior of Distance Metrics in High Dimensional Space},
 pages = {420--434},
 bookpagination = {page},
 publisher = {{Springer Berlin Heidelberg}},
 isbn = {978-3-540-44503-6},
 editor = {{van den Bussche}, Jan and Vianu, Victor},
 booktitle = {Database Theory --- ICDT 2001},
 year = {2001},
 abstract = {In recent years, the effect of the curse of high dimensionality has been studied in great detail on several problems such as clustering, nearest neighbor search, and indexing. In high dimensional space the data becomes sparse, and traditional indexing and algorithmic techniques fail from a effciency and/or effectiveness perspective. Recent research results show that in high dimensional space, the concept of proximity, distance or nearest neighbor may not even be qualitatively meaningful. In this paper, we view the dimensionality curse from the point of view of the distance metrics which are used to measure the similarity between objects. We specifically examine the behavior of the commonly used Lknorm and show that the problem of meaningfulness in high dimensionality is sensitive to the value of k. For example, this means that the Manhattan distance metric L(1norm) is consistently more preferable than the Euclidean distance metric L(2norm) for high dimensional data mining applications. Using the intuition derived from our analysis, we introduce and examine a natural extension of the Lknorm to fractional distance metrics. We show that the fractional distance metric provides more meaningful results both from the theoretical and empirical perspective. The results show that fractional distance metrics can significantly improve the effectiveness of standard clustering algorithms such as the k-means algorithm.},
 location = {Berlin, Heidelberg}
}


@article{Alam.2014,
 author = {Alam},
 year = {2014},
 title = {HYPERPARAMETER SELECTION IN KERNEL PRINCIPAL COMPONENT ANALYSIS},
 pages = {1139--1150},
 pagination = {page},
 volume = {10},
 issn = {1549-3636},
 journaltitle = {Journal of Computer Science},
 doi = {10.3844/jcssp.2014.1139.1150},
 number = {7},
 abstract = {},
 file = {HYPERPARAMETER SELECTION IN KERNEL PRINCIPAL COMPONENT A:Attachments/HYPERPARAMETER SELECTION IN KERNEL PRINCIPAL COMPONENT A.pdf:application/pdf}
}


@proceedings{B.Scholkopf.2006,
 year = {2006},
 title = {Advances in Neural Information Processing Systems},
 publisher = {{MIT Press}},
 editor = {{B. Sch{\"o}lkopf} and {J. Platt} and {T. Hoffman}},
 abstract = {}
}


@article{Bac.2021,
 author = {Bac, Jonathan and Mirkes, Evgeny M. and Gorban, Alexander N. and Tyukin, Ivan and Zinovyev, Andrei},
 year = {2021},
 title = {Scikit-Dimension: A Python Package for Intrinsic Dimension Estimation},
 volume = {23},
 journaltitle = {Entropy (Basel, Switzerland)},
 language = {eng},
 doi = {10.3390/e23101368},
 number = {10},
 abstract = {Dealing with uncertainty in applications of machine learning to real-life data critically depends on the knowledge of intrinsic dimensionality (ID). A number of methods have been suggested for the purpose of estimating ID, but no standard package to easily apply them one by one or all at once has been implemented in Python. This technical note introduces scikit-dimension, an open-source Python package for intrinsic dimension estimation. The scikit-dimension package provides a uniform implementation of most of the known ID estimators based on the scikit-learn application programming interface to evaluate the global and local intrinsic dimension, as well as generators of synthetic toy and benchmark datasets widespread in the literature. The package is developed with tools assessing the code quality, coverage, unit testing and continuous integration. We briefly describe the package and demonstrate its use in a large-scale (more than 500 datasets) benchmarking of methods for ID estimation for real-life and synthetic data.},
 file = {Scikit-Dimension A Python Package for Intrinsic Dimensio:Attachments/Scikit-Dimension A Python Package for Intrinsic Dimensio.pdf:application/pdf},
 note = {Journal Article},
 eprint = {34682092}
}


@article{Baldi.1989,
 author = {Baldi, Pierre and Hornik, Kurt},
 year = {1989},
 title = {Neural networks and principal component analysis: Learning from examples without local minima},
 pages = {53--58},
 pagination = {page},
 volume = {2},
 issn = {08936080},
 journaltitle = {Neural Networks},
 doi = {10.1016/0893-6080(89)90014-2},
 number = {1},
 abstract = {},
 note = {PII:  0893608089900142}
}


@misc{Bank.2020,
 author = {Bank, Dor and Koenigstein, Noam and Giryes, Raja},
 year = {2020},
 title = {Autoencoders},
 url = {https://arxiv.org/pdf/2003.05991},
 keywords = {Computer Vision and Pattern Recognition (cs.CV);Machine Learning (cs.LG);Machine Learning (stat.ML)},
 abstract = {An autoencoder is a specific type of a neural network, which is mainly designed to encode the input into a compressed and meaningful representation, and then decode it back such that the reconstructed input is similar as possible to the original one. This chapter surveys the different types of autoencoders that are mainly used today. It also describes various applications and use-cases of autoencoders.},
 file = {Autoencoders:Attachments/Autoencoders.pdf:application/pdf}
}


@book{Bellman.1957,
 author = {Bellman, Richard},
 year = {1957},
 title = {Dynamic programming},
 publisher = {{Princeton Univ. Pr}},
 isbn = {069107951X},
 location = {Princeton, NJ},
 abstract = {},
 pagetotal = {339}
}


@inproceedings{Bengio.2006,
 author = {Bengio, Yoshua and Lamblin, Pascal and Popovici, Dan and Larochelle, Hugo},
 title = {Greedy Layer-Wise Training of Deep Networks},
 url = {https://proceedings.neurips.cc/paper/2006/file/5da713a690c067105aeb2fae32403405-Paper.pdf},
 volume = {19},
 publisher = {{MIT Press}},
 editor = {{B. Sch{\"o}lkopf} and {J. Platt} and {T. Hoffman}},
 booktitle = {Advances in Neural Information Processing Systems},
 year = {2006},
 abstract = {}
}


@article{Bennett.1969,
 author = {Bennett, R.},
 year = {1969},
 title = {The intrinsic dimensionality of signal collections},
 pages = {517--525},
 pagination = {page},
 volume = {15},
 issn = {0018-9448},
 journaltitle = {IEEE Transactions on Information Theory},
 shortjournal = {IEEE Trans. Inform. Theory},
 doi = {10.1109/TIT.1969.1054365},
 number = {5},
 abstract = {}
}


@book{Bishop.2006,
 author = {Bishop, Christopher M.},
 year = {2006},
 title = {Pattern recognition and machine learning},
 url = {https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf},
 keywords = {Artificial intelligence;Artificial Intelligence (incl. Robotics);Computer Science;Image Processing and Computer Vision;Machine learning;Maschinelles Lernen;Mustererkennung;Optical data processing;Pattern Recognition;Statistics;Statistics for Engineering, Physics, Computer Science, Chemistry {\&} Geosciences},
 publisher = {{Springer Science+Business Media LLC}},
 language = {eng},
 location = {New York, NY},
 series = {Information Science and Statistics},
 abstract = {Probability Distributions -- Linear Models for Regression -- Linear Models for Classification -- Neural Networks -- Kernel Methods -- Sparse Kernel Machines -- Graphical Models -- Mixture Models and EM -- Approximate Inference -- Sampling Methods -- Continuous Latent Variables -- Sequential Data -- Combining Models.



The dramatic growth in practical applications for machine learning over the last ten years has been accompanied by many important developments in the underlying algorithms and techniques. For example, Bayesian methods have grown from a specialist niche to become mainstream, while graphical models have emerged as a general framework for describing and applying probabilistic techniques. The practical applicability of Bayesian methods has been greatly enhanced by the development of a range of approximate inference algorithms such as variational Bayes and expectation propagation, while new models based on kernels have had a significant impact on both algorithms and applications. This completely new textbook reflects these recent developments while providing a comprehensive introduction to the fields of pattern recognition and machine learning. It is aimed at advanced undergraduates or first-year PhD students, as well as researchers and practitioners. No previous knowledge of pattern recognition or machine learning concepts is assumed. Familiarity with multivariate calculus and basic linear algebra is required, and some experience in the use of probabilities would be helpful though not essential as the book includes a self-contained introduction to basic probability theory. The book is suitable for courses on machine learning, statistics, computer science, signal processing, computer vision, data mining, and bioinformatics. Extensive support is provided for course instructors, including more than 400 exercises, graded according to difficulty. Example solutions for a subset of the exercises are available from the book web site, while solutions for the remainder can be obtained by instructors from the publisher. The book is supported by a great deal of additional material, and the reader is encouraged to visit the book web site for the latest information. Christopher M. Bishop is Deputy Director of Microsoft Research Cambridge, and holds a Chair in Computer Science at the University of Edinburgh. He is a Fellow of Darwin College Cambridge, a Fellow of the Royal Academy of Engineering, and a Fellow of the Royal Society of Edinburgh. His previous textbook {\textquotedbl}Neural Networks for Pattern Recognition{\textquotedbl} has been widely adopted. Coming soon: *For students, worked solutions to a subset of exercises available on a public web site (for exercises marked {\textquotedbl}www{\textquotedbl} in the text) *For instructors, worked solutions to remaining exercises from the Springer web site *Lecture slides to accompany each chapter *Data sets available for download.},
 pagetotal = {758},
 note = {Bishop, Christopher M. (VerfasserIn)}
}


@article{Blum.1997,
 author = {Blum, Avrim L. and Langley, Pat},
 year = {1997},
 title = {Selection of relevant features and examples in machine learning},
 url = {https://www.sciencedirect.com/science/article/pii/S0004370297000635},
 keywords = {Machine learning;Relevant examples;Relevant features},
 pages = {245--271},
 pagination = {page},
 volume = {97},
 issn = {00043702},
 journaltitle = {Artificial Intelligence},
 doi = {10.1016/S0004-3702(97)00063-5},
 number = {1-2},
 abstract = {In this survey, we review work in machine learning on methods for handling data sets containing large amounts of irrelevant information. We focus on two key issues: the problem of selecting relevant features, and the problem of selecting relevant examples. We describe the advances that have been made on these topics in both empirical and theoretical work in machine learning, and we present a general framework that we use to compare different methods. We close with some challenges for future work in this area.},
 file = {Selection of relevant features and examples in machine l:Attachments/Selection of relevant features and examples in machine l.pdf:application/pdf},
 note = {PII:  S0004370297000635}
}


@article{Boser.1992,
 author = {Boser, Bernhard E. and Guyon, Isabelle M. and Vapnik, Vladimir N.},
 year = {1992},
 title = {A training algorithm for optimal margin classifiers},
 pages = {144--152},
 pagination = {page},
 journaltitle = {Proceedings of the fifth annual workshop on Computational learning theory},
 doi = {10.1145/130385.130401},
 abstract = {},
 file = {A training algorithm for optimal margin classifiers:Attachments/A training algorithm for optimal margin classifiers.pdf:application/pdf}
}


@article{Bourlard.1988,
 author = {Bourlard, H. and Kamp, Y.},
 year = {1988},
 title = {Auto-association by multilayer perceptrons and singular value decomposition},
 url = {https://link.springer.com/article/10.1007/BF00332918#citeas},
 keywords = {Bioinformatics;Complex Systems;Computer Appl;in Life Sciences;Neurobiology;Neurosciences},
 pages = {291--294},
 pagination = {page},
 volume = {59},
 issn = {0340-1200},
 journaltitle = {Biological Cybernetics},
 shortjournal = {Biol. Cybern.},
 language = {En;en},
 doi = {10.1007/BF00332918},
 number = {4-5},
 abstract = {The multilayer perceptron, when working in auto-association mode, is sometimes considered as an interesting candidate to perform data compression or dimensionality reduction of the feature space in information processing applications. The present paper shows that, for auto-association, the nonlinearities of the hidden units are useless and that the optimal parameter values can be derived directly by purely linear techniques relying on singular value decomposition and low rank matrix approximation, similar in spirit to the well-known Karhunen-Lo{\`e}ve transform. This approach appears thus as an efficient alternative to the general error back-propagation algorithm commonly used for training multilayer perceptrons. Moreover, it also gives a clear interpretation of the r{\^o}le of the different parameters.},
 file = {Auto-association by multilayer perceptrons and singular:Attachments/Auto-association by multilayer perceptrons and singular.pdf:application/pdf},
 note = {PII:  BF00332918}
}


@proceedings{Brodley.2004,
 year = {2004},
 title = {Twenty-first international conference on Machine learning - ICML '04},
 publisher = {{ACM Press}},
 isbn = {1581138285},
 editor = {Brodley, Carla},
 abstract = {},
 doi = {10.1145/1015330},
 venue = {Banff, Alberta, Canada},
 location = {New York, New York, USA},
 eventdate = {7/4/2004 - 7/8/2004},
 eventtitle = {Twenty-first international conference}
}


@article{Burges.2009b,
 author = {Burges, Christopher J. C.},
 year = {2009},
 title = {Dimension Reduction: A Guided Tour},
 pages = {275--364},
 pagination = {page},
 volume = {2},
 issn = {1935-8237},
 journaltitle = {Foundations and Trends{\circledR} in Machine Learning},
 shortjournal = {FNT in Machine Learning},
 doi = {10.1561/2200000002},
 number = {4},
 abstract = {},
 file = {Dimension Reduction A Guided Tour:Attachments/Dimension Reduction A Guided Tour.pdf:application/pdf}
}


@article{Camastra.2002,
 author = {Camastra, F. and Vinciarelli, A.},
 year = {2002},
 title = {Estimating the intrinsic dimension of data with a fractal-based method},
 pages = {1404--1407},
 pagination = {page},
 volume = {24},
 issn = {01628828},
 journaltitle = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
 shortjournal = {IEEE Trans. Pattern Anal. Machine Intell.},
 doi = {10.1109/TPAMI.2002.1039212},
 number = {10},
 abstract = {},
 file = {Estimating the intrinsic dimension of data with a fracta:Attachments/Estimating the intrinsic dimension of data with a fracta.pdf:application/pdf}
}


@article{Campadelli.2015,
 author = {Campadelli, P. and Casiraghi, E. and Ceruti, C. and Rozza, A.},
 year = {2015},
 title = {Intrinsic Dimension Estimation: Relevant Techniques and a Benchmark Framework},
 pages = {1--21},
 pagination = {page},
 volume = {2015},
 issn = {1024-123X},
 journaltitle = {Mathematical Problems in Engineering},
 doi = {10.1155/2015/759567},
 abstract = {},
 file = {Intrinsic Dimension Estimation Relevant Techniques and a:Attachments/Intrinsic Dimension Estimation Relevant Techniques and a.pdf:application/pdf},
 note = {PII:  759567}
}


@article{CarreiraPerpinan.1997,
 author = {Carreira-Perpi{\~n}{\'a}n, Miguel {\'A}.},
 year = {1997},
 title = {A Review of Dimension Reduction Techniques},
 url = {http://www.pca.narod.ru/DimensionReductionBrifReview.pdf},
 abstract = {A classification of dimension reduction problems is proposed.},
 pagetotal = {69},
 file = {A Review of Dimension Reduction Techniques:Attachments/A Review of Dimension Reduction Techniques.pdf:application/pdf}
}


@book{Casella.2008,
 author = {Casella, G. and Fienberg, S. and Olkin, I. and Izenman, Alan J.},
 year = {2008},
 title = {Modern Multivariate Statistical Techniques},
 publisher = {{Springer New York}},
 isbn = {978-0-387-78188-4},
 location = {New York, NY},
 abstract = {},
 doi = {10.1007/978-0-387-78189-1},
 file = {Modern Multivariate Statistical Techniques:Attachments/Modern Multivariate Statistical Techniques.pdf:application/pdf}
}


@article{Cayton.2005,
 author = {Cayton, Lawrence},
 year = {2005},
 title = {Algorithms for manifold learning},
 url = {https://www.lcayton.com/resexam.pdf},
 abstract = {},
 pagetotal = {17},
 file = {resexampdf:Attachments/resexampdf.pdf:application/pdf}
}


@article{Charte.2018,
 author = {Charte, David and Charte, Francisco and Garc{\'i}a, Salvador and {Del Jesus}, Mar{\'i}a J. and Herrera, Francisco},
 year = {2018},
 title = {A practical tutorial on autoencoders for nonlinear feature fusion: Taxonomy, models, software and guidelines},
 url = {https://arxiv.org/pdf/1801.01586},
 keywords = {Machine Learning (cs.LG);Neural and Evolutionary Computing (cs.NE)},
 pages = {78--96},
 pagination = {page},
 volume = {44},
 issn = {15662535},
 journaltitle = {Information Fusion},
 doi = {10.1016/j.inffus.2017.12.007},
 abstract = {Many of the existing machine learning algorithms, both supervised and unsupervised, depend on the quality of the input characteristics to generate a good model. The amount of these variables is also important, since performance tends to decline as the input dimensionality increases, hence the interest in using feature fusion techniques, able to produce feature sets that are more compact and higher level. A plethora of procedures to fuse original variables for producing new ones has been developed in the past decades. The most basic ones use linear combinations of the original variables, such as PCA (Principal Component Analysis) and LDA (Linear Discriminant Analysis), while others find manifold embeddings of lower dimensionality based on non-linear combinations, such as Isomap or LLE (Linear Locally Embedding) techniques.

More recently, autoencoders (AEs) have emerged as an alternative to manifold learning for conducting nonlinear feature fusion. Dozens of AE models have been proposed lately, each with its own specific traits. Although many of them can be used to generate reduced feature sets through the fusion of the original ones, there also AEs designed with other applications in mind.

The goal of this paper is to provide the reader with a broad view of what an AE is, how they are used for feature fusion, a taxonomy gathering a broad range of models, and how they relate to other classical techniques. In addition, a set of didactic guidelines on how to choose the proper AE for a given task is supplied, together with a discussion of the software tools available. Finally, two case studies illustrate the usage of AEs with datasets of handwritten digits and breast cancer.},
 file = {A practical tutorial on autoencoders for nonlinear featu:Attachments/A practical tutorial on autoencoders for nonlinear featu.pdf:application/pdf},
 note = {PII:  S1566253517307844}
}


@article{Chen.2009,
 author = {Chen, Lisha and Buja, Andreas},
 year = {2009},
 title = {Local Multidimensional Scaling for Nonlinear Dimension Reduction, Graph Drawing, and Proximity Analysis},
 pages = {209--219},
 pagination = {page},
 volume = {104},
 issn = {0162-1459},
 journaltitle = {Journal of the American Statistical Association},
 doi = {10.1198/jasa.2009.0111},
 number = {485},
 abstract = {In the past decade there has been a resurgence of interest in nonlinear dimension reduction. Among new proposals are ?Local Linear Embedding,? ?Isomap,? and Kernel Principal Components Analysis which all construct global low-dimensional embeddings from local affine or metric information. We introduce a competing method called ?Local Multidimensional Scaling? (LMDS). Like LLE, Isomap, and KPCA, LMDS constructs its global embedding from local information, but it uses instead a combination of MDS and ?force-directed? graph drawing. We apply the force paradigm to create localized versions of MDS stress functions with a tuning parameter to adjust the strength of nonlocal repulsive forces. We solve the problem of tuning parameter selection with a meta-criterion that measures how well the sets of K-nearest neighbors agree between the data and the embedding. Tuned LMDS seems to be able to outperform MDS, PCA, LLE, Isomap, and KPCA, as illustrated with two well-known image datasets. The meta-criterion can also be used in a pointwise version as a diagnostic tool for measuring the local adequacy of embeddings and thereby detect local problems in dimension reductions.},
 file = {Local Multidimensional Scaling for Nonlinear Dimension R:Attachments/Local Multidimensional Scaling for Nonlinear Dimension R.pdf:application/pdf}
}


@article{Chen.2011,
 author = {Chen, Jing and Liu, Yang},
 year = {2011},
 title = {Locally linear embedding: a survey},
 pages = {29--48},
 pagination = {page},
 volume = {36},
 issn = {0269-2821},
 journaltitle = {Artificial Intelligence Review},
 shortjournal = {Artif Intell Rev},
 doi = {10.1007/s10462-010-9200-z},
 number = {1},
 abstract = {},
 file = {Locally linear embedding a survey:Attachments/Locally linear embedding a survey.pdf:application/pdf},
 note = {PII:  9200}
}


@incollection{Cox.2008,
 author = {Cox, Michael A. A. and Cox, Trevor F.},
 title = {Multidimensional Scaling},
 pages = {315--347},
 bookpagination = {page},
 publisher = {{Springer Berlin Heidelberg}},
 isbn = {978-3-540-33037-0},
 booktitle = {Handbook of Data Visualization},
 year = {2008},
 abstract = {Suppose dissimilarity data have been collected on a set of n objects or individuals, where there is a value of dissimilarity measured for each pair.The dissimilarity measure used might be a subjective judgement made by a judge, where for example a teacher subjectively scores the strength of friendship between pairs of pupils in her class, or, as an alternative, more objective, measure, she might count the number of contacts made in a day between each pair of pupils. In other situations the dissimilarity measure might be based on a data matrix. The general aim of multidimensional scaling is to find a configuration of points in a space, usually Euclidean, where each point represents one of the objects or individuals, and the distances between pairs of points in the configuration match as well as possible the original dissimilarities between the pairs of objects or individuals. Such configurations can be found using metric and non-metric scaling, which are covered in Sects. 2 and 3. A number of other techniques are covered by the umbrella title of multidimensional scaling (MDS), and here the techniques of Procrustes analysis, unidimensional scaling, individual differences scaling, correspondence analysis and reciprocal averaging are briefly introduced and illustrated with pertinent data sets.},
 doi = {10.1007/978-3-540-33037-0_14},
 location = {Berlin, Heidelberg}
}


@article{Cunningham.2014,
 author = {Cunningham, John P. and Ghahramani, Zoubin},
 year = {2014},
 title = {Linear Dimensionality Reduction: Survey, Insights, and Generalizations},
 url = {http://arxiv.org/pdf/1406.0873v2},
 keywords = {Statistics - Machine Learning},
 journaltitle = {Journal of Machine Learning Research. 16(Dec): 2859-2900},
 abstract = {Linear dimensionality reduction methods are a cornerstone of analyzing high dimensional data, due to their simple geometric interpretations and typically attractive computational properties. These methods capture many data features of interest, such as covariance, dynamical structure, correlation between data sets, input-output relationships, and margin between data classes. Methods have been developed with a variety of names and motivations in many fields, and perhaps as a result the connections between all these methods have not been highlighted. Here we survey methods from this disparate literature as optimization programs over matrix manifolds. We discuss principal component analysis, factor analysis, linear multidimensional scaling, Fisher's linear discriminant analysis, canonical correlations analysis, maximum autocorrelation factors, slow feature analysis, sufficient dimensionality reduction, undercomplete independent component analysis, linear regression, distance metric learning, and more. This optimization framework gives insight to some rarely discussed shortcomings of well-known methods, such as the suboptimality of certain eigenvector solutions. Modern techniques for optimization over matrix manifolds enable a generic linear dimensionality reduction solver, which accepts as input data and an objective to be optimized, and returns, as output, an optimal low-dimensional projection of the data. This simple optimization framework further allows straightforward generalizations and novel variants of classical methods, which we demonstrate here by creating an orthogonal-projection canonical correlations analysis. More broadly, this survey and generic solver suggest that linear dimensionality reduction can move toward becoming a blackbox, objective-agnostic numerical technology.},
 pagetotal = {42},
 file = {Linear Dimensionality Reduction Survey Insights and Gene:Attachments/Linear Dimensionality Reduction Survey Insights and Gene.pdf:application/pdf},
 note = {Journal of Machine Learning Research. 16(Dec): 2859-2900, 2015

42 pages, 5 figures, 1 table}
}


@article{Donald.2022b,
 author = {Donald, Maoliosa and Beanlands, Heather and Straus, Sharon and Harwood, Lori and Herrington, Gwen and Waldvogel, Blair and Delgado, Maria and Sparkes, Dwight and Watson, Paul and Elliott, Meghan and McBrien, Kerry and Bello, Aminu and Hemmelgarn, Brenda},
 year = {2022},
 title = {A Research Protocol for Implementation and Evaluation of a Patient-Focused eHealth Intervention for Chronic Kidney Disease},
 pages = {85--94},
 pagination = {page},
 volume = {2},
 journaltitle = {Global implementation research and applications},
 language = {eng},
 doi = {10.1007/s43477-022-00038-3},
 number = {1},
 abstract = {UNLABELLED

Self-management in chronic kidney disease (CKD) can slow disease progression; however, there are few tools available to support patients with early CKD. My Kidneys My Health is a patient-focused electronic health (eHealth) self-management tool developed by patients and caregivers. This study will investigate the implementation of My Kidneys My Health across primary care and general nephrology clinics. The study aims to: (1) identify and address barriers and facilitators that may impact implementation and sustainability of the website into routine clinical care; (2) evaluate implementation quality to inform spread and scale-up. We will conduct a multi-stage approach using qualitative methods, guided by the Quality Implementation Framework and using a qualitative content analysis approach. First, we will identify perceived barriers and facilitators to implementation and considerations for sustainability through interviews with clinicians, based on the Readiness Thinking Tool and the Long Term Success Tool. Analysis will be guided by the Consolidated Framework for Implementation Research and the Theoretical Domains Framework. Appropriate implementation strategies will be identified using the Expert Recommendations for Implementing Change compilation, and implementation plans will be developed based on Proctor's recommendations and the Action, Actor, Context, Target, Time framework. Finally, we will explore implementation quality guided by the RE-AIM framework. There is limited literature describing systematic approaches to implementing and sustaining patient-focused self-management tools into clinical care, in addition to employing tailored implementation strategies to promote adoption and sustainability. We aim to generate insights on how My Kidneys My Health can be integrated into clinical care and how to sustain use of patient-centric eHealth tools in clinical settings on a larger scale.

SUPPLEMENTARY INFORMATION

The online version contains supplementary material available at 10.1007/s43477-022-00038-3.},
 file = {A Research Protocol for Implementation and Evaluation of:Attachments/A Research Protocol for Implementation and Evaluation of.pdf:application/pdf},
 note = {Journal Article

Conflict of interestThe authors declare that they have no conflict of interest.},
 eprint = {35402999}
}


@article{Donoho.2003,
 author = {Donoho, David L. and Grimes, Carrie},
 year = {2003},
 title = {Hessian eigenmaps: locally linear embedding techniques for high-dimensional data},
 url = {https://www.pnas.org/doi/10.1073/pnas.1031596100},
 keywords = {manifold learning|ISOMAP|tangent coordinates|isometry| Laplacian eigenmaps},
 pages = {5591--5596},
 pagination = {page},
 volume = {100},
 issn = {0027-8424},
 journaltitle = {Proceedings of the National Academy of Sciences of the United States of America},
 language = {eng},
 doi = {10.1073/pnas.1031596100},
 number = {10},
 abstract = {We describe a method for recovering the underlying parametrization of scattered data (m(i)) lying on a manifold M embedded in high-dimensional Euclidean space. The method, Hessian-based locally linear embedding, derives from a conceptual framework of local isometry in which the manifold M, viewed as a Riemannian submanifold of the ambient Euclidean Space R(n), is locally isometric to an open, connected subset \textgreek{J} of Euclidean space R(d). Because \textgreek{J} does not have to be convex, this framework is able to handle a significantly wider class of situations than the original ISOMAP algorithm. The theoretical framework revolves around a quadratic form H(f) = $\int$(M)||H(f)(m)||$^2$(F)dm defined on functions f: M--{\textgreater} R. Here Hf denotes the Hessian of f, and H(f) averages the Frobenius norm of the Hessian over M. To define the Hessian, we use orthogonal coordinates on the tangent planes of M. The key observation is that, if M truly is locally isometric to an open, connected subset of R(d), then H(f) has a (d + 1)-dimensional null space consisting of the constant functions and a d-dimensional space of functions spanned by the original isometric coordinates. Hence, the isometric coordinates can be recovered up to a linear isometry. Our method may be viewed as a modification of locally linear embedding and our theoretical framework as a modification of the Laplacian eigenmaps framework, where we substitute a quadratic form based on the Hessian in place of one based on the Laplacian.},
 file = {Hessian eigenmaps Locally linear embedding techniques fo:Attachments/Hessian eigenmaps Locally linear embedding techniques fo.pdf:application/pdf},
 note = {Journal Article},
 eprint = {16576753}
}


@proceedings{Dorffner.2001,
 year = {2001},
 title = {Artificial Neural Networks --- ICANN 2001},
 publisher = {{Springer Berlin Heidelberg}},
 isbn = {978-3-540-44668-2},
 editor = {Dorffner, Georg and Bischof, Horst and Hornik, Kurt},
 abstract = {},
 location = {Berlin, Heidelberg}
}


@misc{DumitruIanGoodfellowWillCukierskiYoshuaBengio.2013,
 author = {{Dumitru, Ian Goodfellow, Will Cukierski, Yoshua Bengio}},
 year = {2013},
 title = {Challenges in Representation Learning: Facial Expression Recognition Challenge},
 url = {https://kaggle.com/competitions/challenges-in-representation-learning-facial-expression-recognition-challenge},
 abstract = {},
 publisher = {Kaggle}
}


@proceedings{Dzeroski.2005,
 year = {2005},
 title = {Proceedings of the 22nd international conference on Machine learning - ICML '05},
 publisher = {{ACM Press}},
 isbn = {1595931805},
 editor = {Dzeroski, Saso},
 abstract = {},
 doi = {10.1145/1102351},
 venue = {Bonn, Germany},
 location = {New York, New York, USA},
 eventdate = {8/7/2005 - 8/11/2005},
 eventtitle = {the 22nd international conference}
}


@article{Eckart.1936,
 author = {Eckart, Carl and Young, Gale},
 year = {1936},
 title = {The approximation of one matrix by another of lower rank},
 url = {https://link.springer.com/article/10.1007/BF02288367},
 keywords = {Assessment;Humanities;Law;Psychometrics;Statistical Theory and Methods;Statistics for Social Sciences;Testing and Evaluation},
 pages = {211--218},
 pagination = {page},
 volume = {1},
 issn = {0033-3123},
 journaltitle = {Psychometrika},
 language = {En;en},
 doi = {10.1007/BF02288367},
 number = {3},
 abstract = {The mathematical problem of approximating one matrix by another of lower rank is closely related to the fundamental postulate of factor-theory. When formulated as a least-squares problem, the normal equations cannot be immediately written down, since the elements of the approximate matrix are not independent of one another. The solution of the problem is simplified by first expressing the matrices in a canonic form. It is found that the problem always has a solution which is usually unique. Several conclusions can be drawn from the form of this solution. A hypothetical interpretation of the canonic components of a score matrix is discussed.},
 file = {The approximation of one matrix by another of lower rank:Attachments/The approximation of one matrix by another of lower rank.pdf:application/pdf},
 note = {PII:  BF02288367}
}


@misc{GaryB.Huang.2007,
 author = {{Gary B. Huang} and {Manu Ramesh} and {Tamara Berg} and {Erik Learned-Miller}},
 year = {2007},
 title = {Labeled Faces in the Wild: A Database for Studying Face Recognition in Unconstrained Environments},
 number = {07-49},
 abstract = {},
 organization = {{University of Massachusetts, Amherst}},
 note = {October}
}


@book{Garzon.2022,
 author = {Garzon, Max and Yang, Ching-Chi and Venugopal, Deepak and Kumar, Nirman and Jana, Kalidas and Deng, Lih-Yuan},
 year = {2022},
 title = {Dimensionality Reduction in Data Science},
 publisher = {{Springer International Publishing}},
 isbn = {978-3-031-05370-2},
 location = {Cham},
 abstract = {},
 doi = {10.1007/978-3-031-05371-9},
 file = {Dimensionality Reduction in Data Science:Attachments/Dimensionality Reduction in Data Science.pdf:application/pdf}
}


@misc{Ghojogh.2020,
 author = {Ghojogh, Benyamin and Ghodsi, Ali and Karray, Fakhri and Crowley, Mark},
 year = {2020},
 title = {Locally Linear Embedding and its Variants: Tutorial and Survey},
 url = {https://arxiv.org/pdf/2011.10925},
 keywords = {Computer Vision and Pattern Recognition (cs.CV);Machine Learning (cs.LG);Machine Learning (stat.ML)},
 abstract = {This is a tutorial and survey paper for Locally Linear Embedding (LLE) and its variants. The idea of LLE is fitting the local structure of manifold in the embedding space. In this paper, we first cover LLE, kernel LLE, inverse LLE, and feature fusion with LLE. Then, we cover out-of-sample embedding using linear reconstruction, eigenfunctions, and kernel mapping. Incremental LLE is explained for embedding streaming data. Landmark LLE methods using the Nystrom approximation and locally linear landmarks are explained for big data embedding. We introduce the methods for parameter selection of number of neighbors using residual variance, Procrustes statistics, preservation neighborhood error, and local neighborhood selection. Afterwards, Supervised LLE (SLLE), enhanced SLLE, SLLE projection, probabilistic SLLE, supervised guided LLE (using Hilbert-Schmidt independence criterion), and semi-supervised LLE are explained for supervised and semi-supervised embedding. Robust LLE methods using least squares problem and penalty functions are also introduced for embedding in the presence of outliers and noise. Then, we introduce fusion of LLE with other manifold learning methods including Isomap (i.e., ISOLLE), principal component analysis, Fisher discriminant analysis, discriminant LLE, and Isotop. Finally, we explain weighted LLE in which the distances, reconstruction weights, or the embeddings are adjusted for better embedding; we cover weighted LLE for deformed distributed data, weighted LLE using probability of occurrence, SLLE by adjusting weights, modified LLE, and iterative LLE.},
 file = {Locally Linear Embedding and its Variants Tutorial and Survey:Attachments/Locally Linear Embedding and its Variants Tutorial and Survey.pdf:application/pdf}
}


@misc{Ghosh.2019,
 author = {Ghosh, Partha and Sajjadi, Mehdi S. M. and Vergari, Antonio and Black, Michael and Sch{\"o}lkopf, Bernhard},
 year = {2019},
 title = {From Variational to Deterministic Autoencoders},
 url = {https://arxiv.org/pdf/1903.12436},
 keywords = {Computer Science - Learning;Machine Learning (cs.LG);Machine Learning (stat.ML);Statistics - Machine Learning},
 abstract = {Variational Autoencoders (VAEs) provide a theoretically-backed and popular framework for deep generative models. However, learning a VAE from data poses still unanswered theoretical questions and considerable practical challenges. In this work, we propose an alternative framework for generative modeling that is simpler, easier to train, and deterministic, yet has many of the advantages of VAEs. We observe that sampling a stochastic encoder in a Gaussian VAE can be interpreted as simply injecting noise into the input of a deterministic decoder. We investigate how substituting this kind of stochasticity, with other explicit and implicit regularization schemes, can lead to an equally smooth and meaningful latent space without forcing it to conform to an arbitrarily chosen prior. To retrieve a generative mechanism to sample new data, we introduce an ex-post density estimation step that can be readily applied also to existing VAEs, improving their sample quality. We show, in a rigorous empirical study, that the proposed regularized deterministic autoencoders are able to generate samples that are comparable to, or better than, those of VAEs and more powerful alternatives when applied to images as well as to structured data such as molecules. \footnote{An implementation is available at: \url{https://github.com/ParthaEth/Regularized_autoencoders-RAE-}}},
 file = {From Variational to Deterministic Autoencoders:Attachments/From Variational to Deterministic Autoencoders.pdf:application/pdf},
 note = {Partha Ghosh and Mehdi S. M. Sajjadi contributed equally to this work}
}


@book{Goodfellow.2016,
 author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
 year = {2016},
 title = {Deep learning},
 publisher = {{The MIT Press}},
 isbn = {9780262035613},
 language = {eng},
 location = {Cambridge, Massachusetts and London, England},
 series = {Adaptive computation and machine learning},
 abstract = {},
 pagetotal = {xxii, 775},
 note = {Goodfellow, Ian (VerfasserIn)

Bengio, Yoshua (VerfasserIn)

Courville, Aaron (VerfasserIn)},
 organization = {{MIT Press}}
}


@article{Gracia.2014,
 author = {Gracia, Antonio and Gonz{\'a}lez, Santiago and Robles, Victor and Menasalvas, Ernestina},
 year = {2014},
 title = {A methodology to compare Dimensionality Reduction algorithms in terms of loss of quality},
 pages = {1--27},
 pagination = {page},
 volume = {270},
 issn = {00200255},
 journaltitle = {Information Sciences},
 doi = {10.1016/j.ins.2014.02.068},
 abstract = {},
 file = {A methodology to compare Dimensionality Reduction algori:Attachments/A methodology to compare Dimensionality Reduction algori.pdf:application/pdf},
 note = {PII:  S0020025514001741}
}


@book{Guler.2010,
 author = {G{\"u}ler, Osman},
 year = {2010},
 title = {Foundations of Optimization},
 publisher = {{Springer Science {\&} Business Media}},
 isbn = {9780387684079},
 language = {en},
 abstract = {This book covers the fundamental principles of optimization in finite dimensions. It develops the necessary material in multivariable calculus both with coordinates and coordinate-free, so recent developments such as semidefinite programming can be dealt with.{\textless}/p{\textgreater}}
}


@collection{Guyon.2006,
 year = {2006},
 title = {Feature Extraction: Foundations and Applications},
 publisher = {{Springer Berlin Heidelberg}},
 isbn = {978-3-540-35488-8},
 editor = {Guyon, Isabelle and Nikravesh, Masoud and Gunn, Steve and Zadeh, Lotfi A.},
 abstract = {},
 location = {Berlin, Heidelberg}
}


@incollection{Guyon.2006b,
 author = {Guyon, Isabelle and Elisseeff, Andr{\'e}},
 title = {An Introduction to Feature Extraction},
 pages = {1--25},
 bookpagination = {page},
 publisher = {{Springer Berlin Heidelberg}},
 isbn = {978-3-540-35488-8},
 editor = {Guyon, Isabelle and Nikravesh, Masoud and Gunn, Steve and Zadeh, Lotfi A.},
 booktitle = {Feature Extraction: Foundations and Applications},
 year = {2006},
 abstract = {This chapter introduces the reader to the various aspects of feature extraction covered in this book. Section 1 reviews definitions and notations and proposes a unified view of the feature extraction problem. Section 2 is an overview of the methods and results presented in the book, emphasizing novel contributions. Section 3 provides the reader with an entry point in the field of feature extraction by showing small revealing examples and describing simple but effective algorithms. Finally, Section 4 introduces a more theoretical formalism and points to directions of research and open problems.},
 doi = {10.1007/978-3-540-35488-8_1},
 location = {Berlin, Heidelberg}
}


@misc{Hallgren.2018,
 author = {Hallgren, Fredrik and Northrop, Paul},
 year = {2018},
 title = {Incremental kernel PCA and the Nystr{\"o}m method},
 url = {https://arxiv.org/pdf/1802.00043},
 keywords = {Computer Science - Learning;Machine Learning (cs.LG);Machine Learning (stat.ML);Statistics - Machine Learning},
 abstract = {Incremental versions of batch algorithms are often desired, for increased time efficiency in the streaming data setting, or increased memory efficiency in general. In this paper we present a novel algorithm for incremental kernel PCA, based on rank one updates to the eigendecomposition of the kernel matrix, which is more computationally efficient than comparable existing algorithms. We extend our algorithm to incremental calculation of the Nystroem approximation to the kernel matrix, the first such algorithm proposed. Incremental calculation of the Nystroem approximation leads to further gains in memory efficiency, and allows for empirical evaluation of when a subset of sufficient size has been obtained.},
 file = {Incremental kernel PCA and the Nystrom method:Attachments/Incremental kernel PCA and the Nystrom method.pdf:application/pdf}
}


@inproceedings{Hein.2005b,
 author = {Hein, Matthias and Audibert, Jean-Yves},
 title = {Intrinsic dimensionality estimation of submanifolds in R d},
 pages = {289--296},
 bookpagination = {page},
 publisher = {{ACM Press}},
 isbn = {1595931805},
 editor = {Dzeroski, Saso},
 booktitle = {Proceedings of the 22nd international conference on Machine learning - ICML '05},
 year = {2005},
 abstract = {},
 doi = {10.1145/1102351.1102388},
 location = {New York, New York, USA},
 eventtitle = {the 22nd international conference},
 venue = {Bonn, Germany},
 eventdate = {8/7/2005 - 8/11/2005}
}


@article{Hornik.1989,
 author = {Hornik, Kurt and Stinchcombe, Maxwell and White, Halbert},
 year = {1989},
 title = {Multilayer feedforward networks are universal approximators},
 pages = {359--366},
 pagination = {page},
 volume = {2},
 issn = {08936080},
 journaltitle = {Neural Networks},
 doi = {10.1016/0893-6080(89)90020-8},
 number = {5},
 abstract = {},
 note = {PII:  0893608089900208}
}


@article{Hotelling.1933,
 author = {Hotelling, H.},
 year = {1933},
 title = {Analysis of a complex of statistical variables into principal components},
 pages = {417--441},
 pagination = {page},
 volume = {24},
 issn = {0022-0663},
 journaltitle = {Journal of Educational Psychology},
 doi = {10.1037/h0071325},
 number = {6},
 abstract = {}
}


@collection{InternationalMachineLearningSociety.2011,
 year = {2011},
 title = {Proceedings of the Twenty-Eighth International Conference on Machine Learning},
 isbn = {9781450306195},
 abstract = {},
 pagetotal = {1216},
 language = {eng},
 subtitle = {Bellevue, Washington, USA, June 28 - July 2,2011},
 location = {Madison},
 organization = {{International Machine Learning Society}}
}


@misc{Ioffe.2015,
 author = {Ioffe, Sergey and Szegedy, Christian},
 year = {2015},
 title = {Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift},
 url = {https://arxiv.org/pdf/1502.03167},
 keywords = {Computer Science - Learning;Machine Learning (cs.LG)},
 abstract = {Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization. It also acts as a regularizer, in some cases eliminating the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.9{\%} top-5 validation error (and 4.8{\%} test error), exceeding the accuracy of human raters.},
 file = {Batch Normalization Accelerating Deep Network Training b:Attachments/Batch Normalization Accelerating Deep Network Training b.pdf:application/pdf}
}


@book{Jolliffe.2002,
 author = {Jolliffe, I. T.},
 year = {2002},
 title = {Principal Component Analysis},
 url = {https://ebookcentral.proquest.com/lib/kxp/detail.action?docID=6485318},
 keywords = {Electronic books;Principal components analysis},
 edition = {2nd ed.},
 publisher = {{Springer New York}},
 isbn = {9780387224404},
 language = {eng},
 location = {New York, NY},
 series = {Springer Series in Statistics Ser},
 abstract = {},
 pagetotal = {513},
 note = {Jolliffe, I. T. (VerfasserIn)}
}


@book{Jones.2018,
 author = {Jones, Peter Watts and Smith, Peter},
 year = {2018},
 title = {Stochastic processes},
 url = {https://search.ebscohost.com/login.aspx?direct=true&scope=site&db=nlebk&db=nlabk&AN=1984388},
 keywords = {Stochastic processes;Textbooks},
 edition = {Third edition},
 publisher = {{CRC Press Taylor {\&} Francis Group}},
 isbn = {9781498778121},
 subtitle = {An introduction},
 language = {eng},
 location = {Boca Raton, FL},
 series = {Chapman {\&} Hall/CRC texts in statistical science series},
 abstract = {Based on a well-established and popular course taught by the authors over many years, Stochastic Processes: An Introduction, Third Edition, discusses the modelling and analysis of random experiments, where processes evolve over time. The text begins with a review of relevant fundamental probability. It then covers gambling problems, random walks, and Markov chains. The authors go on to discuss random processes continuous in time, including Poisson, birth and death processes, and general population models, and present an extended discussion on the analysis of associated stationary processes in queues. The book also explores reliability and other random processes, such as branching, martingales, and simple epidemics. A new chapter describing Brownian motion, where the outcomes are continuously observed over continuous time, is included. Further applications, worked examples and problems, and biographical details have been added to this edition. Much of the text has been reworked. The appendix contains key results in probability for reference. This concise, updated book makes the material accessible, highlighting simple applications and examples.--},
 note = {Jones, Peter Watts (VerfasserIn)

Smith, Peter (VerfasserIn)}
}


@article{Kohonen.1990,
 author = {Kohonen, T.},
 year = {1990},
 title = {The self-organizing map},
 pages = {1464--1480},
 pagination = {page},
 volume = {78},
 issn = {00189219},
 journaltitle = {Proceedings of the IEEE},
 shortjournal = {Proc. IEEE},
 doi = {10.1109/5.58325},
 number = {9},
 abstract = {}
}


@article{Kramer.1991,
 author = {Kramer, Mark A.},
 year = {1991},
 title = {Nonlinear principal component analysis using autoassociative neural networks},
 url = {https://aiche.onlinelibrary.wiley.com/doi/10.1002/aic.690370209},
 pages = {233--243},
 pagination = {page},
 volume = {37},
 issn = {0001-1541},
 journaltitle = {AIChE Journal},
 shortjournal = {AIChE J.},
 language = {en},
 doi = {10.1002/aic.690370209},
 number = {2},
 abstract = {Abstract Nonlinear principal component analysis is a novel technique for multivariate data analysis, similar to the well-known method of principal component analysis. NLPCA, like PCA, is used to id...}
}


@article{Kruskal.1964,
 author = {Kruskal, J. B.},
 year = {1964},
 title = {Multidimensional scaling by optimizing goodness of fit to a nonmetric hypothesis},
 pages = {1--27},
 pagination = {page},
 volume = {29},
 issn = {0033-3123},
 journaltitle = {Psychometrika},
 doi = {10.1007/BF02289565},
 number = {1},
 abstract = {},
 file = {Multidimensional scaling by optimizing goodness of fit t:Attachments/Multidimensional scaling by optimizing goodness of fit t.pdf:application/pdf},
 note = {PII:  BF02289565}
}


@misc{Kunin.2019,
 author = {Kunin, Daniel and Bloom, Jonathan M. and Goeva, Aleksandrina and Seed, Cotton},
 year = {2019},
 title = {Loss Landscapes of Regularized Linear Autoencoders},
 url = {https://arxiv.org/pdf/1901.08168},
 keywords = {Computer Science - Learning;Machine Learning (cs.LG);Machine Learning (stat.ML);Statistics - Machine Learning},
 abstract = {Autoencoders are a deep learning model for representation learning. When trained to minimize the distance between the data and its reconstruction, linear autoencoders (LAEs) learn the subspace spanned by the top principal directions but cannot learn the principal directions themselves. In this paper, we prove that $L_2$-regularized LAEs are symmetric at all critical points and learn the principal directions as the left singular vectors of the decoder. We smoothly parameterize the critical manifold and relate the minima to the MAP estimate of probabilistic PCA. We illustrate these results empirically and consider implications for PCA algorithms, computational neuroscience, and the algebraic topology of learning.},
 pagetotal = {12},
 file = {Loss Landscapes of Regularized Linear Autoencoders:Attachments/Loss Landscapes of Regularized Linear Autoencoders.pdf:application/pdf},
 note = {12 pages, 8 figures. ICML 2019}
}


@article{Kwok.2004,
 author = {Kwok, James Tin-yau and Tsang, Ivor Wai-hung},
 year = {2004},
 title = {The pre-image problem in kernel methods},
 keywords = {Algorithms;Artificial intelligence;Cluster Analysis;Computer Simulation;Decision Support Techniques;Feedback;Image Interpretation, Computer-Assisted/methods;Information Storage and Retrieval/methods;Neural Networks, Computer;Pattern Recognition, Automated/methods;Principal Component Analysis},
 pages = {1517--1525},
 pagination = {page},
 volume = {15},
 issn = {1045-9227},
 journaltitle = {IEEE transactions on neural networks},
 language = {eng},
 doi = {10.1109/TNN.2004.837781},
 number = {6},
 abstract = {In this paper, we address the problem of finding the pre-image of a feature vector in the feature space induced by a kernel. This is of central importance in some kernel applications, such as on using kernel principal component analysis (PCA) for image denoising. Unlike the traditional method which relies on nonlinear optimization, our proposed method directly finds the location of the pre-image based on distance constraints in the feature space. It is noniterative, involves only linear algebra and does not suffer from numerical instability or local minimum problems. Evaluations on performing kernel PCA and kernel clustering on the USPS data set show much improved performance.},
 file = {The pre-image problem in kernel methods:Attachments/The pre-image problem in kernel methods.pdf:application/pdf},
 note = {Comparative Study

Evaluation Study

Journal Article

Research Support, Non-U.S. Gov't},
 eprint = {15565778}
}


@proceedings{L.Saul.2004,
 year = {2004},
 title = {Advances in Neural Information Processing Systems},
 publisher = {{MIT Press}},
 editor = {{L. Saul} and {Y. Weiss} and {L. Bottou}},
 abstract = {}
}


@misc{Ladjal.2019,
 author = {Ladjal, Sa{\"i}d and Newson, Alasdair and Pham, Chi-Hieu},
 year = {2019},
 title = {A PCA-like Autoencoder},
 url = {https://arxiv.org/pdf/1904.01277},
 keywords = {Computer Vision and Pattern Recognition (cs.CV);Machine Learning (cs.LG)},
 abstract = {An autoencoder is a neural network which data projects to and from a lower dimensional latent space, where this data is easier to understand and model. The autoencoder consists of two sub-networks, the encoder and the decoder, which carry out these transformations. The neural network is trained such that the output is as close to the input as possible, the data having gone through an information bottleneck : the latent space. This tool bears significant ressemblance to Principal Component Analysis (PCA), with two main differences. Firstly, the autoencoder is a non-linear transformation, contrary to PCA, which makes the autoencoder more flexible and powerful. Secondly, the axes found by a PCA are orthogonal, and are ordered in terms of the amount of variability which the data presents along these axes. This makes the interpretability of the PCA much greater than that of the autoencoder, which does not have these attributes. Ideally, then, we would like an autoencoder whose latent space consists of independent components, ordered by decreasing importance to the data. In this paper, we propose an algorithm to create such a network. We create an iterative algorithm which progressively increases the size of the latent space, learning a new dimension at each step. Secondly, we propose a covariance loss term to add to the standard autoencoder loss function, as well as a normalisation layer just before the latent space, which encourages the latent space components to be statistically independent. We demonstrate the results of this autoencoder on simple geometric shapes, and find that the algorithm indeed finds a meaningful representation in the latent space. This means that subsequent interpolation in the latent space has meaning with respect to the geometric properties of the images.},
 file = {A PCA-like Autoencoder:Attachments/A PCA-like Autoencoder.pdf:application/pdf}
}


@article{Lecun.1998,
 author = {Lecun, Y. and Bottou, L. and Bengio, Y. and Haffner, P.},
 year = {1998},
 title = {Gradient-based learning applied to document recognition},
 pages = {2278--2324},
 pagination = {page},
 volume = {86},
 issn = {00189219},
 journaltitle = {Proceedings of the IEEE},
 shortjournal = {Proc. IEEE},
 doi = {10.1109/5.726791},
 number = {11},
 abstract = {},
 file = {Gradient-based learning applied to document recognition:Attachments/Gradient-based learning applied to document recognition.pdf:application/pdf}
}


@article{LeCun.2010,
 author = {LeCun, Yann and Cortes, Corinna and Burges, C. J.},
 year = {2010},
 title = {MNIST handwritten digit database},
 volume = {2},
 journaltitle = {ATT Labs [Online]. Available: http://yann.lecun.com/exdb/mnist},
 abstract = {}
}


@book{Lee.2007,
 author = {Lee, John A. and Verleysen, Michel},
 year = {2007},
 title = {Nonlinear Dimensionality Reduction},
 publisher = {{Springer New York}},
 isbn = {978-0-387-39350-6},
 location = {New York, NY},
 abstract = {},
 doi = {10.1007/978-0-387-39351-3}
}


@article{Lee.2009,
 author = {Lee, John A. and Verleysen, Michel},
 year = {2009},
 title = {Quality assessment of dimensionality reduction: Rank-based criteria},
 pages = {1431--1443},
 pagination = {page},
 volume = {72},
 issn = {09252312},
 journaltitle = {Neurocomputing},
 doi = {10.1016/j.neucom.2008.12.017},
 number = {7-9},
 abstract = {},
 note = {PII:  S0925231209000101}
}


@book{Lee.2011,
 author = {Lee, John M.},
 year = {2011},
 title = {Introduction to Topological Manifolds},
 volume = {202},
 publisher = {{Springer New York}},
 isbn = {978-1-4419-7939-1},
 location = {New York, NY},
 abstract = {},
 doi = {10.1007/978-1-4419-7940-7},
 file = {Introduction to Topological Manifolds:Attachments/Introduction to Topological Manifolds.pdf:application/pdf}
}


@book{Lee.2012,
 author = {Lee, John M.},
 year = {2012},
 title = {Introduction to Smooth Manifolds},
 volume = {218},
 publisher = {{Springer New York}},
 isbn = {978-1-4419-9981-8},
 location = {New York, NY},
 abstract = {},
 doi = {10.1007/978-1-4419-9982-5},
 file = {Introduction to Smooth Manifolds:Attachments/Introduction to Smooth Manifolds.pdf:application/pdf}
}


@book{Lespinats.2022,
 author = {Lespinats, Sylvain and Colange, Benoit and Dutykh, Denys},
 year = {2022},
 title = {Nonlinear dimensionality reduction techniques},
 publisher = {Springer},
 isbn = {9783030810252},
 subtitle = {A data structure preservation approach},
 language = {eng},
 location = {Cham},
 abstract = {},
 pagetotal = {247},
 doi = {10.1007/978-3-030-81026-9},
 note = {Lespinats, Sylvain (VerfasserIn)

Colange, Benoit (VerfasserIn)

Dutykh, Denys (VerfasserIn)},
 file = {Nonlinear dimensionality reduction techniques:Attachments/Nonlinear dimensionality reduction techniques.pdf:application/pdf}
}


@inproceedings{Levina.2004,
 author = {Levina, Elizaveta and Bickel, Peter},
 title = {Maximum Likelihood Estimation of Intrinsic Dimension},
 url = {https://proceedings.neurips.cc/paper/2004/file/74934548253bcab8490ebd74afed7031-Paper.pdf},
 volume = {17},
 publisher = {{MIT Press}},
 editor = {{L. Saul} and {Y. Weiss} and {L. Bottou}},
 booktitle = {Advances in Neural Information Processing Systems},
 year = {2004},
 abstract = {}
}


@misc{McInnes.2018,
 author = {McInnes, Leland and Healy, John and Melville, James},
 year = {2018},
 title = {UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction},
 url = {https://arxiv.org/pdf/1802.03426},
 keywords = {Computational Geometry (cs.CG);Machine Learning (cs.LG);Machine Learning (stat.ML)},
 abstract = {UMAP (Uniform Manifold Approximation and Projection) is a novel manifold learning technique for dimension reduction. UMAP is constructed from a theoretical framework based in Riemannian geometry and algebraic topology. The result is a practical scalable algorithm that applies to real world data. The UMAP algorithm is competitive with t-SNE for visualization quality, and arguably preserves more of the global structure with superior run time performance. Furthermore, UMAP has no computational restrictions on embedding dimension, making it viable as a general purpose dimension reduction technique for machine learning.},
 file = {UMAP Uniform Manifold Approximation and Projection for D:Attachments/UMAP Uniform Manifold Approximation and Projection for D.pdf:application/pdf}
}


@article{MustafaAbdulSalam.2021,
 author = {{Mustafa Abdul Salam} and {Ahmad Taher} and {Mustafa Samy} and {Khaled Mohamed}},
 year = {2021},
 title = {The Effect of Different Dimensionality Reduction Techniques on Machine Learning Overfitting Problem},
 url = {https://www.researchgate.net/profile/mustafa-abdul-salam/publication/351312502_the_effect_of_different_dimensionality_reduction_techniques_on_machine_learning_overfitting_problem},
 volume = {12},
 issn = {2156-5570},
 journaltitle = {International Journal of Advanced Computer Science and Applications},
 doi = {10.14569/IJACSA.2021.0120480},
 number = {4},
 abstract = {PDF | On Jan 1, 2021, Mustafa Abdul Salam and others published The Effect of Different Dimensionality Reduction Techniques on Machine Learning Overfitting Problem | Find, read and cite all the research you need on ResearchGate},
 file = {The Effect of Different Dimensionality Reduction Techniq:Attachments/The Effect of Different Dimensionality Reduction Techniq.pdf:application/pdf}
}


@collection{OdedMaimon.2009,
 year = {2009},
 title = {Data Mining and Knowledge Discovery Handbook},
 publisher = {{Springer, Boston, MA}},
 editor = {Maimon, Oded and Rokach, Lior},
 abstract = {},
 file = {Data Mining and Knowledge Discovery Handbook:Attachments/Data Mining and Knowledge Discovery Handbook.pdf:application/pdf}
}


@incollection{Parmigiani.2003,
 author = {Parmigiani, Giovanni and Garrett, Elizabeth S. and Irizarry, Rafael A. and Zeger, Scott L.},
 title = {The Analysis of Gene Expression Data: An Overview of Methods and Software},
 url = {https://link.springer.com/chapter/10.1007/0-387-21679-0_1#chapter-info},
 pages = {1--45},
 bookpagination = {page},
 publisher = {{Springer, New York, NY}},
 booktitle = {The Analysis of Gene Expression Data},
 year = {2003},
 abstract = {This chapter is a rough map of the book. It provides a concise overview of data-analytic tasks associated with microarray studies, pointers to chapters that can help perform these tasks, and connections with selected data-analytic tools not covered in any of the...},
 doi = {10.1007/0-387-21679-0_1},
 language = {en}
}


@article{Pearson.1901,
 author = {Pearson, Karl},
 year = {1901},
 title = {On lines and planes of closest fit to systems of points in space},
 url = {http://www.stats.org.uk/pca/Pearson1901.pdf},
 journaltitle = {Philosophical Magazine},
 abstract = {},
 pagetotal = {14},
 file = {Microsoft Word - pearson1901doc:Attachments/Microsoft Word - pearson1901doc.pdf:application/pdf}
}


@inproceedings{Plastria.2008,
 author = {Plastria, Frank and de Bruyne, Steven and Carrizosa, Emilio},
 title = {Dimensionality Reduction for Classification},
 pages = {411--418},
 bookpagination = {page},
 publisher = {{Springer Berlin Heidelberg}},
 isbn = {978-3-540-88192-6},
 editor = {Tang, Changjie and Ling, Charles X. and Zhou, Xiaofang and Cercone, Nick J. and Li, Xue},
 booktitle = {Advanced Data Mining and Applications},
 year = {2008},
 abstract = {We investigate the effects of dimensionality reduction using different techniques and different dimensions on six two-class data sets with numerical attributes as pre-processing for two classification algorithms. Besides reducing the dimensionality with the use of principal components and linear discriminants, we also introduce four new techniques. After this dimensionality reduction two algorithms are applied. The first algorithm takes advantage of the reduced dimensionality itself while the second one directly exploits the dimensional ranking. We observe that neither a single superior dimensionality reduction technique nor a straightforward way to select the optimal dimension can be identified. On the other hand we show that a good choice of technique and dimension can have a major impact on the classification power, generating classifiers that can rival industry standards. We conclude that dimensionality reduction should not only be used for visualisation or as pre-processing on very high dimensional data, but also as a general pre-processing technique on numerical data to raise the classification power. The difficult choice of both the dimensionality reduction technique and the reduced dimension however, should be directly based on the effects on the classification power.},
 location = {Berlin, Heidelberg}
}


@misc{Plaut.2018,
 author = {Plaut, Elad},
 year = {2018},
 title = {From Principal Subspaces to Principal Components with Linear Autoencoders},
 url = {https://arxiv.org/pdf/1804.10253},
 keywords = {Machine Learning (cs.LG);Machine Learning (stat.ML)},
 abstract = {The autoencoder is an effective unsupervised learning model which is widely used in deep learning. It is well known that an autoencoder with a single fully-connected hidden layer, a linear activation function and a squared error cost function trains weights that span the same subspace as the one spanned by the principal component loading vectors, but that they are not identical to the loading vectors. In this paper, we show how to recover the loading vectors from the autoencoder weights.},
 file = {From Principal Subspaces to Principal Components with Li:Attachments/From Principal Subspaces to Principal Components with Li.pdf:application/pdf}
}


@misc{Radford.2022,
 author = {Radford, Alec and Kim, Jong Wook and Xu, Tao and Brockman, Greg and McLeavey, Christine and Sutskever, Ilya},
 year = {2022},
 title = {Robust Speech Recognition via Large-Scale Weak Supervision},
 url = {https://arxiv.org/pdf/2212.04356},
 keywords = {Audio and Speech Processing (eess.AS);Computation and Language (cs.CL);Computer Science - Computation and Language;Computer Science - Learning;Computer Science - Sound;Machine Learning (cs.LG);Sound (cs.SD)},
 abstract = {We study the capabilities of speech processing systems trained simply to predict large amounts of transcripts of audio on the internet. When scaled to 680,000 hours of multilingual and multitask supervision, the resulting models generalize well to standard benchmarks and are often competitive with prior fully supervised results but in a zero-shot transfer setting without the need for any fine-tuning. When compared to humans, the models approach their accuracy and robustness. We are releasing models and inference code to serve as a foundation for further work on robust speech processing.},
 file = {Robust Speech Recognition via Large-Scale Weak Supervision:Attachments/Robust Speech Recognition via Large-Scale Weak Supervision.pdf:application/pdf}
}


@incollection{Rifai.2011,
 author = {Rifai, Salah and Vincent, Pascal and Muller, Xavier and Glorot, Xavier and Bengio, Yoshua},
 title = {Contractive Auto-Encoders: Explicit Invariance during Feature Extraction},
 pages = {833--840},
 bookpagination = {page},
 isbn = {9781450306195},
 booktitle = {Proceedings of the Twenty-Eighth International Conference on Machine Learning},
 year = {2011},
 abstract = {We present in this paper a novel approach for training deterministic auto-encoders. We show that by adding a well chosen penalty term to the classical reconstruction cost function, we can achieve results that equal or surpass those attained by other regularized auto-encoders as well as denoising auto-encoders on a range of datasets. This penalty term corresponds to the Frobenius norm of the Jacobian matrix of the encoder activations with respect to the input. We show that this penalty term results in a localized space contraction which in turn yields robust features on the activation layer. Furthermore, we show how this penalty term is related to both regularized auto-encoders and denoising auto-encoders and how it can be seen as a link between deterministic and non-deterministic auto-encoders. We find empirically that this penalty helps to carve a representation that better captures the local directions of variation dictated by the data, corresponding to a lower-dimensional non-linear manifold, while being more invariant to the vast majority of directions orthogonal to the manifold. Finally, we show that by using the learned features to initialize a MLP, we achieve state of the art classification error on a range of datasets, surpassing other methods of pretraining.},
 location = {Madison},
 booksubtitle = {Bellevue, Washington, USA, June 28 - July 2,2011}
}


@inproceedings{RohanPandit.2016,
 author = {{Rohan Pandit} and {Amarda Shehu}},
 title = {A Principled Comparative Analysis of Dimensionality Reduction Techniques on Protein Structure Decoy Data},
 year = {2016},
 abstract = {}
}


@misc{Rombach.12202021,
 author = {Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Bj{\"o}rn},
 year = {12/20/2021},
 title = {High-Resolution Image Synthesis with Latent Diffusion Models},
 url = {https://arxiv.org/pdf/2112.10752},
 keywords = {Computer Science - Computer Vision and Pattern Recognition;Computer Vision and Pattern Recognition (cs.CV)},
 abstract = {By decomposing the image formation process into a sequential application of denoising autoencoders, diffusion models (DMs) achieve state-of-the-art synthesis results on image data and beyond. Additionally, their formulation allows for a guiding mechanism to control the image generation process without retraining. However, since these models typically operate directly in pixel space, optimization of powerful DMs often consumes hundreds of GPU days and inference is expensive due to sequential evaluations. To enable DM training on limited computational resources while retaining their quality and flexibility, we apply them in the latent space of powerful pretrained autoencoders. In contrast to previous work, training diffusion models on such a representation allows for the first time to reach a near-optimal point between complexity reduction and detail preservation, greatly boosting visual fidelity. By introducing cross-attention layers into the model architecture, we turn diffusion models into powerful and flexible generators for general conditioning inputs such as text or bounding boxes and high-resolution synthesis becomes possible in a convolutional manner. Our latent diffusion models (LDMs) achieve a new state of the art for image inpainting and highly competitive performance on various tasks, including unconditional image generation, semantic scene synthesis, and super-resolution, while significantly reducing computational requirements compared to pixel-based DMs. Code is available at https://github.com/CompVis/latent-diffusion .},
 file = {High-Resolution Image Synthesis with Latent Diffusion Models:Attachments/High-Resolution Image Synthesis with Latent Diffusion Models.pdf:application/pdf},
 note = {CVPR 2022}
}


@misc{Rombach.2021,
 author = {Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Bj{\"o}rn},
 year = {2021},
 title = {High-Resolution Image Synthesis with Latent Diffusion Models},
 url = {https://arxiv.org/pdf/2112.10752},
 keywords = {Computer Vision and Pattern Recognition (cs.CV)},
 abstract = {By decomposing the image formation process into a sequential application of denoising autoencoders, diffusion models (DMs) achieve state-of-the-art synthesis results on image data and beyond. Additionally, their formulation allows for a guiding mechanism to control the image generation process without retraining. However, since these models typically operate directly in pixel space, optimization of powerful DMs often consumes hundreds of GPU days and inference is expensive due to sequential evaluations. To enable DM training on limited computational resources while retaining their quality and flexibility, we apply them in the latent space of powerful pretrained autoencoders. In contrast to previous work, training diffusion models on such a representation allows for the first time to reach a near-optimal point between complexity reduction and detail preservation, greatly boosting visual fidelity. By introducing cross-attention layers into the model architecture, we turn diffusion models into powerful and flexible generators for general conditioning inputs such as text or bounding boxes and high-resolution synthesis becomes possible in a convolutional manner. Our latent diffusion models (LDMs) achieve a new state of the art for image inpainting and highly competitive performance on various tasks, including unconditional image generation, semantic scene synthesis, and super-resolution, while significantly reducing computational requirements compared to pixel-based DMs. Code is available at this https URL .},
 file = {High-Resolution Image Synthesis with Latent Diffusion Models:Attachments/High-Resolution Image Synthesis with Latent Diffusion Models.pdf:application/pdf}
}


@article{Roweis.2000,
 author = {Roweis, S. T. and Saul, L. K.},
 year = {2000},
 title = {Nonlinear dimensionality reduction by locally linear embedding},
 keywords = {Algorithms;Artificial intelligence;Face;Humans;Mathematics;Pattern Recognition, Visual},
 pages = {2323--2326},
 pagination = {page},
 volume = {290},
 issn = {0036-8075},
 journaltitle = {Science (New York, N.Y.)},
 language = {eng},
 doi = {10.1126/science.290.5500.2323},
 number = {5500},
 abstract = {Many areas of science depend on exploratory data analysis and visualization. The need to analyze large amounts of multivariate data raises the fundamental problem of dimensionality reduction: how to discover compact representations of high-dimensional data. Here, we introduce locally linear embedding (LLE), an unsupervised learning algorithm that computes low-dimensional, neighborhood-preserving embeddings of high-dimensional inputs. Unlike clustering methods for local dimensionality reduction, LLE maps its inputs into a single global coordinate system of lower dimensionality, and its optimizations do not involve local minima. By exploiting the local symmetries of linear reconstructions, LLE is able to learn the global structure of nonlinear manifolds, such as those generated by images of faces or documents of text.},
 note = {Journal Article

Research Support, Non-U.S. Gov't

Research Support, U.S. Gov't, Non-P.H.S.},
 eprint = {11125150}
}


@article{Sarveniazi.2014,
 author = {Sarveniazi, Alireza},
 year = {2014},
 title = {An Actual Survey of Dimensionality Reduction},
 pages = {55--72},
 pagination = {page},
 volume = {04},
 issn = {2161-1211},
 journaltitle = {American Journal of Computational Mathematics},
 doi = {10.4236/ajcm.2014.42006},
 number = {02},
 abstract = {},
 file = {An Actual Survey of Dimensionality Reduction:Attachments/An Actual Survey of Dimensionality Reduction.pdf:application/pdf}
}


@online{Saul.2000,
 author = {Saul, L. K. and Roweis, S. T.},
 year = {2000},
 title = {An introduction to locally linear embedding},
 url = {http://www.cs.columbia.edu/~jebara/6772/papers/lleintro.pdf},
 abstract = {},
 file = {An introduction to locally linear embedding:Attachments/An introduction to locally linear embedding.pdf:application/pdf}
}


@misc{Scholkopf.1997,
 author = {Sch{\"o}lkopf, Bernhard and Smola, Alexander and M{\"u}ller, Klaus-Robert},
 year = {1997},
 title = {Kernel principal component analysis},
 abstract = {},
 doi = {10.1007/BFb0020217},
 file = {Kernel principal component analysis:Attachments/Kernel principal component analysis.pdf:application/pdf}
}


@book{ShaweTaylor.2011,
 author = {Shawe-Taylor, John and Cristianini, Nello},
 year = {2011},
 title = {Kernel Methods for Pattern Analysis},
 publisher = {{Cambridge University Press}},
 isbn = {9780521813976},
 abstract = {},
 doi = {10.1017/CBO9780511809682}
}


@misc{Sorzano.2014,
 author = {Sorzano, C. O. S. and Vargas, J. and Montano, A. Pascual},
 year = {2014},
 title = {A survey of dimensionality reduction techniques},
 url = {https://arxiv.org/pdf/1403.2877},
 keywords = {Machine Learning (cs.LG);Machine Learning (stat.ML);Quantitative Methods (q-bio.QM)},
 abstract = {Experimental life sciences like biology or chemistry have seen in the recent decades an explosion of the data available from experiments. Laboratory instruments become more and more complex and report hundreds or thousands measurements for a single experiment and therefore the statistical methods face challenging tasks when dealing with such high dimensional data. However, much of the data is highly redundant and can be efficiently brought down to a much smaller number of variables without a significant loss of information. The mathematical procedures making possible this reduction are called dimensionality reduction techniques; they have widely been developed by fields like Statistics or Machine Learning, and are currently a hot research topic. In this review we categorize the plethora of dimension reduction techniques available and give the mathematical insight behind them.},
 file = {A survey of dimensionality reduction techniques:Attachments/A survey of dimensionality reduction techniques.pdf:application/pdf}
}


@proceedings{Tang.2008,
 year = {2008},
 title = {Advanced Data Mining and Applications},
 publisher = {{Springer Berlin Heidelberg}},
 isbn = {978-3-540-88192-6},
 editor = {Tang, Changjie and Ling, Charles X. and Zhou, Xiaofang and Cercone, Nick J. and Li, Xue},
 abstract = {},
 location = {Berlin, Heidelberg}
}


@article{Tenenbaum.2000,
 author = {Tenenbaum, J. B. and de Silva, V. and Langford, J. C.},
 year = {2000},
 title = {A global geometric framework for nonlinear dimensionality reduction},
 keywords = {Algorithms;Artificial intelligence;Face;Humans;Mathematics;Pattern Recognition, Visual;Visual Perception},
 pages = {2319--2323},
 pagination = {page},
 volume = {290},
 issn = {0036-8075},
 journaltitle = {Science (New York, N.Y.)},
 language = {eng},
 doi = {10.1126/science.290.5500.2319},
 number = {5500},
 abstract = {Scientists working with large volumes of high-dimensional data, such as global climate patterns, stellar spectra, or human gene distributions, regularly confront the problem of dimensionality reduction: finding meaningful low-dimensional structures hidden in their high-dimensional observations. The human brain confronts the same problem in everyday perception, extracting from its high-dimensional sensory inputs-30,000 auditory nerve fibers or 10(6) optic nerve fibers-a manageably small number of perceptually relevant features. Here we describe an approach to solving dimensionality reduction problems that uses easily measured local metric information to learn the underlying global geometry of a data set. Unlike classical techniques such as principal component analysis (PCA) and multidimensional scaling (MDS), our approach is capable of discovering the nonlinear degrees of freedom that underlie complex natural observations, such as human handwriting or images of a face under different viewing conditions. In contrast to previous algorithms for nonlinear dimensionality reduction, ours efficiently computes a globally optimal solution, and, for an important class of data manifolds, is guaranteed to converge asymptotically to the true structure.},
 note = {Journal Article

Research Support, Non-U.S. Gov't

Research Support, U.S. Gov't, Non-P.H.S.},
 eprint = {11125149}
}


@proceedings{vandenBussche.2001,
 year = {2001},
 title = {Database Theory --- ICDT 2001},
 publisher = {{Springer Berlin Heidelberg}},
 isbn = {978-3-540-44503-6},
 editor = {{van den Bussche}, Jan and Vianu, Victor},
 abstract = {},
 location = {Berlin, Heidelberg}
}


@article{vanderMaaten.2008,
 author = {{van der Maaten}, Laurens and Hinton, Goeffrey},
 year = {2008},
 title = {Visualizing Data using t-SNE},
 url = {https://lvdmaaten.github.io/publications/papers/JMLR_2008.pdf},
 abstract = {},
 pagetotal = {27},
 file = {JMLR2008pdf:Attachments/JMLR2008pdf.pdf:application/pdf}
}


@article{vanderMaaten.2009,
 author = {{van der Maaten}, Laurens and Postma, Eric and {van den Herik}, Jaap},
 year = {2009},
 title = {Dimensionality Reduction: A comparative review},
 url = {https://lvdmaaten.github.io/publications/papers/TR_Dimensionality_Reduction_Review_2009.pdf},
 abstract = {},
 pagetotal = {36},
 file = {TR_Dimensiereductie:Attachments/TR_Dimensiereductie.pdf:application/pdf}
}


@inproceedings{Venna.2001,
 author = {Venna, Jarkko and Kaski, Samuel},
 title = {Neighborhood Preservation in Nonlinear Projection Methods: An Experimental Study},
 pages = {485--491},
 bookpagination = {page},
 publisher = {{Springer Berlin Heidelberg}},
 isbn = {978-3-540-44668-2},
 editor = {Dorffner, Georg and Bischof, Horst and Hornik, Kurt},
 booktitle = {Artificial Neural Networks --- ICANN 2001},
 year = {2001},
 abstract = {},
 location = {Berlin, Heidelberg}
}


@article{Venna.2006,
 author = {Venna, Jarkko and Kaski, Samuel},
 year = {2006},
 title = {Local multidimensional scaling},
 keywords = {Algorithms;Artificial intelligence;Cluster Analysis;Computer Simulation;Humans;Models, Statistical;Pattern Recognition, Automated;Principal Component Analysis},
 pages = {889--899},
 pagination = {page},
 volume = {19},
 issn = {0893-6080},
 journaltitle = {Neural networks : the official journal of the International Neural Network Society},
 language = {eng},
 doi = {10.1016/j.neunet.2006.05.014},
 number = {6-7},
 abstract = {In a visualization task, every nonlinear projection method needs to make a compromise between trustworthiness and continuity. In a trustworthy projection the visualized proximities hold in the original data as well, whereas a continuous projection visualizes all proximities of the original data. We show experimentally that one of the multidimensional scaling methods, curvilinear components analysis, is good at maximizing trustworthiness. We then extend it to focus on local proximities both in the input and output space, and to explicitly make a user-tunable parameterized compromise between trustworthiness and continuity. The new method compares favorably to alternative nonlinear projection methods.},
 note = {Comparative Study

Journal Article

Research Support, Non-U.S. Gov't},
 eprint = {16787737}
}


@article{Verveer.1995,
 author = {Verveer, P. J. and Duin, R.P.W.},
 year = {1995},
 title = {An evaluation of intrinsic dimensionality estimators},
 pages = {81--86},
 pagination = {page},
 volume = {17},
 issn = {01628828},
 journaltitle = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
 shortjournal = {IEEE Trans. Pattern Anal. Machine Intell.},
 doi = {10.1109/34.368147},
 number = {1},
 abstract = {},
 file = {An evaluation of intrinsic dimensionality estimators:Attachments/An evaluation of intrinsic dimensionality estimators.pdf:application/pdf}
}


@inproceedings{Weinberger.2004b,
 author = {Weinberger, Kilian Q. and Sha, Fei and Saul, Lawrence K.},
 title = {Learning a kernel matrix for nonlinear dimensionality reduction},
 pages = {106},
 bookpagination = {page},
 publisher = {{ACM Press}},
 isbn = {1581138285},
 editor = {Brodley, Carla},
 booktitle = {Twenty-first international conference on Machine learning - ICML '04},
 year = {2004},
 abstract = {},
 doi = {10.1145/1015330.1015345},
 location = {New York, New York, USA},
 eventtitle = {Twenty-first international conference},
 venue = {Banff, Alberta, Canada},
 eventdate = {7/4/2004 - 7/8/2004}
}


@book{Weinberger.2006,
 author = {Weinberger, Kilian Q. and Saul, L. K.},
 year = {2006},
 title = {An introduction to nonlinear dimensionality reduction by maximum variance unfolding},
 url = {https://www.aaai.org/papers/aaai/2006/aaai06-280.pdf},
 abstract = {},
 file = {An introduction to nonlinear dimensionality reduction by:Attachments/An introduction to nonlinear dimensionality reduction by.pdf:application/pdf}
}


@article{Wolpert.1997,
 author = {Wolpert, D. H. and Macready, W. G.},
 year = {1997},
 title = {No free lunch theorems for optimization},
 pages = {67--82},
 pagination = {page},
 volume = {1},
 issn = {1089778X},
 journaltitle = {IEEE Transactions on Evolutionary Computation},
 shortjournal = {IEEE Trans. Evol. Computat.},
 doi = {10.1109/4235.585893},
 number = {1},
 abstract = {},
 file = {No free lunch theorems for optimization:Attachments/No free lunch theorems for optimization.pdf:application/pdf}
}


@misc{Xiao.2017,
 author = {Xiao, Han and Rasul, Kashif and Vollgraf, Roland},
 year = {2017},
 title = {Fashion-MNIST: a Novel Image Dataset for Benchmarking Machine Learning Algorithms},
 url = {https://arxiv.org/pdf/1708.07747},
 keywords = {Computer Science - Computer Vision and Pattern Recognition;Computer Science - Learning;Computer Vision and Pattern Recognition (cs.CV);Machine Learning (cs.LG);Machine Learning (stat.ML);Statistics - Machine Learning},
 abstract = {We present Fashion-MNIST, a new dataset comprising of 28x28 grayscale images of 70,000 fashion products from 10 categories, with 7,000 images per category. The training set has 60,000 images and the test set has 10,000 images. Fashion-MNIST is intended to serve as a direct drop-in replacement for the original MNIST dataset for benchmarking machine learning algorithms, as it shares the same image size, data format and the structure of training and testing splits. The dataset is freely available at https://github.com/zalandoresearch/fashion-mnist},
 file = {Fashion-MNIST a Novel Image Dataset for Benchmarking Mac:Attachments/Fashion-MNIST a Novel Image Dataset for Benchmarking Mac.pdf:application/pdf},
 note = {Dataset is freely available at  https://github.com/zalandoresearch/fashion-mnist Benchmark is available at  http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/}
}


@inproceedings{Zeiler.2010,
 author = {Zeiler, Matthew D. and Krishnan, Dilip and Taylor, Graham W. and Fergus, Rob},
 title = {Deconvolutional networks},
 pages = {2528--2535},
 bookpagination = {page},
 booktitle = {2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
 year = {2010},
 abstract = {},
 doi = {10.1109/CVPR.2010.5539957}
}


@misc{Zhang.2002,
 author = {Zhang, Zhenyue and Zha, Hongyuan},
 year = {2002},
 title = {Principal Manifolds and Nonlinear Dimension Reduction via Local Tangent Space Alignment},
 url = {https://arxiv.org/pdf/cs/0212008},
 keywords = {Artificial Intelligence (cs.AI);Machine Learning (cs.LG)},
 abstract = {Nonlinear manifold learning from unorganized data points is a very challenging unsupervised learning and data visualization problem with a great variety of applications. In this paper we present a new algorithm for manifold learning and nonlinear dimension reduction. Based on a set of unorganized data points sampled with noise from the manifold, we represent the local geometry of the manifold using tangent spaces learned by fitting an affine subspace in a neighborhood of each data point. Those tangent spaces are aligned to give the internal global coordinates of the data points with respect to the underlying manifold by way of a partial eigendecomposition of the neighborhood connection matrix. We present a careful error analysis of our algorithm and show that the reconstruction errors are of second-order accuracy. We illustrate our algorithm using curves and surfaces both in

2D/3D and higher dimensional Euclidean spaces, and 64-by-64 pixel face images with various pose and lighting conditions. We also address several theoretical and algorithmic issues for further research and improvements.},
 file = {Principal Manifolds and Nonlinear Dimension Reduction vi:Attachments/Principal Manifolds and Nonlinear Dimension Reduction vi.pdf:application/pdf}
}


@article{Zhang.2021,
 author = {Zhang, Yinsheng and Shang, Qian and Zhang, Guoming},
 year = {2021},
 title = {pyDRMetrics - A Python toolkit for dimensionality reduction quality assessment},
 pages = {e06199},
 pagination = {page},
 volume = {7},
 issn = {2405-8440},
 journaltitle = {Heliyon},
 language = {eng},
 doi = {10.1016/j.heliyon.2021.e06199},
 number = {2},
 abstract = {High-dimensional data are pervasive in this bigdata era. To avoid the curse of the dimensionality problem, various dimensionality reduction (DR) algorithms have been proposed. To facilitate systematic DR quality comparison and assessment, this paper reviews related metrics and develops an open-source Python package pyDRMetrics. Supported metrics include reconstruction error, distance matrix, residual variance, ranking matrix, co-ranking matrix, trustworthiness, continuity, co-k-nearest neighbor size, LCMC (local continuity meta criterion), and rank-based local/global properties. pyDRMetrics provides a native Python class and a web-oriented API. A case study of mass spectra is conducted to demonstrate the package functions. A web GUI wrapper is also published to support user-friendly B/S applications.},
 file = {pyDRMetrics - A Python toolkit for dimensionality reduct:Attachments/pyDRMetrics - A Python toolkit for dimensionality reduct.pdf:application/pdf},
 note = {Journal Article

The authors declare no conflict of interest.},
 eprint = {33644472}
}


