% This file was created with Citavi 6.14.0.281

@misc{Bank.2020,
 author = {Bank, Dor and Koenigstein, Noam and Giryes, Raja},
 year = {2020},
 title = {Autoencoders},
 url = {https://arxiv.org/pdf/2003.05991},
 keywords = {Computer Vision and Pattern Recognition (cs.CV);Machine Learning (cs.LG);Machine Learning (stat.ML)},
 abstract = {An autoencoder is a specific type of a neural network, which is mainly designed to encode the input into a compressed and meaningful representation, and then decode it back such that the reconstructed input is similar as possible to the original one. This chapter surveys the different types of autoencoders that are mainly used today. It also describes various applications and use-cases of autoencoders.},
 file = {Autoencoders:Attachments/Autoencoders.pdf:application/pdf}
}


@book{Bishop.2006,
 author = {Bishop, Christopher M.},
 year = {2006},
 title = {Pattern recognition and machine learning},
 url = {https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf},
 keywords = {Artificial intelligence;Artificial Intelligence (incl. Robotics);Computer Science;Image Processing and Computer Vision;Machine learning;Maschinelles Lernen;Mustererkennung;Optical data processing;Pattern Recognition;Statistics;Statistics for Engineering, Physics, Computer Science, Chemistry {\&} Geosciences},
 publisher = {{Springer Science+Business Media LLC}},
 language = {eng},
 location = {New York, NY},
 series = {Information Science and Statistics},
 abstract = {Probability Distributions -- Linear Models for Regression -- Linear Models for Classification -- Neural Networks -- Kernel Methods -- Sparse Kernel Machines -- Graphical Models -- Mixture Models and EM -- Approximate Inference -- Sampling Methods -- Continuous Latent Variables -- Sequential Data -- Combining Models.



The dramatic growth in practical applications for machine learning over the last ten years has been accompanied by many important developments in the underlying algorithms and techniques. For example, Bayesian methods have grown from a specialist niche to become mainstream, while graphical models have emerged as a general framework for describing and applying probabilistic techniques. The practical applicability of Bayesian methods has been greatly enhanced by the development of a range of approximate inference algorithms such as variational Bayes and expectation propagation, while new models based on kernels have had a significant impact on both algorithms and applications. This completely new textbook reflects these recent developments while providing a comprehensive introduction to the fields of pattern recognition and machine learning. It is aimed at advanced undergraduates or first-year PhD students, as well as researchers and practitioners. No previous knowledge of pattern recognition or machine learning concepts is assumed. Familiarity with multivariate calculus and basic linear algebra is required, and some experience in the use of probabilities would be helpful though not essential as the book includes a self-contained introduction to basic probability theory. The book is suitable for courses on machine learning, statistics, computer science, signal processing, computer vision, data mining, and bioinformatics. Extensive support is provided for course instructors, including more than 400 exercises, graded according to difficulty. Example solutions for a subset of the exercises are available from the book web site, while solutions for the remainder can be obtained by instructors from the publisher. The book is supported by a great deal of additional material, and the reader is encouraged to visit the book web site for the latest information. Christopher M. Bishop is Deputy Director of Microsoft Research Cambridge, and holds a Chair in Computer Science at the University of Edinburgh. He is a Fellow of Darwin College Cambridge, a Fellow of the Royal Academy of Engineering, and a Fellow of the Royal Society of Edinburgh. His previous textbook {\textquotedbl}Neural Networks for Pattern Recognition{\textquotedbl} has been widely adopted. Coming soon: *For students, worked solutions to a subset of exercises available on a public web site (for exercises marked {\textquotedbl}www{\textquotedbl} in the text) *For instructors, worked solutions to remaining exercises from the Springer web site *Lecture slides to accompany each chapter *Data sets available for download.},
 pagetotal = {758},
 note = {Bishop, Christopher M. (VerfasserIn)}
}


@article{Boser.1992,
 author = {Boser, Bernhard E. and Guyon, Isabelle M. and Vapnik, Vladimir N.},
 year = {1992},
 title = {A training algorithm for optimal margin classifiers},
 pages = {144--152},
 pagination = {page},
 journaltitle = {Proceedings of the fifth annual workshop on Computational learning theory},
 doi = {10.1145/130385.130401},
 abstract = {},
 file = {A training algorithm for optimal margin classifiers:Attachments/A training algorithm for optimal margin classifiers.pdf:application/pdf}
}


@article{CarreiraPerpinan.1997,
 author = {Carreira-Perpi{\~n}{\'a}n, Miguel {\'A}.},
 year = {1997},
 title = {A Review of Dimension Reduction Techniques},
 url = {http://www.pca.narod.ru/DimensionReductionBrifReview.pdf},
 abstract = {A classification of dimension reduction problems is proposed.},
 pagetotal = {69},
 file = {A Review of Dimension Reduction Techniques:Attachments/A Review of Dimension Reduction Techniques.pdf:application/pdf}
}


@article{Cayton.2005,
 author = {Cayton, Lawrence},
 year = {2005},
 title = {Algorithms for manifold learning},
 url = {https://www.lcayton.com/resexam.pdf},
 abstract = {},
 pagetotal = {17},
 file = {resexampdf:Attachments/resexampdf.pdf:application/pdf}
}


@book{Garzon.2022,
 author = {Garzon, Max and Yang, Ching-Chi and Venugopal, Deepak and Kumar, Nirman and Jana, Kalidas and Deng, Lih-Yuan},
 year = {2022},
 title = {Dimensionality Reduction in Data Science},
 publisher = {{Springer International Publishing}},
 isbn = {978-3-031-05370-2},
 location = {Cham},
 abstract = {},
 doi = {10.1007/978-3-031-05371-9},
 file = {Dimensionality Reduction in Data Science:Attachments/Dimensionality Reduction in Data Science.pdf:application/pdf}
}


@book{Goodfellow.2016,
 author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
 year = {2016},
 title = {Deep learning},
 publisher = {{The MIT Press}},
 isbn = {9780262035613},
 language = {eng},
 location = {Cambridge, Massachusetts and London, England},
 series = {Adaptive computation and machine learning},
 abstract = {},
 pagetotal = {xxii, 775},
 note = {Goodfellow, Ian (VerfasserIn)

Bengio, Yoshua (VerfasserIn)

Courville, Aaron (VerfasserIn)},
 organization = {{MIT Press}}
}


@article{Hotelling.1933,
 author = {Hotelling, H.},
 year = {1933},
 title = {Analysis of a complex of statistical variables into principal components},
 pages = {417--441},
 pagination = {page},
 volume = {24},
 issn = {0022-0663},
 journaltitle = {Journal of Educational Psychology},
 doi = {10.1037/h0071325},
 number = {6},
 abstract = {}
}


@book{Jolliffe.2002,
 author = {Jolliffe, I. T.},
 year = {2002},
 title = {Principal Component Analysis},
 url = {https://ebookcentral.proquest.com/lib/kxp/detail.action?docID=6485318},
 keywords = {Electronic books;Principal components analysis},
 edition = {2nd ed.},
 publisher = {{Springer New York}},
 isbn = {9780387224404},
 language = {eng},
 location = {New York, NY},
 series = {Springer Series in Statistics Ser},
 abstract = {},
 pagetotal = {513},
 note = {Jolliffe, I. T. (VerfasserIn)}
}


@article{Kohonen.1990,
 author = {Kohonen, T.},
 year = {1990},
 title = {The self-organizing map},
 pages = {1464--1480},
 pagination = {page},
 volume = {78},
 issn = {00189219},
 journaltitle = {Proceedings of the IEEE},
 shortjournal = {Proc. IEEE},
 doi = {10.1109/5.58325},
 number = {9},
 abstract = {}
}


@misc{Ladjal.2019,
 author = {Ladjal, Sa{\"i}d and Newson, Alasdair and Pham, Chi-Hieu},
 year = {2019},
 title = {A PCA-like Autoencoder},
 url = {https://arxiv.org/pdf/1904.01277},
 keywords = {Computer Vision and Pattern Recognition (cs.CV);Machine Learning (cs.LG)},
 abstract = {An autoencoder is a neural network which data projects to and from a lower dimensional latent space, where this data is easier to understand and model. The autoencoder consists of two sub-networks, the encoder and the decoder, which carry out these transformations. The neural network is trained such that the output is as close to the input as possible, the data having gone through an information bottleneck : the latent space. This tool bears significant ressemblance to Principal Component Analysis (PCA), with two main differences. Firstly, the autoencoder is a non-linear transformation, contrary to PCA, which makes the autoencoder more flexible and powerful. Secondly, the axes found by a PCA are orthogonal, and are ordered in terms of the amount of variability which the data presents along these axes. This makes the interpretability of the PCA much greater than that of the autoencoder, which does not have these attributes. Ideally, then, we would like an autoencoder whose latent space consists of independent components, ordered by decreasing importance to the data. In this paper, we propose an algorithm to create such a network. We create an iterative algorithm which progressively increases the size of the latent space, learning a new dimension at each step. Secondly, we propose a covariance loss term to add to the standard autoencoder loss function, as well as a normalisation layer just before the latent space, which encourages the latent space components to be statistically independent. We demonstrate the results of this autoencoder on simple geometric shapes, and find that the algorithm indeed finds a meaningful representation in the latent space. This means that subsequent interpolation in the latent space has meaning with respect to the geometric properties of the images.},
 file = {A PCA-like Autoencoder:Attachments/A PCA-like Autoencoder.pdf:application/pdf}
}


@article{Pearson.1901,
 author = {Pearson, Karl},
 year = {1901},
 title = {On lines and planes of closest fit to systems of points in space},
 url = {http://www.stats.org.uk/pca/Pearson1901.pdf},
 journaltitle = {Philosophical Magazine},
 abstract = {},
 pagetotal = {14},
 file = {Microsoft Word - pearson1901doc:Attachments/Microsoft Word - pearson1901doc.pdf:application/pdf}
}


@article{Sarveniazi.2014,
 author = {Sarveniazi, Alireza},
 year = {2014},
 title = {An Actual Survey of Dimensionality Reduction},
 pages = {55--72},
 pagination = {page},
 volume = {04},
 issn = {2161-1211},
 journaltitle = {American Journal of Computational Mathematics},
 doi = {10.4236/ajcm.2014.42006},
 number = {02},
 abstract = {},
 file = {An Actual Survey of Dimensionality Reduction:Attachments/An Actual Survey of Dimensionality Reduction.pdf:application/pdf}
}


@misc{Scholkopf.1997,
 author = {Sch{\"o}lkopf, Bernhard and Smola, Alexander and M{\"u}ller, Klaus-Robert},
 year = {1997},
 title = {Kernel principal component analysis},
 abstract = {},
 doi = {10.1007/BFb0020217},
 file = {Kernel principal component analysis:Attachments/Kernel principal component analysis.pdf:application/pdf}
}


@book{ShaweTaylor.2011,
 author = {Shawe-Taylor, John and Cristianini, Nello},
 year = {2011},
 title = {Kernel Methods for Pattern Analysis},
 publisher = {{Cambridge University Press}},
 isbn = {9780521813976},
 abstract = {},
 doi = {10.1017/CBO9780511809682}
}


@article{vanderMaaten.2008,
 author = {{van der Maaten}, Laurens and Hinton, Goeffrey},
 year = {2008},
 title = {Visualizing Data using t-SNE},
 url = {https://lvdmaaten.github.io/publications/papers/JMLR_2008.pdf},
 abstract = {},
 pagetotal = {27},
 file = {JMLR2008pdf:Attachments/JMLR2008pdf.pdf:application/pdf}
}


@article{vanderMaaten.2009,
 author = {{van der Maaten}, Laurens and Postma, Eric and {van den Herik}, Jaap},
 year = {2009},
 title = {Dimensionality Reduction: A comparative review},
 url = {https://lvdmaaten.github.io/publications/papers/TR_Dimensionality_Reduction_Review_2009.pdf},
 abstract = {},
 pagetotal = {36},
 file = {TR_Dimensiereductie:Attachments/TR_Dimensiereductie.pdf:application/pdf}
}


@article{Wolpert.1997,
 author = {Wolpert, D. H. and Macready, W. G.},
 year = {1997},
 title = {No free lunch theorems for optimization},
 pages = {67--82},
 pagination = {page},
 volume = {1},
 issn = {1089778X},
 journaltitle = {IEEE Transactions on Evolutionary Computation},
 shortjournal = {IEEE Trans. Evol. Computat.},
 doi = {10.1109/4235.585893},
 number = {1},
 abstract = {},
 file = {No free lunch theorems for optimization:Attachments/No free lunch theorems for optimization.pdf:application/pdf}
}


