%% ==============================
\chapter{Schluss}
\label{ch:Schluss}
%% ==============================

Die Dimensionsreduktion ist ein für hochdimensionale Daten relevantes Verfahren zur Reduktion von
benötigten Rechen- und Speicherkapazitäten. Daneben wird den unvorteilhaften Eigenschaften von
hochdimensionalen Räumen entgegengewirkt und irrelevante Merkmale in den Daten entfernt. Aus diesen
Gründen wurden eine Vielzahl an Methoden entwickelt, die in ihrer Natur nicht unterschiedlicher
sein könnten. Neben klassischen Methoden wie der Principal Component Analysis wurden in den letzten
Jahren Methoden wie Locally Linear Embedding aus einem Teilgebiet der Dimensionsreduktion, dem
Erlernen von Mannigfaltigkeiten, zu wichtigen Vertretern der Dimensionsreduktion. Erfolge im
Bereich des modernen Machine Learnings mit Deep Learning haben dazu geführt, dass auch diese
Methoden auf dem Bereich der Dimensionsreduktion angewandt und weiterentwickelt wurden. Eine
prominente Methode ist dabei der Autoencoder. Es stellt sich jedoch die Frage, ob diese modernen
Machine Learning Ansätze auch auf dem Bereich der Dimensionsreduktion etablierte statistische
Verfahren ablösen könnten. Aus diesem Grund wurden in einem empirischen Vergleich drei Vertreter
der statistischen Methoden den zwei Machine Learning Ansätzen gegenübergestellt und sowohl auf
künstlichen als auch natürlichen Datensätzen mithilfe dreier Qualitätskriterien evaluiert. Die hier
eingesetzten statischen Methoden zeigen die Vorteile von Zielfunktionen, deren Optimum durch
geschlossene Form berechnet werden kann. Auf Datensätzen mit geringer Stichprobengröße ist dies
meist einer iterativen Optimierung überlegen. Insbesondere bei neuronalen Netzen wie den
Autoencodern ist ein kleiner Datensatz problematisch, da aufgrund der hohen Anzahl an
Freiheitsgraden die Optimierung deutlich instabiler ist. Andererseits skalieren dadurch die Machine
Learning Methoden auch besser auf größere Datensätze. Hier kommen die statischen Methoden wie die
Kernel Principal Component Analysis und Locally Linear Embedding wegen ihrer hohen Zeit- und auch
Speicherkomplexität schnell an ihre Grenzen. Dies macht bereits auf mittelgroßen Datensätzen wie
MNIST und Fashion MNIST mit 60 000 Stichproben ein Downsampling erforderlich. Ein weiterer Vorteil
der Machine Learning Ansätze ist die hohe Flexibilität in der Architektur. So kann
domänenspezifisches Wissen ausgenutzt werden, wie z.B. mit dem Convolutional Autoencoder, der die
Struktur von Bildern oder ähnlichen gitterartigen Topologien ausnutzen kann. Allerdings ist dieses
hohe Maß an Flexibilität zugleich auch ein Nachteil, da die vielen Hyperparameter eine
zeitaufwendige Feinadjustierung notwendig machen. Auch ist die konkrete Wahl eines Hyperparameters
meist willkürlich, was in einem Trial-and-Error-Vorgehen für das Finden der optimalen
Hyperparameter resultiert. Allerdings trifft dies auch schon auf die wenigen Hyperparameter der
statistischen Methoden, insbesondere der Kernel Principal Component Analysis zu. Die Principal
Component Analysis umgeht diese Problematik komplett, da keine Hyperparameter gewählt werden
müssen. Gleichzeitig schneidet diese Methode auf vielen natürlichen Datensätzen relativ gesehen am
besten ab, was die Vorteile einer Flexibilität und insbesondere der Nichtlinearität infrage stellt.
Auf künstlichen Datensätzen kann die Principal Component Analysis jedoch mangels der Fähigkeit,
nichtlineare Mannigfaltigkeiten zu entdecken, durch Autoencoder outperformed werden. Auf diesen
künstlichen Datensätzen ist Locally Linear Embedding auch einem Autoencoder deutlich überlegen. Die
Performance von Locally Linear Embedding auf natürlichen Datensätzen ist hingegen weniger
überzeugend.

\idea{statistisch

	\begin{itemize}
		\item + keine suboptimale Lösung
		\item + wenig Trainingsdaten nötig
		\item + wenig Hyperparameter
		\item -- hohe Laufzeit -> skaliert nicht
		\item -- PCA: nur lineare Mannigfaltigkeiten
	\end{itemize}
}

\idea{ML-Methoden

	\begin{itemize}
		\item + flexibel in der Architektur
		\item + skaliert gut auf große Datensätze
		\item + kann domänenspezifisches Wissen ausnutzen (ConvAE)
		\item -- hohe Laufzeit
		\item -- viele Parameter -> Parameter-Tuning
		\item -- Nichtlinearität wird zum Problem in der Optimierung
	\end{itemize}
}