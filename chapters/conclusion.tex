%% ==============================
\chapter{Schluss}
\label{ch:Schluss}
%% ==============================

Die Dimensionsreduktion ist ein für hochdimensionale Daten relevantes Verfahren zur Reduktion von
benötigten Rechen- und Speicherkapazitäten. Zusätzlich wird den unvorteilhaften Eigenschaften von
hochdimensionalen Räumen entgegengewirkt und irrelevante Merkmale in den Daten entfernt. Aus diesen
Gründen wurden eine Vielzahl an Methoden entwickelt, die in ihrer Natur nicht unterschiedlicher
sein könnten. Neben klassischen Methoden wie der Principal Component Analysis wurden in den letzten
Jahren Methoden wie Locally Linear Embedding aus dem Teilgebiet des Erlernens von
Mannigfaltigkeiten zu wichtigen Vertretern der Dimensionsreduktion. Erfolge im Bereich des modernen
Machine Learnings mit Deep Learning haben dazu geführt, dass auch diese Methoden auf dem Bereich
der Dimensionsreduktion angewandt und weiterentwickelt wurden. Eine prominente Methode ist dabei
der Autoencoder. Es stellt sich jedoch die Frage, ob diese modernen Machine Learning Ansätze auch
auf dem Bereich der Dimensionsreduktion etablierte statistische Verfahren ablösen könnten. Aus
diesem Grund wurden in einem empirischen Vergleich drei Vertreter der statistischen Methoden den
zwei Machine Learning Ansätzen gegenübergestellt und sowohl auf künstlichen als auch natürlichen
Datensätzen mithilfe dreier Qualitätskriterien evaluiert.

Die statistischen Methoden zeigen die Vorteile von Zielfunktionen auf, deren Optimum durch
geschlossene Form berechnet werden kann. Auf Datensätzen mit geringer Stichprobengröße ist dies
meist einer iterativen Optimierung einer komplexeren Zielfunktion überlegen. Insbesondere bei
neuronalen Netzen wie den Autoencodern ist ein kleiner Datensatz problematisch, da aufgrund der
hohen Anzahl an Freiheitsgraden die Optimierung deutlich instabiler ist. Andererseits skalieren
dadurch die Machine Learning Methoden auch besser auf größere Datensätze. Hier kommen die
statistischen Methoden wie die Kernel Principal Component Analysis und Locally Linear Embedding
wegen ihrer hohen Zeit- und auch Speicherkomplexität schnell an ihre Grenzen. Dies macht bereits
auf mittelgroßen Datensätzen wie MNIST mit \num{60000} Stichproben ein Downsampling erforderlich.
Ein weiterer Vorteil der Machine Learning Ansätze ist die hohe Flexibilität in der Architektur. So
kann mittels des Convolutional Autoencoders die Struktur von Bildern oder ähnlichen gitterartigen
Topologien ausgenutzt werden. Der Convolutional Autoencoder schnitt deshalb zumindest auf
hinreichend großen Datensätzen sehr gut ab und zeigte die visuell beste Rekonstruktion eines Bildes
aus dem MNIST-Datensatz. Allerdings ist dieses hohe Maß an Flexibilität zugleich auch ein Nachteil,
da die vielen Hyperparameter eine zeitaufwendige Feinadjustierung notwendig machen. Allerdings
trifft dies auch schon auf die wenigen Hyperparameter der statistischen Methoden, insbesondere der
Kernel Principal Component Analysis zu. Die Principal Component Analysis umgeht diese Problematik
komplett, da keine Hyperparameter gewählt werden müssen. Gleichzeitig schneidet diese Methode auf
vielen natürlichen Datensätzen relativ gesehen am besten ab, was die Vorteile einer Flexibilität
und insbesondere der Nichtlinearität infrage stellt. Auf künstlichen Datensätzen kann die Principal
Component Analysis aber mangels der Fähigkeit, nichtlineare Mannigfaltigkeiten zu entdecken, durch
Autoencoder und die Kernel PCA outperformed werden. Auf diesen künstlichen Datensätzen ist Locally
Linear Embedding jedoch auch einem Autoencoder deutlich überlegen. Die Performance von Locally
Linear Embedding auf natürlichen Datensätzen ist hingegen weniger überzeugend, da die
zugrundeliegende Mannigfaltigkeit möglicherweise deutlich komplexer ist und weniger gute
Eigenschaften aufweist. Ebenfalls konnte der Contractive Autoencoder durch die Anpassung der
Zielfunktion für eine robustere latente Repräsentation nicht überzeugen. In diesem Vergleich wurde
allerdings nur der Gaußsche Kernel verwendet. In künftigen Arbeiten könnte noch eine größere
Auswahl an Kernel-Funktionen untersucht werden. Bei Autoencoder wurden meistens Sigmoid- oder
ReLU-Aktivierungsfunktionen eingesetzt. Hier könnten noch einige weitere Aktivierungsfunktionen
hinsichtlich ihrer Relevanz für die Dimensionsreduktion evaluiert werden.

Darüber hinaus wurde der Zusammenhang eines linearen Autoencoders mit der Principal Component
Analysis untersucht. Man stellt hierbei fest, dass ein dreischichtiger Autoencoder mit linearen
Aktivierungsfunktionen einen Unterraum des durch die Koeffizienten der Hauptkomponenten
aufgespannten Raumes lernt. Regularisiert man den Autoencoder mittels \textit{weight decay}, so
können mithilfe einer Singulärwertzerlegung der Decoder-Matrix die Richtungen der Hauptkomponenten
bis hin zu einer orthogonalen Transformation gelernt werden. So kann ein skalierbarer Algorithmus
als Alternative zur PCA z.B. für Datensätze, die nicht als Ganzes in den Hauptspeicher passen,
eingesetzt werden. Allerdings existieren auch inkrementelle Versionen von PCA, sodass die Relevanz
dieses Ansatzes wahrscheinlich eher von theoretischer Natur bleibt. Nimmt man Nichtlinearität zum
Autoencoder hinzu, so geht die Ähnlichkeit zur PCA im Falle eines dreischichtigen Autoencoders erst
verloren, wenn sowohl Encoder als auch Decoder nichtlinear sind.

Zusammenfassend ist die Frage \textit{Old but gold?} also berechtigt. Insbesondere die Principal
Component Analysis wird wahrscheinlich nicht ohne weiteres durch moderne Machine Learning Ansätze
abgelöst. Das gute Abschneiden hinsichtlich der Qualitätskriterien zusammen mit den Vorteilen in
der Laufzeit und Parameterwahl macht die Principal Component Analysis nicht ohne Grund zur ersten
Wahl vieler Anwender. Auf kleineren Datensätzen kann es jedoch sinnvoll sein, auch nichtlineare
statistische Methoden wie Locally Linear Embedding zu verwenden. Die Machine Learning Methoden sind
aufgrund ihrer Skalierbarkeit insbesondere auf großen Datensätzen interessant, da hier einige
statistische Methoden hinsichtlich des Speicher- und Rechenaufwands an ihre Grenzen kommen. Nimmt
man den hohen Aufwand des Hyperparameter-Tunings und der generell relativ hohen Laufzeit von
Autoencoder in Kauf, so sind die Stärken der Flexibilität gerade auf Bilddatensätzen nicht zu
vernachlässigen. In vielen Fällen rechtfertigt die marginale Outperformance gegenüber statistischen
Methoden diesen Aufwand jedoch nicht.