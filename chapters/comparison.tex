%% ==============================
\chapter{Vergleich der Methoden}
\label{ch:Vergleich}
%% ==============================

In \chapref{ch:Dimensionsreduktion} haben wir grundlegende Begriffe geklärt und in
\chapref{ch:MethodenDerDimRed} sechs Methoden der Dimensionsreduktion näher betrachtet. Im jetzigen
Kapitel möchten wir die traditionellen Methoden aus \secref{ch:MethodenDerDimRed:traditionell} mit
den modernen Methoden aus \secref{ch:MethodenDerDimRed:modern} vergleichen. Dazu gehen wir in
\secref{ch:Vergleich:sec:Methodik} auf die Methodik ein, wobei hier unterschiedliche
Qualitätskriterien und Methoden zur Schätzung der intrinsischen Dimension betrachtet werden. In
\secref{ch:Vergleich:sec:VerwendeteDatensaetze} wird auf die im Vergleich verwendeten Datensätzen
eingegangen und anschließend werden in \secref{ch:Vergleich:sec:Resultate} die Ergebnisse des
empirischen Vergleichs vorgestellt.

\section{Methodik}
\label{ch:Vergleich:sec:Methodik}

In diesem Abschnitt wird auf die Methodik des Vergleichs der Dimensionsreduktionsmethoden
eingegangen. Dazu werden in \subsecref{ch:Vergleich:sec:Methodik:subsec:Qualitaetskriterien} die
hier verwendeten Qualitätskriterien einer Dimensionsreduktion genauer betrachtet. Des Weiteren wird
in \subsecref{ch:Vergleich:sec:Methodik:subsec:SchaetzenDerIntrinsischenDim} kurz die Schätzung der
intrinsischen Dimension behandelt.\todo{hier noch kurz auf das allgemeine Setup eingehen und PCA-AE
	Vergleich erwähnen} \idea{Vergleich von PCA mit Autoencoder

	Hier könnte man vor allem untersuchen, inwiefern ein Autoencoder mit linearen Akt.Funktionen PCA
	identisch ist, sowie schrittweise Nichtlinearität hinzunehmen }
\subsection{Qualitätskriterien der Dimensionsreduktion}
\label{ch:Vergleich:sec:Methodik:subsec:Qualitaetskriterien}
Trotz des immensen Forschungsinteresses für Methoden der Dimensionsreduktion, sind Qualitätskriterien, die die Güte einer Dimensionsreduktion beschreiben, vergleichsweise wenig erforscht. Deshalb gibt es in der Literatur keine eindeutige Kennzahl, die bei einem Vergleich von Dimensionsreduktionsmethoden standardmäßig eingesetzt wird \parencite[vgl.][1 -- 2]{Lee.2009}. Stattdessen bedient man sich mehrerer Kennzahlen, die
unterschiedliche Dinge bestrafen und versucht so die Stärken und Schwächen einer
Dimensionsreduktionsmethode zu erkennen \parencite[486]{Venna.2001}.

\ldots
\todo{hier noch auf den naheliegenden Rekonstruktionsfehler eingehen + erklären wieso er nicht sehr aussagekräftig ist}

Im ausführlichen Benchmark von \textcite{vanderMaaten.2009} wird auf den Generalisierungsfehler
eines 1-Nächste-Nachbar Klassifikators, sowie auf die zwei Kennzahlen
\newterm{Vertrauenswürdigkeit} (engl. \textit{Trustworthiness}) und \newterm{Kontinuität} (engl.
\textit{Continuity}) \parencites{Venna.2001}{Venna.2006} gesetzt. Daneben gibt es noch viele weitere rangbasierte
Qualitätskriterien, welche einheitlich durch die sogenannte \newterm{Co-Ranking Matrix} ausgedrückt
werden können. Ebenso können die Vertrauenswürdigkeit und Kontinuität über die Co-Ranking Matrix
berechnet werden. Für eine ausführliche Behandlung dessen wird auf \textcite{Lee.2009} verwiesen.
Im Folgenden werden die in dieser Arbeit verwendeten Qualitätskriterien kurz vorgestellt.

\subsubsection{Vertrauenswürdigkeit und Kontinuität}
Diese beiden Kennzahlen basieren auf der Idee des Erhalts von Nachbarschaften (engl.
\textit{neighborhood preservation}) einer Dimensionsreduktion. Sie bilden also ab, wie gut die
lokale Struktur erhalten wird. Eine $K$-Nachbarschaft $\set{M}_i(K)$ eines Punktes $\vect{x}_i$ ist
definiert als die Menge der Indizes\unsure{Nachbarschaft als Menge der Indizes oder Vektoren
	definieren? je nachdem muss ich dann auch unten die Mengen U und V anpassen} der $K$-nächsten
Punkte zu $\vect{x}_i$ ($i = 1, \ldots, n$). Analog kann die $K$-Nachbarschaft
$\widetilde{\set{M}}_i(K)$ des dazugehörigen niedrigdimensionalen Punktes $\vect{y}_i$ definiert
werden. Diese Nachbarschaft wird in einer Dimensionsreduktion erhalten, wenn $\set{M}_i(K) =
	\widetilde{\set{M}}_i(K)$, das heißt die Nachbarschaften bleiben von der Dimensionsreduktion
unverändert.

Zum einen kann es nun passieren, dass Punkte, die \textit{vor} der Projektion weit weg voneinander
lagen, \textit{nach} der Projektion aber nah beieinander sind. Mit anderen Worten können Punkte,
die eigentlich unterschiedlich sind, nun ähnlich erscheinen. Aus diesem Grund sagt man, dass die
Vertrauenswürdigkeit der Dimensionsreduktion niedrig ist. Zum anderen ist der gegenteilige Fall
möglich. Nah beieinander liegende Punkte sind nach der Projektion weit weg voneinander. Dies
reduziert die Kontinuität einer Dimensionsreduktion \parencite[486 -- 487]{Venna.2001}.

Formal definiert man zusätzlich zu den Nachbarschaftsmengen von oben die beiden Mengen
$\set{U}_i(K)$ und $\set{V}_i(K)$ wie folgt:
\begin{gather}
	\set{U}_i(K) =  \left\{ j \in \N \mid j \notin \set{M}_i(K) \land j \in \widetilde{\set{M}}_i(K) \right\} \\
	\set{V}_i(K) =  \left\{ j \in \N \mid j \in \set{M}_i(K) \land j \notin \widetilde{\set{M}}_i(K) \right\}
\end{gather}
Diese beiden Mengen bilden lediglich die zwei intuitiv besprochenen Fälle im vorherigen Absatz mathematisch ab. Hierbei entspricht $\set{U}_i(K)$ dem ersten und $\set{V}_i(K)$ dem zweiten Fall.
Damit kann die Vertrauenswürdigkeit $T(K)$ als
\begin{equation}
	T(K) = 1 - \frac{2}{nK(2n - 3K - 1)} \sum_{i = 1}^{n}\sum_{j \in \set{U}_i(K) } \left( \tilde{r}(i, j) - K \right)
\end{equation}
definiert werden \parencite[487]{Venna.2001}, wobei $\tilde{r}(i, j)$ den Rang von des niedrigdimensionalen Vektors
$\vect{y}_j$ bezeichnet, wenn die Datenpunkte absteigend nach der euklidischen Distanz von
$\vect{y}_i$ geordnet sind. Der Term vor der Summation skaliert das Qualitätskriterium so, dass $0
	\leq T(K) \leq 1$ gilt.\footnote{Dies gilt nur für den Fall, dass $k < n/2$ gilt.} Ein Wert von
$T(K) = 1­$ spricht für eine hohe Vertrauenswürdigkeit.

Analog wird die Kontinuität $C(K)$ über $\set{V}_i(K)$ wie folgt definiert:
\begin{equation}
	C(K) = 1 - \frac{2}{nK(2n - 3K - 1)} \sum_{i = 1}^{n}\sum_{j \in \set{V}_i(K) } \left( r(i, j) - K \right)
\end{equation}
wobei $r(i, j)$ nun den Rang zwischen den Datenpunkten in der hochdimensionalen Repräsentation bezeichnet \parencite[487]{Venna.2001}. Auch hier gilt $0 \leq C(K) \leq 1$ und höher ist besser. Die Kontinuität
misst also, wie gut die ursprünglichen Nachbarschaften erhalten werden. \idea{Man könnte die
	Gesamtqualität dann auch als den Mittelwert der beiden Kennzahlen T\&C definieren \parencite[1435]{Lee.2009}}

\subsubsection{Die Co-Ranking Matrix}
Ein Eintrag $q_{kl}$ der Co-Ranking Matrix $\mat{Q}$ ist die Anzahl der Paare von Datenpunkten $(i,
	j)$, die den Rang $r(i, j) = k$ in der hochdimensionalen und den Rang $\tilde{r}(i, j) = l$ in der
niedrigdimensionalen Repräsentation haben.

\idea{Welche Gütemaße, die auf dieser Matrix basieren verwende ich?
	Diese werden in \textcite{Lee.2009} vorgestellt, darunter
	\begin{enumerate}
		\item local continuity meta criterion (LCMC)
		\item Mean relative Rank error (ähnliche Idee wie T\&C)
		\item andere Kennzahlen, die jeweils unterschiedliche Teile der Co-Ranking Matrix betrachten. Um diese zu
		      verstehen, muss man noch weitere Begriffe wie (milde, harte) ($k$)-Intrusionen und Extrusionen
		      erklären. Kann dann relativ lang werden dieser Abschnitt
	\end{enumerate}
	LCMC fokussiert mehr darauf was in einer Projektion gut läuft (T\&C fokussieren auf das was schlecht läuft). Und es hat den Vorteil nur ein einziger Skalar zu sein.
}

\subsection{Schätzen der intrinsischen Dimension}
\label{ch:Vergleich:sec:Methodik:subsec:SchaetzenDerIntrinsischenDim}

Bis jetzt wurde immer angenommen, dass die intrinsische Dimension $d$ der Daten bekannt ist, da die
meisten Dimensionsreduktionsmethoden die intrinsische Dimension nicht selbst berechnen. Das
Schätzen der intrinsischen Dimension ist also ein nicht unwichtiges Teilproblem der
Dimensionsreduktion, da eine Unterschätzung von $d$ dazu führt, dass relevante Strukturen
zwangsweise verloren gehen \parencite[1]{Levina.2004}. Erschwert wird dieses Problem durch die Tatsache, dass es sehr viele
Definitionen der intrinsischen Dimension und damit auch sehr viele unterschiedliche Schätzer der
intrinsischen Dimension gibt. Im Folgenden soll ein kurzer Überblick verschafft werden, jedoch geht
eine detaillierte Behandlung der Schätzung über den Rahmen dieser Arbeit hinaus. Daher wird für
einen detaillierten systematischen Überblick und Vergleich dieser Schätzer auf
\textcites{Campadelli.2015}{Bac.2021}{Verveer.1995} verwiesen.

In \secref{ch:Dimensionsreduktion:MannigfaltigkeitenIntrinsDim} haben wir die \textit{topologische
	Dimension} kennengelernt, welche sich in der Literatur zur Strukturerkennung durchgesetzt hat \parencite[1]{Campadelli.2015}. Allerdings bringt diese Definition praktische Schwierigkeiten mit sich,
weswegen die meisten Schätzer der intrinsischen Dimension auf der damit verwandten
\textit{fraktalen Dimension} wie zum Beispiel der Schätzer der Korrelationsdimension \parencite{Camastra.2002} basieren. Daneben gibt es noch Nächste-Nachbar-basierte Schätzer \parencite[1]{Campadelli.2015}. Zu dieser Kategorie gehört auch der weit verbreitete und in dieser
Arbeit verwendete \newterm{Maximum Likelihood Schätzer} von \textcite{Levina.2004}. Diese Schätzer
betrachten die Verteilung der Nachbarschaft eines Punktes $\rvect{x}$ als Funktion der
intrinsischen Dimension, üblicherweise innerhalb einer kleinen Kugel um $\rvect{x}$
\Parencite[8]{Campadelli.2015}. Der Maximum Likelihood Schätzer nimmt konkret an, dass die
Beobachtungen, die in einer solchen Kugel liegen, einem homogenen Poisson-(Zähl-)Prozess folgen
\Parencite[2]{Levina.2004}. Dieser Prozess hängt von $d$ ab, weshalb mittels der Maximum Likelihood
Methode ein Schätzwert $\estNormal{d}$ für einen fixen Punkt $\rvect{x}_i$ als
\begin{equation}
	\estNormal{d}_K(\vect{x}_i) = \left( \frac{1}{K - 1} \sum_{j=1}^{K - 1} \log \frac{T_K(\vect{x}_i)}{T_j(\vect{x}_i)} \right)^{-1}
\end{equation}
berechnet werden kann \Parencite[4]{Levina.2004}. Hierbei ist $K$ die Anzahl der nächste Nachbarn und $T_{K}(\vect{x}_i)$ die euklidische Distanz von $\vect{x}_i$ zu seinem $K$-ten Nachbar. Dies ist jedoch nur eine lokale Schätzung für einen fixen Punkt $\vect{x}_i$. Um eine globale Schätzung zu erhalten, wird der Mittelwert über alle Beobachtungen für mehrere Werte von $K$ gebildet.
\section{Verwendete Datensätze}
\label{ch:Vergleich:sec:VerwendeteDatensaetze}
\idea{Es gibt viele Standard-Benchmark Datensätze die in anderen Arbeiten verwendet werden. Vlt ist es sinnvoll, 1-2 davon auch hier reinzunehmen für die Vergleichbarkeit, und dann 1-2 neue Datensätze,

	insgesamt vlt 2 künstliche Datensätze und 4 real-world Datensätze }
\section{Resultate}
\label{ch:Vergleich:sec:Resultate}
