%% ==============================
\chapter{Einleitung}
\label{ch:Enleitung}
%% ==============================

Viele interessante Anwendungsgebiete für Machine Learning umfassen eine große Anzahl an Merkmalen,
wie bei der Analyse von Genausprägungen mit tausenden von Genen \parencite{Parmigiani.2003} oder der Verarbeitung von Bild- oder Videodaten mit vielen Pixeln. Gerade
bei Bildern oder Videos besteht zum Beispiel in angrenzenden Pixeln viel Redundanz. Diese große
Anzahl an Merkmalen, das heißt die hohe Dimensionalität der Daten, bereitet vielen Algorithmen
Probleme -- Probleme, die unter der Ausnutzung der Abhängigkeitsstrukturen möglicherweise vermieden
werden können. Die mathematischen und statistischen Probleme, die bei Datensätzen hoher
Dimensionalität auftreten, werden unter dem \newterm{Fluch der Dimensionalität} \parencite{Aggarwal.2001} zusammengefasst. Es treten jedoch nicht nur algorithmische, sondern auch
praktische Probleme auf, da Datensätze mit vielen Stichproben und zusätzlich hoher Dimension eine
enorme Speicherkapazität und Rechenleistung benötigen können.
% \footnote{Eine Teilmenge des Datensatzes \href{https://laion.ai/blog/laion-5b/}{LAION-5B} umfasst circa 5,85 Milliarden Text-Bilder-Paare, wobei
% 	der Datensatz bei einer Bildgröße von $3 \times 384 \times 384$ haben. Dies entspricht einer Dimensionalität
% 	von 442 368. In dieser Auflösung werden 240 TeraByte Speicherplatz benötigt.}
% Ein Teil dieses Datensatzes wurde
% für das Trainieren von Stable Diffusion \addref verwendet. Dies ist ein \textit{latent diffusion model} oder mit anderen Worten:
% Stable Diffusion arbeitet nicht in der hochdimensionalen Auflösung sondern in der latenten
% Repräsentation eines Bildes, das eine deutlich niedrigere Auflösung besitzt und macht so das
% effiziente Trainieren auf einem solch großen Datensatz überhaupt erst möglich. Auch wenn LAION-5B
% ein extrem großer Datensatz ist, so sind auch schon kleinere Datensätze ein Problem für
% herkömmliche Rechner aufgrund von Beschränkungen des Arbeitsspeichers und der Rechenleistung.
Diesen und noch einigen anderen Problemen versucht die Dimensionsreduktion entgegenzuwirken, indem
der Datensatz auf eine kleinere Dimension abgebildet wird. Man erhofft sich, dass der
Informationsgehalt der Daten nach der Transformation immer noch hoch ist und damit die Redundanz
reduziert wurde. Häufig wird die Dimensionsreduktion als ein Vorverabeitungsschritt eingesetzt,
woraufhin dann der eigentliche Klassifikator oder Regressor trainiert wird. In diesen Fällen sind
die Algorithmen teilweise stabiler, da durch die Dimensionsreduktion unrepräsentative Merkmale
(\textit{noise}) entfernt wurden und somit die Gefahr des \textit{Overfitting} geringer ist \parencites[siehe]{Plastria.2008}{MustafaAbdulSalam.2021}. Die Dimensionsreduktion ist darüber hinaus
für die Datenvisualisierung unumgänglich, da hochdimensionale natürliche Datensätze so auf zwei
oder drei Dimensionen für die Veranschaulichung reduziert werden können.

Über die Zeit wurden deshalb eine Vielzahl von Dimensionsreduktionsmethoden entwickelt; für eine Übersicht wird auf \textcites{Burges.2009}{Sarveniazi.2014}{Sorzano.2014}{Cunningham.2014}{Lee.2007} verwiesen. Die große Auswahl an verschiedensten Methoden macht die Selektion der richtigen Methode für eine spezifische Aufgabe allerdings schwer. \Textcite{vanderMaaten.2009} hat dazu zwölf nichtlineare Methoden untersucht und mit der Hauptkomponentenanalyse verglichen.\rewrite{hier mehr dzau schreiben}

In jüngster Zeit wurden entscheidende Fortschritte in der Spracherkennung mit großen Sprachmodellen
wie \textit{Whisper} \parencite{Radford.2022} und auf dem Gebiet der \textit{Computer Vision} mit Text-zu-Bilder-Modellen
wie \textit{Stable Diffusion} \parencite{Rombach.2021}. Diese Modelle sind Varianten von neuronalen Netzen und damit
\enquote{moderne} Machine Learning Methoden. Dabei wird ein möglichst großer Datensatz für das
Trainieren der Modelle verwendet und man erhofft sich, dass diese neuronalen Netze mit ihrer
Eigenschaft der universalen Approximationsfunktion \parencites[194 -- 197]{Goodfellow.2016}{Hornik.1989} gute Ergebnisse erzielen. Es stellt sich also die
Frage, ob diese \enquote{modernen} Machine Learning Methoden gegenüber traditionellen und
vergleichsweise alten statistischen Methoden auf dem Gebiet der Dimensionsreduktion einen Vorteil
durch die erhöhte Flexibilität erzielen können. Dazu werden drei statistische und zwei Machine
Learning Methoden genauer betrachtet. Die hier vorgestellten Methoden sind die weitverbreitete
\newterm{Hauptkomponentenanalyse} (engl. \textit{Principal Component Analysis} (PCA)), die
\newterm{Kernel PCA} sowie das \newterm{Locally Linear Embedding} als Vertreter der statistischen
Methoden (\secref{ch:MethodenDerDimRed:statistisch}). Der \newterm{Autoencoder} in der klassischen
Form und eine Variante davon, der \newterm{Contractive Autoencoder} werden als Vertreter der
Machine Learning Ansätze (\secref{ch:MethodenDerDimRed:modern}) vorgestellt. Vor diesem Hintergrund
soll untersucht werden, ob Machine Learning Methoden traditionelle statistische Methoden auf dem
Gebiet der Dimensionsreduktion übertreffen. Mehrere Veröffentlichungen zur Relation zwischen der
Hauptkomponentenanalyse und den Autoencodern haben außerdem gezeigt, dass diese beiden Methoden
unter gewissen Annahmen ähnlich sind \parencites{Baldi.1989}{Bourlard.1988}{Plaut.2018}. Es soll daher empirisch herausgearbeitet werden,
inwiefern ein Autoencoder mit der Hauptkomponentenanalyse übereinstimmt.

Die Arbeit ist wie folgt aufgebaut: Zuerst wird in \chapref{ch:Dimensionsreduktion} die Grundlage
für das Verständnis der Dimensionsreduktionsmethoden gelegt, sowie wichtige Begrifflichkeiten
erläutert. Im darauffolgenden \chapref{ch:MethodenDerDimRed} werden dann die einzelnen
statistischen und Machine Learning Methoden vorgestellt. Anschließend werden die zwei Gruppen in
einem empirischen Vergleich in \chapref{ch:Vergleich} gegenübergestellt und abgeschlossen wird die
Arbeit mit einem Fazit in \chapref{ch:Schluss}.