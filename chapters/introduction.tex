%% ==============================
\chapter{Einleitung}
\label{ch:Enleitung}
%% ==============================

Viele interessante Anwendungsgebiete für Machine Learning umfassen oftmals eine sehr große Anzahl
an Merkmalen, wie es beispielsweise bei der Genexpressionsanalyse mit tausenden von Genen \parencite{Parmigiani.2003} oder der Verarbeitung von Bild-, Text- oder Videodaten der Fall ist. Diese
große Anzahl an Merkmalen, das heißt die hohe Dimensionalität der Daten, bereitet vielen
Algorithmen Probleme. Die mathematischen und statistischen Probleme, die bei Datensätzen hoher
Dimensionalität auftreten, werden unter dem \newterm{Fluch der Dimensionalität} \parencite{Aggarwal.2001} zusammengefasst. Hinzu kommt, dass die Anzahl der Merkmale auf diesen
Anwendungsgebieten oft die Stichprobengröße eines Datensatzes übersteigt. Beispielsweise gibt es in
klinischen Studien meistens nur wenige Teilnehmer, weshalb gerade medizinische Datensätze wie die
Magnetresonanztomographie (MRT) von diesem Problem betroffen sind. Es treten jedoch nicht nur
algorithmische, sondern auch praktische Probleme auf, da Datensätze mit sehr vielen Stichproben und
zusätzlich hoher Dimension eine immense Speicherkapazität und Rechenleistung benötigen
können.\footnote{Der Bilddatensatz LAION-5B umfasst circa 5,8 Milliarden Text-Bilder-Paare, wobei
	die Bilder eine Größe von $3 \times 384 \times 384$ haben. Dies entspricht einer Dimensionalität
	von 442 368. In dieser Auflösung werden 240 TeraByte Speicherplatz benötigt.}
% Ein Teil dieses Datensatzes wurde
% für das Trainieren von Stable Diffusion \addref verwendet. Dies ist ein \textit{latent diffusion model} oder mit anderen Worten:
% Stable Diffusion arbeitet nicht in der hochdimensionalen Auflösung sondern in der latenten
% Repräsentation eines Bildes, das eine deutlich niedrigere Auflösung besitzt und macht so das
% effiziente Trainieren auf einem solch großen Datensatz überhaupt erst möglich. Auch wenn LAION-5B
% ein extrem großer Datensatz ist, so sind auch schon kleinere Datensätze ein Problem für
% herkömmliche Rechner aufgrund von Beschränkungen des Arbeitsspeichers und der Rechenleistung.
Diesen und noch einigen anderen Problemen versucht die Dimensionsreduktion entgegenzuwirken, indem
der Datensatz auf eine kleinere Dimension abgebildet wird. Man erhofft sich, dass der
Informationsgehalt der Daten nach der Transformation immer noch hoch ist und damit die Redundanz
reduziert wurde. Häufig wird die Dimensionsreduktion als ein Vorverabeitungsschritt eingesetzt,
woraufhin dann der eigentliche Klassifikator oder Regressor trainiert wird. In diesen Fällen sind
die Algorithmen teilweise stabiler, da durch die Dimensionsreduktion unrepräsentative Merkmale
(\textit{noise}) entfernt wurden und somit die Gefahr des \textit{Overfitting} geringer ist \parencites[siehe]{Plastria.2008}{MustafaAbdulSalam.2021}. Die Dimensionsreduktion ist darüber hinaus
für die Datenvisualisierung unumgänglich, da hochdimensionale natürliche Datensätze so auf zwei
oder drei Dimensionen für die Veranschaulichung reduziert werden können.

Über die Zeit wurden deshalb eine Vielzahl von Dimensionsreduktionsmethoden entwickelt, für eine Übersicht wird auf \textcites{Burges.2009}{Sarveniazi.2014}{Sorzano.2014}{Cunningham.2014}{Lee.2007} verwiesen.
Der wohl bekannteste und immer noch vielfach verwendete Algorithmus ist die
Hauptkomponentenanalyse ist. Die große Auswahl an verschiedensten Methoden macht die Selektion der richtigen Methode für eine spezifische Aufgabe allerdings sehr schwer. \Textcite{vanderMaaten.2009} hat dazu zwölf nichtlineare Methoden untersucht und mit der Hauptkomponentenanalyse verglichen.

Das Ziel dieser Arbeit ist es herauszufinden, ob Machine Learning Methoden gegenüber statistischen
Methoden auf dem Gebiet der Dimensionsreduktion einen Vorteil durch die erhöhte Flexibilität
bringen können. Dazu werden drei statistische und zwei Machine Learning Methoden genauer
betrachtet. Die hier vorgestellten Methoden sind die weit verbreitete
\newterm{Hauptkomponentenanalyse} (engl. \textit{Principal Component Analysis} (PCA)), die
\newterm{Kernel PCA} sowie das \newterm{Locally Linear Embedding} als Vertreter der statistischen
Methoden (\secref{ch:MethodenDerDimRed:statistisch}). Der Autoencoder in der klassischen Form und
eine Variante davon, der \newterm{Contractive Autoencoder} werden als Vertreter der Machine
Learning Ansätze (\secref{ch:MethodenDerDimRed:modern}) vorgestellt. Vor diesem Hintergrund soll
untersucht werden, ob Machine Learning Methoden traditionelle statistische Methoden auf dem Gebiet
der Dimensionsreduktion übertreffen. Mehrere Veröffentlichungen zur Relation zwischen der
Hauptkomponentenanalyse und den Autoencodern haben außerdem gezeigt, dass die Ergebnisse dieser
beiden Methoden unter gewissen Annahmen sehr ähnlich sind \parencites{Baldi.1989}{Bourlard.1988}{Plaut.2018}. Es soll daher empirisch herausgearbeitet werden,
inwiefern ein Autoencoder mit der Hauptkomponentenanalyse übereinstimmt.

Die Arbeit ist wie folgt aufgebaut: Zuerst wird in \chapref{ch:Dimensionsreduktion} die Grundlage
für das Verständnis der Dimensionsreduktionsmethoden gelegt, sowie wichtige Begrifflichkeiten
erläutert. Im darauffolgenden \chapref{ch:MethodenDerDimRed} werden dann die einzelnen
statistischen und Machine Learning Methoden vorgestellt. Anschließend werden die zwei Gruppen in
einem empirischen Vergleich in \chapref{ch:Vergleich} gegenübergestellt und abgeschlossen wird die
Arbeit mit einem Fazit in \chapref{ch:Schluss}.