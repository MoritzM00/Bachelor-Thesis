%% ==============================
\chapter{Einleitung}
\label{ch:Enleitung}
%% ==============================

Die Analyse hochdimensionaler Daten wie z.B. die Analyse von Gendaten mit Ausprägungen von
tausenden Genen \parencite[siehe z.B.][]{Parmigiani.2003} oder der Analyse von Bilddaten ist geprägt von statistischen
Schwierigkeiten im Zusammenhang mit der hohen Dimensionalität der Daten. Anschaulich entspricht
eine Dimension dabei im Kontext der Gendaten gerade einer Ausprägung eines Genes. Da diese
Datensätze meist aufwärts von \num{10000} Genen umfassen, kann eine Stichprobe als ein
\num{10000}-dimensionaler Vektor mit Genausprägungen beschrieben werden. Bei einer derartig hohen
Dimension treten Probleme mit der Bestimmung der Ähnlichkeit zweier Vektoren auf, da u.a.
Distanzmaße nahezu bedeutungslos werden. Diese Herausforderungen von Daten hoher Dimensionalität
werden umgangssprachlich unter dem Begriff \newterm{Fluch der Dimensionalität} \parencite{Bellman.1961} zusammengefasst. Neben diesen statistischen Problemen treten aber auch
praktische Probleme auf, da Datensätze mit vielen Stichproben und zusätzlich hoher Dimension unter
Umständen eine enorme Speicherkapazität und Rechenleistung benötigen und damit zum Problem
herkömmlicher Rechner werden.
% \footnote{Eine englischsprachige Teilmenge des Datensatzes \href{https://laion.ai/blog/laion-5b/}{LAION-5B} umfasst circa 5,85 Milliarden Text-Bilder-Paare, wobei
% der Datensatz bei einer Bildgröße von $3 \times 384 \times 384$ haben. Dies entspricht einer Dimensionalität
% von 442 368. In dieser Auflösung werden 240 TeraByte Speicherplatz benötigt.}

Möglicherweise ist jedoch nur eine deutlich geringere Anzahl an \newterm{Merkmalen} (z.B. Gene)
relevant für die Analyse, da Redundanz im Informationsgehalt der ursprünglichen Repräsentation
besteht. Bei Bilddaten ist dies bspw. in angrenzenden Pixeln der Fall, da sich hier die Intensität
der Pixel oft nur an wenigen Stellen im Bild ändert. Die Idee der Dimensionsreduktion ist es daher,
eine neue sog. \newterm{latente Repräsentation} der Daten zu finden, die möglichst viel Information
erhält und dabei Redundanz eliminiert. Damit kann ein hochdimensionaler Vektor auf einen Vektor von
niedrigerer Dimension reduziert werden, womit sowohl die benötigte Speicher- als auch
Rechenkapazität verringert und der Fluch der Dimensionalität abgeschwächt wird. Neben der
Transformation auf neue Merkmale kann auch lediglich eine Selektion der ursprünglichen Merkmale
durchgeführt werden. Dieser Fall gehört zu einem Teilgebiet der Merkmalsextraktion \parencite{Guyon.2006} und wird in dieser Arbeit nicht weiter behandelt. Die Dimensionsreduktion kann
ebenfalls als ein Werkzeug der Merkmalsextraktion aufgefasst werden \parencite[3]{Guyon.2006b}. In dieser Sichtweise wird die Dimensionsreduktion als ein
Vorverarbeitungsschritt eingesetzt, wobei anschließend der eigentliche Schätzer (z.B. ein
Klassifikator) auf der latenten Repräsentation trainiert wird. Insgesamt erhofft man sich durch
eine vorhergehende Extraktion der relevanten Merkmale einen Schätzer, der robuster gegenüber
Rauschen ist und weniger zu \newterm{Overfitting} neigt \parencites[siehe][]{Plastria.2008}{MustafaAbdulSalam.2021}, d.h. besser auf ungesehene Daten
generalisiert.

% Ein Teil dieses Datensatzes wurde
% für das Trainieren von Stable Diffusion \addref verwendet. Dies ist ein \textit{latent diffusion model} oder mit anderen Worten:
% Stable Diffusion arbeitet nicht in der hochdimensionalen Auflösung sondern in der latenten
% Repräsentation eines Bildes, das eine deutlich niedrigere Auflösung besitzt und macht so das
% effiziente Trainieren auf einem solch großen Datensatz überhaupt erst möglich. Auch wenn LAION-5B
% ein extrem großer Datensatz ist, so sind auch schon kleinere Datensätze ein Problem für
% herkömmliche Rechner aufgrund von Beschränkungen des Arbeitsspeichers und der Rechenleistung.

Über die Zeit wurden daher eine Vielzahl von Dimensionsreduktionsmethoden entwickelt \parencite[siehe z.B.][]{Lee.2007} -- die große Auswahl macht die Selektion der richtigen Methode für
eine spezifische Aufgabe allerdings nicht einfacher. Einer der umfassendsten Vergleiche von
Methoden der Dimensionsreduktion wurde von \Textcite{vanderMaaten.2009} durchgeführt. Die Autoren
haben zwölf nichtlineare Methoden untersucht und diese mit der Principal Component Analysis
verglichen. Sie kommen zu dem Ergebnis, dass nichtlineare Methoden die Principal Component Analysis
auf künstlichen Datensätzen übertreffen können, dies aber bei natürlichen Datensätzen generell
nicht zutrifft. In der Arbeit von \textcite{RohanPandit.2016} wurde dieses Ergebnis auf Daten von
Proteinstrukturen bestätigt. \Textcite{Gracia.2014} stellen eine Methodik vor, die die Methoden
nach dem Qualitätsverlust der Dimensionsreduktion evaluiert. Dazu wird eine Übersicht über einige
Qualitätskriterien gegeben und die Methodik in einem Vergleich von zwölf Methoden auf zwölf
Datensätzen angewandt. Der Fokus liegt jedoch mehr auf dem Vergleich der unterschiedlichen
Qualitätskriterien und der vorgestellten Methodik. Dasselbe trifft auf die Arbeit von
\textcite{Lee.2009} zu, in der rangbasierte Qualitätskriterien in einem einheitlichen Framework
zusammengefasst werden. Daneben gibt es noch einige weitere Arbeiten, die sich lediglich mit
theoretischen Eigenschaften der Methoden befassen und diese nicht empirisch miteinander vergleichen \parencites{Cunningham.2014}{Sorzano.2014}{Lee.2007}{Sarveniazi.2014}{Burges.2009b}.

Die in jüngster Zeit gemachten Fortschritte in der automatischen Spracherkennung mit Sprachmodellen
wie \textit{Whisper} \parencite{Radford.2022} und auf dem Gebiet der \textit{Computer Vision} mit Text-zu-Bilder-Modellen
wie \textit{Stable Diffusion} \parencite{Rombach.2021} haben den Fokus in der Forschung immer mehr Richtung \textit{Deep Learning}
gerichtet. Diese Modelle sind Varianten von neuronalen Netzen und damit \enquote{moderne} Machine
Learning Methoden. Dabei wird ein möglichst großer Datensatz für das Trainieren der Modelle
verwendet und man erhofft sich, dass diese neuronalen Netze mit ihrer Eigenschaft der universalen
Approximationsfunktion \parencites[194 -- 197]{Goodfellow.2016}{Hornik.1989} gute Ergebnisse erzielen. Fraglich ist, ob diese
modernen Machine Learning Methoden auch auf dem Gebiet der Dimensionsreduktion die traditionellen
und vergleichsweise alten statistischen Methoden ersetzen können. Um dieser Frage etwas näher zu
kommen, werden drei statistische und zwei Machine Learning Methoden genauer betrachtet und in einem
empirischen Vergleich auf künstlichen und natürlichen Datensätzen gegenübergestellt. Die hier
vorgestellten Methoden sind die weitverbreitete \newterm{Principal Component Analysis}, die
\newterm{Kernel Principal Component Analysis} sowie \newterm{Locally Linear Embedding} als
Vertreter der statistischen Methoden (\secref{ch:MethodenDerDimRed:statistisch}). Der
\newterm{Autoencoder} und eine Variante in Form des \newterm{Contractive Autoencoders} werden als
Vertreter der Machine Learning Ansätze (\secref{ch:MethodenDerDimRed:modern}) vorgestellt. Darüber
hinaus haben mehrere Veröffentlichungen zur Relation der Principal Component Analysis und
Autoencodern gezeigt, dass diese beiden Methoden unter gewissen Annahmen ähnlich sind \parencites{Baldi.1989}{Bourlard.1988}{Plaut.2018}. Es soll daher empirisch herausgearbeitet werden,
inwiefern ein Autoencoder mit der Principal Component Analysis übereinstimmt.

Die Arbeit ist wie folgt aufgebaut: Zuerst wird in \chapref{ch:Dimensionsreduktion} die Grundlage
für das Verständnis der Dimensionsreduktionsmethoden gelegt, sowie wichtige Begrifflichkeiten
erläutert. Im darauf folgenden \chapref{ch:MethodenDerDimRed} werden dann die einzelnen
statistischen und Machine Learning Methoden vorgestellt. Anschließend werden die zwei Gruppen in
einem empirischen Vergleich in \chapref{ch:Vergleich} gegenübergestellt und abgeschlossen wird die
Arbeit mit einem Fazit in \chapref{ch:Schluss}.