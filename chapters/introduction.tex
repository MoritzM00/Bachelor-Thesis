%% ==============================
\chapter{Einleitung}
\label{ch:Enleitung}
%% ==============================

Viele interessante Anwendungsgebiete für Machine Learning umfassen eine große Anzahl an Merkmalen,
wie bei der Analyse von Genausprägungen mit tausenden von Genen \parencite{Parmigiani.2003} oder der Verarbeitung von Bild- oder Videodaten mit vielen Pixeln. Gerade
bei Bildern oder Videos besteht zum Beispiel in angrenzenden Pixeln viel Redundanz. Diese große
Anzahl an Merkmalen, das heißt die hohe Dimensionalität der Daten, bereitet vielen Algorithmen
Probleme -- Probleme, die unter der Ausnutzung der Abhängigkeitsstrukturen möglicherweise vermieden
werden können. Die mathematischen und statistischen Probleme, die bei Datensätzen hoher
Dimensionalität auftreten, werden unter dem \newterm{Fluch der Dimensionalität} \parencite{Aggarwal.2001} zusammengefasst. Es treten jedoch nicht nur algorithmische, sondern auch
praktische Probleme auf, da Datensätze mit vielen Stichproben und zusätzlich hoher Dimension eine
enorme Speicherkapazität und Rechenleistung benötigen können.
% \footnote{Eine Teilmenge des Datensatzes \href{https://laion.ai/blog/laion-5b/}{LAION-5B} umfasst circa 5,85 Milliarden Text-Bilder-Paare, wobei
% 	der Datensatz bei einer Bildgröße von $3 \times 384 \times 384$ haben. Dies entspricht einer Dimensionalität
% 	von 442 368. In dieser Auflösung werden 240 TeraByte Speicherplatz benötigt.}
% Ein Teil dieses Datensatzes wurde
% für das Trainieren von Stable Diffusion \addref verwendet. Dies ist ein \textit{latent diffusion model} oder mit anderen Worten:
% Stable Diffusion arbeitet nicht in der hochdimensionalen Auflösung sondern in der latenten
% Repräsentation eines Bildes, das eine deutlich niedrigere Auflösung besitzt und macht so das
% effiziente Trainieren auf einem solch großen Datensatz überhaupt erst möglich. Auch wenn LAION-5B
% ein extrem großer Datensatz ist, so sind auch schon kleinere Datensätze ein Problem für
% herkömmliche Rechner aufgrund von Beschränkungen des Arbeitsspeichers und der Rechenleistung.
Diesen und noch einigen anderen Problemen versucht die Dimensionsreduktion entgegenzuwirken, indem
der Datensatz auf eine kleinere Dimension abgebildet wird. Man erhofft sich, dass der
Informationsgehalt der Daten nach der Transformation immer noch hoch ist und damit die Redundanz
reduziert wurde. Häufig wird die Dimensionsreduktion als ein Vorverabeitungsschritt eingesetzt,
woraufhin dann der eigentliche Klassifikator oder Regressor trainiert wird. In diesen Fällen sind
die Algorithmen teilweise stabiler, da durch die Dimensionsreduktion unrepräsentative Merkmale
(\textit{noise}) entfernt wurden und somit die Gefahr des \textit{Overfitting} geringer ist \parencites[siehe]{Plastria.2008}{MustafaAbdulSalam.2021}. Die Dimensionsreduktion ist darüber hinaus
für die Datenvisualisierung unumgänglich, da hochdimensionale natürliche Datensätze so auf zwei
oder drei Dimensionen für die Veranschaulichung reduziert werden können.

Über die Zeit wurde daher eine Vielzahl von Dimensionsreduktionsmethoden entwickelt \parencite[siehe z.B.][]{Lee.2007} -- die große Auswahl macht die Selektion der richtigen Methode für
eine spezifische Aufgabe allerdings nicht einfacher. Daher wurden bereits mehrere Anstrengungen
gemacht, die Stärken und Schwächen einzelner Methoden herauszufinden. Einer der umfassendsten
Vergleiche von Methoden der Dimensionsreduktion wurde von \Textcite{vanderMaaten.2009}
durchgeführt. Die Autoren haben zwölf nichtlineare Methoden untersucht und mit der
Hauptkomponentenanalyse verglichen. Sie kommen zu dem Ergebnis, dass nichtlineare Methoden die
Hauptkomponentenanalyse auf künstlichen Datensätzen übertreffen können, dies aber bei natürlichen
Datensätzen generell nicht zutrifft. In der Arbeit von \textcite{RohanPandit.2016} wird dieses
Ergebnis auf Daten von Proteinstrukturen bestätigt. \Textcite{Gracia.2014} stellen eine Methodik
vor, die die Methoden nach dem Qualitätsverlust der Dimensionsreduktion evaluiert. Dazu wird eine
Übersicht über einige Qualitätskriterien gegeben und die Methodik in einem Vergleich von zwölf
Methoden auf zwölf natürlichen Datensätzen angewandt. Der Fokus liegt hier jedoch mehr auf dem
Vergleich der unterschiedlichen Qualitätskriteria und der vorgestellten Methodik. Dasselbe trifft
auf die Arbeit von \textcite{Lee.2009} zu, in der rangbasierte Qualitätskriteria in einem
einheitlichen Framework zusammengefasst werden. Daneben gibt es noch einige weitere Arbeiten, die
sich lediglich mit theoretischen Eigenschaften der Methoden befassen und diese nicht empirisch
miteinander vergleichen \parencites{Cunningham.2014}{Sorzano.2014}{Lee.2007}{Sarveniazi.2014}{Burges.2009b}.

Die in jüngster Zeit gemachten Fortschritte in der automatischen Spracherkennung mit Sprachmodellen
wie \textit{Whisper} \parencite{Radford.2022} und auf dem Gebiet der \textit{Computer Vision} mit Text-zu-Bilder-Modellen
wie \textit{Stable Diffusion} \parencite{Rombach.2021} haben den Fokus in der Forschung immer mehr Richtung \textit{Deep Learning}
gerichtet. Diese Modelle sind Varianten von neuronalen Netzen und damit \enquote{moderne} Machine
Learning Methoden. Dabei wird ein möglichst großer Datensatz für das Trainieren der Modelle
verwendet und man erhofft sich, dass diese neuronalen Netze mit ihrer Eigenschaft der universalen
Approximationsfunktion \parencites[194 -- 197]{Goodfellow.2016}{Hornik.1989} gute Ergebnisse erzielen. Fraglich ist, ob diese
modernen Machine Learning Methoden auch auf dem Gebiet der Dimensionsreduktion die traditionellen
und vergleichsweise alten statistischen Methoden ersetzen können. Um dieser Frage etwas näher zu
kommen, werden drei statistische und zwei Machine Learning Methoden genauer betrachtet und in einem
empirischen Vergleich auf künstlichen und natürlichen Datensätzen gegenübergestellt. Die hier
vorgestellten Methoden sind die weitverbreitete \newterm{Hauptkomponentenanalyse} (engl.
\textit{Principal Component Analysis} (PCA)), die \newterm{Kernel PCA} sowie das \newterm{Locally
	Linear Embedding} als Vertreter der statistischen Methoden
(\secref{ch:MethodenDerDimRed:statistisch}). Der \newterm{Autoencoder} in der klassischen Form und
eine Variante davon, der \newterm{Contractive Autoencoder} werden als Vertreter der Machine
Learning Ansätze (\secref{ch:MethodenDerDimRed:modern}) vorgestellt. Mehrere Veröffentlichungen zur
Relation zwischen der Hauptkomponentenanalyse und Autoencodern haben außerdem gezeigt, dass diese
beiden Methoden unter gewissen Annahmen ähnlich sind \parencites{Baldi.1989}{Bourlard.1988}{Plaut.2018}. Es soll daher empirisch herausgearbeitet werden,
inwiefern ein Autoencoder mit der Hauptkomponentenanalyse übereinstimmt.

Die Arbeit ist wie folgt aufgebaut: Zuerst wird in \chapref{ch:Dimensionsreduktion} die Grundlage
für das Verständnis der Dimensionsreduktionsmethoden gelegt, sowie wichtige Begrifflichkeiten
erläutert. Im darauffolgenden \chapref{ch:MethodenDerDimRed} werden dann die einzelnen
statistischen und Machine Learning Methoden vorgestellt. Anschließend werden die zwei Gruppen in
einem empirischen Vergleich in \chapref{ch:Vergleich} gegenübergestellt und abgeschlossen wird die
Arbeit mit einem Fazit in \chapref{ch:Schluss}.