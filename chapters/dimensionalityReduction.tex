%% ==============================
\chapter{Dimensionsreduktion}
\label{ch:Dimensionsreduktion}
%% ==============================

Die Dimensionsreduktion hat im Kern das Ziel der Abbildung eines hochdimensionalen Datensatzes auf
eine niedrigdimensionale, sogenannte \newterm{latente} Repräsentation, wobei möglichst wenig
Information über die Daten verloren gehen soll.\addref Dies ist darin begründet, dass Daten oft nur
künstlich hochdimensional, also \textit{redundant} sind. Dies bedeutet, dass die Daten effizienter
über eine kleinere Menge von Merkmalen $y_1,\ldots,y_d$ ausgedrückt werden kann, als über die
ursprüngliche Repräsentation durch die Merkmale $x_1,\ldots,x_D$ mit $d < D$. Hierbei bezeichnet
$D$ die \newterm{extrinsische Dimension} des zugehörigen Ursprungsraumes $\mathcal{X}$ (welcher
üblicherweise dem $\real^D$ entspricht) und $d$ bezeichnet die \newterm{intrinsische Dimension} der
Daten. Die intrinsische Dimension wird teilweise auch als latente Dimension bezeichnet.

Formell wird die ursprüngliche (hochdimensionale) Repräsentation mit dem $D$-dimensionalen
Zufallsvektor $\rvect{x} = \tr{(x_1, \ldots, x_D)}$ und die latente (niedrigdimensionale)
Repräsentation mit dem $d$-dimensionalen Zufallsvektor $\rvect{y}$ gekennzeichnet. Liegt eine
konkrete Stichprobe vor, so werden die einzelnen Stichproben $\vect{x}_i, i = 1,\ldots,n$ als
Spaltenvektoren in der $n \times D$ Datenmatrix $\mat{X}$ angeordnet. Analog dazu werden die
transformierten Daten in der Matrix $\mat{Y} \in \real^{n \times d}$ angeordnet. Es wird
angenommen, dass die Datenmatrix $\mat{X}$ \textit{zentriert} ist. Dies kann jederzeit durch
Subtraktion des Erwartungswertes einer Variable $x_i$ von Spalte $i$ sichergestellt werden.

Nachdem nun grundlegende Terminologie und das Ziel der Dimensionsreduktion geklärt wurde, wird nun
noch auf einige weitere wichtige Ideen der Dimensionsreduktion eingegangen. Dazu wird in
\secref{ch:Dimensionsreduktion:FluchDerDim} der Fluch der Dimensionalität sowie in
\secref{ch:Dimensionsreduktion:MannigfaltigkeitenIntrinsDim} die Idee von Mannigfaltigkeiten und
der intrinsischen Dimension behandelt. In \secref{ch:Dimensionsreduktion:Ansaetze} werden kurz
verschiedene Ansätze zur Dimensionsreduktion vorgestellt und in
\secref{ch:Dimensionsreduktion:Merkmalsextrahierung} wird die Relation zum Gebiet der
Merksmalsextrahierung (engl. \textit{feature extraction}) besprochen.
\section{Der Fluch der Dimensionalität}
\label{ch:Dimensionsreduktion:FluchDerDim}

\section{Mannigfaltigkeiten und intrinsische Dimension}
\label{ch:Dimensionsreduktion:MannigfaltigkeitenIntrinsDim}

Wie eingangs besprochen wird bei hochdimensionalen Daten oft von Redundanz oder
Abhängigkeitsstrukturen in den Merkmalen ausgegangen. Eng damit verbunden ist die Idee, dass Daten
auf einer sogenannten \newterm{Mannigfaltigkeit} (engl. \textit{manifold}) liegen.
Dimensionsreduktionsmethoden, die auf dieser Idee basieren, gehören zu einem wichtigen Teilgebiet
der Dimensionreduktion: dem Erlernen von Mannigfaltigkeiten \parencite{Cayton.2005}. Motiviert werden diese Ansätze durch die Hypothese, dass reale
hochdimensionale Daten in sehr vielen Fällen auf einer in diesem hochdimensionalen Raum
\textit{eingebetteten} Mannigfaltigkeit $\mathcal{M}$ der Dimensionalität $d$ < $D$ liegen \parencite[vgl.][1]{Cayton.2005}. In diesem Abschnitt wird ein kurzer Überblick über den abstrakten
Begriff einer Mannigfaltigkeit gegeben, wodurch ein geometrischer Bezug der intrinsischen Dimension
hergestellt werden kann.

Eine $d$-dimensionale Mannigfaltigkeit $\mathcal{M}$ ist lokal \textit{homöomorph} zum $\real^d$,
das heißt $\mathcal{M}$ ähnelt \textit{lokal} dem $\real^d$ \parencite[3]{Lee.2011}. Das bedeutet, dass es für jeden Punkt $\vect{z} \in \mathcal{M}$ eine stetige
Abbildung $\phi: B_\epsilon(\vect{z}) \rightarrow \real^d$ gibt, deren Inverse ebenfalls stetig
ist. Hierbei ist $B_\epsilon(\vect{z})$ ein Ball mit Radius $\epsilon > 0$ um $\vect{z}$. $\phi$
heißt \textit{Karte} und die Gesamtheit aller Karten ergibt den \textit{Atlas} von $\mathcal{M}$ \parencite[4]{Cayton.2005}. Ein anschauliches Beispiel für eine (zweidimensionale) Mannigfaltigkeit ist
die Erdoberfläche. Lokal erscheint die Erdoberfläche flach, das heißt zweidimensional, eingebettet
ist sie aber in einem dreidimensionalen Raum. Eine Karte kann informell also genau wie eine
traditionelle Karte eines Teils der Erdoberfläche beschrieben werden. Bewegt man sich nun entlang
der Erdoberfläche, so muss die Karte irgendwann gewechselt werden. Dieser Übergang von zwei sich
überlappenden Karten wird durch den Homöomorphismus $\phi$ formalisiert. Betrachtet man alle Karten
von allen Teilen der Erdoberfläche ergibt dies den Atlas der Erde. Auf diese Weise kann man sich
ebenfalls die intrinsische Dimension vorstellen: Während die Erdoberfläche in einem
dreidimensionalen Raum eingebettet ist (extrinsische Dimension), so ist die zugrundeliegende Fläche
zweidimensional (intrinsische Dimension). Die intrinsische Dimension entspricht also in diesem
Kontext der Dimension der im $\real^D$ eingebetteten $d$-Mannigfaltigkeit $\mathcal{M}$. Auf eine
umfassende formale Definition wird an dieser Stelle jedoch verzichtet. Für eine mathematisch
korrekte Definition der hier genannten Begriffe aus der Topologie wird dazu auf
\textcites{Lee.2011}{Lee.2012} verwiesen.

Eine Mannigfaltigkeit kann darüberhinaus noch mit weiteren Eigenschaften versehen werden. Dazu
gehören glatte Mannigfaltigkeiten, auf denen Differentialrechnung möglich ist und ein sogenannter
Tangentialraum definiert werden kann. Außerdem kann zwischen zusammenhängenden und
nicht-zusammenhängenden Mannigfaltigkeiten unterschieden werden, wobei letztere aus mehreren
nicht-verbundenen Untermannigfaltigkeiten besteht. Im restlichen Teil dieser Arbeit wird mit einer
Mannigfaltigkeit Bezug auf eine im $\real^D$ eingebettete $d$-dimensionale Mannigfaltigkeit ohne
weitere Annahmen über die Struktur( glatt, zusammenhängend) genommen.

\section{Ansätze der Dimensionsreduktion}
\label{ch:Dimensionsreduktion:Ansaetze}
Methoden der Dimensionsreduktion können auf unterschiedlichste Weise in Kategorien eingeteilt werden. In diesem Abschnitt werden kurz die gängigsten Kategorisierungsmöglichkeiten vorgestellt. Dazu gehört die Unterteilung in (i) lineare und nichtlineare Methoden, (ii) konvexe und nicht-konvexe Methoden, sowie (iii) Distanz- und Topologie-erhaltende Methoden.

Eine simple und weitverbreitete Unterteilung ist die in lineare und nichtlineare Methoden. Während
lineare Methoden wie die Hauptkomponentenanalyse (\secref{ch:MethodenDerDimRed:statistisch:PCA})
nur lineare Zusammenhänge in den Daten erkennen können, sind nichtlineare Methoden in dieser
Hinsicht deutlich flexibler. Zu letzteren gehören beispielsweise die Kernel PCA
(\secref{ch:MethodenDerDimRed:statistisch:kPCA}) und Autoencoder
(\secref{ch:MethodenDerDimRed:ML:AE}).

\textbf{Konvexe} Methoden optimieren eine Zielfunktion, die nur globale Optima besitzt und dadurch nicht in suboptimalen Lösungen resultieren kann. Die Optimierung ist daher leichter, jedoch sind die möglichen Zielfunktionen auch deutlich eingeschränkter. Nicht-konvexe Methoden wie Autoencoder, Sammon Mapping oder Manifold Charting optimieren nicht-konvexe Zielfunktionen und können damit bei der Optimierung in suboptimalen lokalen Extrempunkten \enquote{steckenbleiben}. Die Optimierung erfolgt meist mittels des Gradientenabstiegsverfahrens (engl. \textit{gradient descent}) oder anderen mathmatischen Optimierungsverfahren.\addref

\textbf{Distanz-basierte Methoden} versuchen die paarweisen Distanzen in den Daten zu erhalten, womit die zugrundeliegende Struktur erhalten werden soll \parencite[3]{Gracia.2014}. Vertreter dieser Kategorie sind die (klassische) Multidimensionale
Skalierung \parencites{Kruskal.1964}{Cox.2008}, das Sammon Mapping \addref und die Curvilinear Component Analysis
\addref. Hierbei muss eine Distanz jedoch nicht die euklidische Distanz zwischen zwei Punkten sein.
Beispielweise ist die \newterm{geodätische Distanz} als die Distanz entlang der Mannigfaltigkeit
definiert, auf der die Punkte liegen. Mit dem Hintergrundwissen zu Mannigfaltigkeiten wird jedoch
deutlich, dass Distanzen vor allem bei nichtlinearen Mannigfaltigkeiten mit einer hohen Krümmung
irreführend sein können. Aber auch schon Mannigfaltigkeiten wie die Swiss Roll deuten hier auf
Probleme hin: Punkte die nah beieinander liegen müssen durch die Dimensionsreduktion
\enquote{entfaltet} werden, wodurch sie danach weiter weg voneinander liegen können aber die
Struktur der Mannigfaltigkeit trotzdem erhalten geblieben ist. Die geodätische Distanz wirkt diesem
Problem entgegen, ist aber nicht immer leicht zu berechnen. Daneben gibt es
\textbf{Topologie-erhaltende Methoden}, die gezielt die Topologie der Mannigfaltigkeit erhalten
wollen und dies durch geometrische Überlegungen erzielen \parencite[4]{Gracia.2014}. Zu dieser Kategorie gehört beispielsweise das Locally Linear Embedding
(\secref{ch:MethodenDerDimRed:statistisch:LLE}), die Self-Organizing Map \parencite{Kohonen.1990} oUniform Manifold Approximation and Projection (UMAP) \parencite{McInnes.2018}.

Andere Algorithmen, die in dieser Arbeit nicht vorgestellt werden können sind Isomap \parencite{Tenenbaum.2000}, Maximum Variance Unfolding (MVU) \parencite{Weinberger.2006}, Laplacian Eigenmaps, Diffusion Maps, Varianten von LLE wie Hessian LLE
oder Local Tangent Space Alignment (LTSA), sowie Manifold Charting und Local Linear
Charting.\addref

\section{Relation zur Merkmalsextrahierung}
\label{ch:Dimensionsreduktion:Merkmalsextrahierung}

Die Merkmalsextrahierung ist ein sehr verwandtes Gebiet zur Dimensionsreduktion, da oftmals
Methoden der Dimensionsreduktion für die Merkmalsextrahierung eingesetzt werden \parencite[3]{Guyon.2006b}. Das Ziel hierbei ist meistens, in einem Vorverarbeitungsschritt
\enquote{irrelevante} Merkmale durch eine Transformation der Merkmale zu entfernen und dann den
eigentlichen Schätzer (Klassifikator oder Regressor) auf den neuen Merkmalen zu trainieren.
Abzugrenzen davon ist ein Teilgebiet der Merkmalsextrahierung, nämlich die Merkmalsauswahl (engl.
\textit{feature selection}) \parencite{Blum.1997}. Hier wird keine Transformation der Merkmale durchgeführt, sondern nur eine
Teilmenge der verfügbaren Merkmale ausgewählt.

Es wird erhofft, dass der Klassifikator (oder Regressor) durch die Merkmalsextrahierung robuster
gegenüber Rauschen (engl. \textit{noise}) ist und teilweise sogar eine höhere Genauigkeit gegenüber
einem Klassifikator, der auf den ursprünglichen Merkmalen trainiert wurde, aufweist.

\ldots

Wie wir in der Einleitung in \chapref{ch:Enleitung} gesehen haben, gibt es viele Gründe, wieso man
eine Reduktion der Dimension eines Datensatzes erreichen möchte. Unter anderem können so sehr große
Datenmengen effizienter gespeichert werden, ohne dabei einen zu großen Informationsverlust zu
erleiden. Deswegen ist das Gebiet der Dimensionsreduktion ein immer noch aktuelles Forschungsthema,
weshalb mittlerweile auch eine ganze Reihe an verschiedensten Methoden zur Dimensionsreduktion zur
Verfügung stehen. Dem Anwender kann es mitunter schwerfallen, den richtigen Algorithmus
auszuwählen, da es laut dem dem \newterm{No Free Lunch Theorem} \parencite{Wolpert.1997} keinen \enquote{besten} Algorithmus für jede Situation gibt. Im nächsten
Schritt (\chapref{ch:MethodenDerDimRed}) werden einige dieser Algorithmen vorgestellt.