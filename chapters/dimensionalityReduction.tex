%% ==============================
\chapter{Dimensionsreduktion}
\label{ch:Dimensionsreduktion}
%% ==============================

Die Dimensionsreduktion hat im Kern das Ziel der Abbildung eines hochdimensionalen Datensatzes auf
eine niedrigdimensionale, sogenannte \newterm{latente} Repräsentation, wobei möglichst wenig
Information über die Daten verloren gehen soll.\addref Dies ist darin begründet, dass Daten oft nur
künstlich hochdimensional, also \textit{redundant} sind. Dies bedeutet, dass die Daten effizienter
über eine kleinere Menge von Merkmalen $y_1,\ldots,y_d$ ausgedrückt werden kann, als über die
ursprüngliche Repräsentation durch die Merkmale $x_1,\ldots,x_D$ mit $d < D$. Hierbei bezeichnet
$D$ die \newterm{extrinsische Dimension} des zugehörigen Ursprungsraumes $\mathcal{X}$ (welcher
üblicherweise dem $\real^D$ entspricht) und $d$ bezeichnet die \newterm{intrinsische Dimension} der
Daten. Die intrinsische Dimension wird teilweise auch als latente Dimension bezeichnet.

Formell wird die ursprüngliche (hochdimensionale) Repräsentation mit dem $D$-dimensionalen
Zufallsvektor $\rvect{x} = \tr{(x_1, \ldots, x_D)}$ und die latente (niedrigdimensionale)
Repräsentation mit dem $d$-dimensionalen Zufallsvektor $\rvect{y}$ gekennzeichnet. Liegt eine
konkrete Stichprobe vor, so werden die einzelnen Stichproben $\vect{x}_i, i = 1,\ldots,n$ als
Spaltenvektoren in der $n \times D$ Datenmatrix $\mat{X}$ angeordnet. Analog dazu werden die
transformierten Daten in der Matrix $\mat{Y} \in \real^{n \times d}$ angeordnet. Es wird
angenommen, dass die Datenmatrix $\mat{X}$ \textit{zentriert} ist. Dies kann jederzeit durch
Subtraktion des Erwartungswertes einer Variable $x_i$ von Spalte $i$ sichergestellt werden.

Nachdem nun grundlegende Terminologie und das Ziel der Dimensionsreduktion geklärt wurde, wird nun
noch auf einige weitere wichtige Ideen der Dimensionsreduktion eingegangen. Dazu wird in
\secref{ch:Dimensionsreduktion:FluchDerDim} der Fluch der Dimensionalität sowie in
\secref{ch:Dimensionsreduktion:MannigfaltigkeitenIntrinsDim} die Idee von Mannigfaltigkeiten und
der intrinsischen Dimension behandelt. In \secref{ch:Dimensionsreduktion:Ansaetze} werden kurz
verschiedene Ansätze zur Dimensionsreduktion vorgestellt und in
\secref{ch:Dimensionsreduktion:Merkmalsextrahierung} wird die Relation zum Gebiet der
Merksmalsextrahierung (engl. \textit{feature extraction}) besprochen.
\section{Der Fluch der Dimensionalität}
\label{ch:Dimensionsreduktion:FluchDerDim}

\section{Mannigfaltigkeiten und intrinsische Dimension}
\label{ch:Dimensionsreduktion:MannigfaltigkeitenIntrinsDim}

Wie eingangs besprochen wird bei hochdimensionalen Daten oft von Redundanz oder
Abhängigkeitsstrukturen in den Merkmalen ausgegangen. Eng damit verbunden ist die Idee, dass Daten
auf einer sogenannten \newterm{Mannigfaltigkeit} (engl. \textit{manifold}) liegen.
Dimensionsreduktionsmethoden, die auf dieser Idee basieren, gehören zu einem wichtigen Teilgebiet
der Dimensionreduktion: dem Erlernen von Mannigfaltigkeiten \parencite{Cayton.2005}. Motiviert werden diese Ansätze durch die Hypothese, dass reale
hochdimensionale Daten in sehr vielen Fällen auf einer in diesem hochdimensionalen Raum
\textit{eingebetteten} Mannigfaltigkeit $\mathcal{M}$ der Dimensionalität $d$ < $D$ liegen \parencite[vgl.][1]{Cayton.2005}. In diesem Abschnitt wird ein kurzer Überblick über den abstrakten
Begriff einer Mannigfaltigkeit gegeben, wodurch ein geometrischer Bezug der intrinsischen Dimension
hergestellt werden kann.

Eine $d$-dimensionale Mannigfaltigkeit $\mathcal{M}$ ist lokal \textit{homöomorph} zum $\real^d$,
das heißt $\mathcal{M}$ ähnelt \textit{lokal} dem $\real^d$ \parencite[3]{Lee.2011}. Das bedeutet, dass es für jeden Punkt $\vect{z} \in \mathcal{M}$ eine stetige
Abbildung $\phi: B_\epsilon(\vect{z}) \rightarrow \real^d$ gibt, deren Inverse ebenfalls stetig
ist. Hierbei ist $B_\epsilon(\vect{z})$ ein Ball mit Radius $\epsilon > 0$ um $\vect{z}$. $\phi$
heißt \textit{Karte} und die Gesamtheit aller Karten ergibt den \textit{Atlas} von $\mathcal{M}$ \parencite[4]{Cayton.2005}. Ein anschauliches Beispiel für eine (zweidimensionale) Mannigfaltigkeit ist
die Erdoberfläche. Lokal erscheint die Erdoberfläche flach, das heißt zweidimensional, eingebettet
ist sie aber in einem dreidimensionalen Raum. Eine Karte kann informell also genau wie eine
traditionelle Karte eines Teils der Erdoberfläche beschrieben werden. Bewegt man sich nun entlang
der Erdoberfläche, so muss die Karte irgendwann gewechselt werden. Dieser Übergang von zwei sich
überlappenden Karten wird durch den Homöomorphismus $\phi$ formalisiert. Betrachtet man alle Karten
von allen Teilen der Erdoberfläche ergibt dies den Atlas der Erde. Auf diese Weise kann man sich
ebenfalls die intrinsische Dimension vorstellen: Während die Erdoberfläche in einem
dreidimensionalen Raum eingebettet ist (extrinsische Dimension), so ist die zugrundeliegende Fläche
zweidimensional (intrinsische Dimension). Die intrinsische Dimension entspricht also in diesem
Kontext der Dimension der im $\real^D$ eingebetteten $d$-Mannigfaltigkeit $\mathcal{M}$. Auf eine
umfassende formale Definition wird an dieser Stelle jedoch verzichtet, da es über den Rahmen dieser
Arbeit hinausgehen würde. Für eine mathematisch korrekte Definition der hier genannten Begriffe aus
der Topologie wird dazu auf \textcites{Lee.2011}{Lee.2012} verwiesen.

\section{Ansätze der Dimensionsreduktion}
\label{ch:Dimensionsreduktion:Ansaetze}
\idea{generelle Ansätze grob formulieren, also nichtlinear vs. linear, topology vs. distance preservation, convex vs. non-convex}

Andere Algorithmen, die in dieser Arbeit nicht vorgestellt werden können sind Isomap \parencite{Tenenbaum.2000}, Maximum Variance Unfolding (MVU) \parencite{Weinberger.2006}, Multidimensional Scaling (MDS) \parencites{Kruskal.1964}{Cox.2008}

\section{Relation zur Merkmalsextrahierung}
\label{ch:Dimensionsreduktion:Merkmalsextrahierung}

Die Merkmalsextrahierung ist ein sehr verwandtes Gebiet zur Dimensionsreduktion, da oftmals
Methoden der Dimensionsreduktion für die Merkmalsextrahierung eingesetzt werden \parencite[3]{Guyon.2006b}. Das Ziel hierbei ist meistens, in einem Vorverarbeitungsschritt
\enquote{irrelevante} Merkmale durch eine Transformation der Merkmale zu entfernen und dann den
eigentlichen Schätzer (Klassifikator oder Regressor) auf den neuen Merkmalen zu trainieren.
Abzugrenzen davon ist ein Teilgebiet der Merkmalsextrahierung, nämlich die Merkmalsauswahl (engl.
\textit{feature selection}) \parencite{Blum.1997}. Hier wird keine Transformation der Merkmale durchgeführt, sondern nur eine
Teilmenge der verfügbaren Merkmale ausgewählt.

Es wird erhofft, dass der Klassifikator (oder Regressor) durch die Merkmalsextrahierung robuster
gegenüber Rauschen (engl. \textit{noise}) ist und teilweise sogar eine höhere Genauigkeit gegenüber
einem Klassifikator, der auf den ursprünglichen Merkmalen trainiert wurde, aufweist.

\ldots

Wie wir in der Einleitung in \chapref{ch:Enleitung} gesehen haben, gibt es viele Gründe, wieso man
eine Reduktion der Dimension eines Datensatzes erreichen möchte. Unter anderem können so sehr große
Datenmengen effizienter gespeichert werden, ohne dabei einen zu großen Informationsverlust zu
erleiden. Deswegen ist das Gebiet der Dimensionsreduktion ein immer noch aktuelles Forschungsthema,
weshalb mittlerweile auch eine ganze Reihe an verschiedensten Methoden zur Dimensionsreduktion zur
Verfügung stehen. Dem Anwender kann es mitunter schwerfallen, den richtigen Algorithmus
auszuwählen, da es laut dem dem \newterm{No Free Lunch Theorem} \parencite{Wolpert.1997} keinen \enquote{besten} Algorithmus für jede Situation gibt. Im nächsten
Schritt (\chapref{ch:MethodenDerDimRed}) werden einige dieser Algorithmen vorgestellt.