%% ==============================
\chapter{Methoden der Dimensionsreduktion}
\label{ch:MethodenDerDimRed}
%% ==============================

Wie bereits in der Einleitung erwähnt wurde, haben kürzlich vor allem verschiedenste Varianten von
neuronale Netzen einen hohen Grad an Aufmerksamkeit durch bemerkenswerte Errungenschaften in
Gebieten der automatischen Spracherkennung oder \textit{Computer Vision} erlangt. Fraglich ist, ob
diese neuen Algorithmen auch auf dem Bereich der Dimensionsreduktion einen entscheidenden
Fortschritt gemacht haben. Deshalb wird von der üblichen Unterteilung (siehe
\secref{ch:Dimensionsreduktion:Ansaetze}) der Methoden abgewichen. Stattdessen werden die Methoden
in dieser Arbeit in statistische Methoden und Machine Learning Ansätze unterteilt, um diese später
in \chapref{ch:Vergleich} gegenüberstellen zu können. Statistische Methoden haben eine solide
theoretische Fundierung und sind meist etablierte Algorithmen auf einem jeweiligen Gebiet, wie es
zum Beispiel mit der Principal Component Analysis
(\subsecref{ch:MethodenDerDimRed:statistisch:PCA}) der Fall ist. Machine Learning Methoden hingegen
versuchen mithilfe einer großen Menge an Trainingsdaten eine Funktion oder Verteilung zu lernen und
damit auf neue, ungesehene Daten zu generalisieren. Da hier oft nicht-konvexe Zielfunktionen
mittels mathematischer Optimierungsfunktionen optimiert werden, kann meistens keine
Optimalitätsgarantie bewiesen werden. Neuronale Netze sind hierfür ein Paradebeispiel und werden
deshalb in \secref{ch:MethodenDerDimRed:modern} eingehend behandelt.

Nachdem die Motivation und Idee in \chapref{ch:Enleitung}, sowie die Terminologie und mathematische
Fundierung für die Dimensionsreduktion in \chapref{ch:Dimensionsreduktion} besprochen wurde, können
nun einige konkrete Methoden der Dimensionsreduktion vorgestellt werden. Allerdings ist hier aus
jeder Kategorie nur eine kleine repräsentative Auswahl getroffen worden. Weitere wichtige
Algorithmen, die hier nicht behandelt werden können, wurden bereits in
\secref{ch:Dimensionsreduktion:Ansaetze} genannt.

\section{Statistische Methoden}
\label{ch:MethodenDerDimRed:statistisch}

Zu den statistischen Methoden, die in dieser Arbeit vorgestellt werden, gehören die
\newterm{Principal Component Analysis} (PCA) in \subsecref{ch:MethodenDerDimRed:statistisch:PCA}
und die nichtlineare Erweiterung \newterm{Kernel Principal Component Analysis} (Kernel PCA)
(\subsecref{ch:MethodenDerDimRed:statistisch:kPCA}). Außerdem wird das \newterm{Locally Linear
	Embedding} (LLE) in \subsecref{ch:MethodenDerDimRed:statistisch:LLE} als Vertreter der
statistischen Methoden genauer behandelt.

%% ==============================
\subsection{Principal Component Analysis}
\label{ch:MethodenDerDimRed:statistisch:PCA}
\nomenclature[Z]{PCA}{Principal Component Analysis}

Die Principal Component Analysis ist \textit{die} Methode der Dimensionsreduktion und wurde
erstmals von \textcite{Pearson.1901} und \textcite{Hotelling.1933} entwickelt. Trotz des
mittlerweile relativ hohen Alters ist die Principal Component Analysis immer noch aufgrund der
simplen Anwendbarkeit oft die erste Wahl einer Dimensionsreduktionsmethode. Im Folgenden wird auf
die zentrale Idee und Motivation der Principal Component Analysis, sowie die mathematischen
Herleitung der sogenannten \newterm{Hauptkomponenten} (engl. \textit{principal components})
eingegangen.

\subsubsection{Grundidee}
\label{ch:MethodenDerDimRed:statistisch:PCA:Grundidee}
Die zentrale Idee der Principal Component Analysis ist die Transformation des Koordinatensystems in einer möglichst \textit{verlustfreien} Art und Weise. Verlustfrei bedeutet im Kontext der Principal Component Analysis, dass möglichst viel Variation in den Daten erhalten bleiben soll \parencite[vgl.][1]{Jolliffe.2002}. Das Ziel ist also, die Richtungen der größten Varianz in den Daten
zu finden, da diese den höchsten Informationsgehalt besitzen. Daher wird sukzessive die Varianz der
im Folgenden definierten Hauptkomponenten maximiert. Was diese Hauptkomponenten sind und wie man
sie herleitet, wird in den folgenden zwei Unterabschnitten genauer betrachtet.

\subsubsection{Definition der Hauptkomponenten}
\label{ch:MethodenDerDimRed:statistisch:PCA:Definition}
Als Ausgangsbasis wird ein $D$-dimensionalen Zufallsvektor $\rvect{x} = \tr{(\rv{x}_1, \ldots, \rv{x}_D)}$ mit $\Exp[\rvect{x}] = \vect{0}$ angenommen und das Ziel ist es, eine $k$-dimensionale Repräsentation $\rvect{y} = \tr{(\rv{y_1},\ldots,\rv{y}_d)}$ zu erhalten. Wir drücken $\rvect{y}$ nun als eine Linearkombination des ursprünglichen hochdimensionalen Vektors $\rvect{x}$ wie folgt aus:
\begin{equation}
	\rv{y}_i = \tr{\vect{a}_i} \rvect{x} = a_{i1} \rv{x}_1 + \cdots + a_{iD} \rv{x}_D
	\quad \text{mit } i = 1,\ldots,d \,
\end{equation}
wobei $\vect{a}_i$ die \newterm{Koeffizienten} (auch: Ladungen) der $i$-ten Hauptkomponente sind \parencite[vgl.][2]{Jolliffe.2002}.

% Die Anzahl $k$ der Hauptkomponenten bestimmt den Grad der
% Dimensionsreduktion, kann jedoch beliebig zwischen $1 \leq k \leq D$ gewählt werden.\footnote{Der
% 	Grenzfall $k = D$ wird oft für eine Anonymisierung des Datensatzes mit personenbezogenen Daten
% 	verwendet. Beispielsweise kann so bei Finanztransaktionen der Datensatz mithilfe von PCA
% 	vorverarbeitet (\textit{Preprocessing}) und erst dann veröffentlicht werden.}

\subsubsection{Herleitung der Hauptkomponenten}
\label{ch:MethodenDerDimRed:statistisch:PCA:HerleitungPC}
Betrachtet wird nun zunächst lediglich die \textit{erste} Hauptkomponente $\tr{\vect{a}_1} \rvect{x}$. Wie in \subsecref{ch:MethodenDerDimRed:statistisch:PCA:Grundidee} erwähnt, soll möglichst viel Variation von der ursprünglichen Repräsentation $\rvect{x}$ erhalten bleiben. Daher wird die erste Hauptkomponente so gewählt, dass die Varianz maximal wird:
\begin{equation}
	\label{eq:PCA-Optimierungsproblem}
	\max_{\vect{a}_1}\, \Var \left( \tr{\vect{a}_1} \rvect{x} \right) = \max_{\vect{a}_1} \tr{\vect{a}_1} \mat{\Sigma} \vect{a}_1
\end{equation}
wobei $\mat{\Sigma}$ die Kovarianzmatrix von $\rvect{x}$ ist. Dieses Maximierungsproblem ist jedoch nach oben unbeschränkt, weshalb man zusätzlich die Bedingung
\begin{equation}
	\norm{ \vect{a}_1 }^2 = \tr{\vect{a}_1} \vect{a}_1 = 1
\end{equation}
zum nun \textit{restringierten} Maximierungsproblem hinzufügt, welches mithilfe des Lagrange-Ansatzes gelöst werden kann. Wie sich heraustellt, ist dieses Maximierungsproblem ein Eigenwertproblem und kann somit effizient gelöst werden \parencite[vgl.][4 -- 6]{Jolliffe.2002}\unsure{Vlt hier die Lagrange Gleichung aufstellen und selber
	lösen, um zu sehen, dass man bei einem Eigenwertproblem landet}. Man erhält die Koeffizienten der
ersten Hauptkomponente $\vect{a}_1$, welche ein Eigenvektor von $\mat{\Sigma}$ mit zugehörigem
Eigenwert
\begin{equation}
	\lambda_1 = \Var(\tr{\vect{a}_1} \rvect{x})
\end{equation}
sind. Man beachte, dass dies gleichzeitig die Zielfunktion unseres Optimierungsproblems (\eqref{eq:PCA-Optimierungsproblem}) ist. Das bedeutet, dass die Koeffizienten der ersten Hauptkomponente dem Eigenvektor mit dem größten zugehörigen Eigenwert entspricht.\unsure{das ist etwas verwirrend beschrieben}

Um nun die zweite Hauptkomponente zu erhalten, wird wieder die Varianz mit der
Einheitsvektor-Nebenbedingung $\norm{\vect{a}_2}$ = 1 maximiert. Zusätzlich muss jedoch gelten,
dass die zweite Hauptkomponente zur Ersten \textit{unkorreliert} ist \parencite[5]{Jolliffe.2002}, das heißt
\begin{equation}
	\Cov(\tr{\vect{a}_1}\rvect{x}, \tr{\vect{a}_2}\rvect{x}) \overset{!}{=} 0
\end{equation}
Auch dieses Optimierungsproblem ist ein Eigenwertproblem und man erhält, dass die Koeffizienten $\vect{a}_2$ der zweiten Hauptkomponente der Eigenvektor mit dem zweitgrößten Eigenwert von $\mat{\Sigma}$ ist.
Generell lässt sich zeigen, dass die Koeffizienten der $i$-ten Hauptkomponente dem Eigenvektor mit dem $i$-ten nach der Größe geordneten Eigenwert entspricht \parencite[6]{Jolliffe.2002}. Aus diesem Grund lassen sich die Hauptkomponenten kompakt durch
\begin{equation}
	\rvect{y} = \tr{\mat{A}} \rvect{x}
\end{equation}
ausdrücken, wobei $\mat{A} = [\vect{a}_1,\ldots,\vect{a}_k] \in \real^{D \times k}$ die Matrix der Eigenvektoren als Spaltenvektoren ist und wieder $1 \leq k \leq D$ gilt.

\subsubsection{Erweiterungen}
\label{ch:MethodenDerDimRed:statistisch:PCA:Erweiterungen}

Ein Schwachpunkt von PCA ist, dass die orthogonale Transformation auf die Eigenvektorbasis der
Kovarianzmatrix inhärent linear ist. Die Principal Component Analysis ist also in dieser
klassischen Form nicht in der Lage, eine nichtlineare Mannigfaltigkeit in den Daten zu erkennen.
Deswegen wurde unter anderem eine Generalisierung von PCA von \textcite{Scholkopf.1997} entwickelt,
die sogenannte \newterm{Kernel Principal Component Analysis} (Kernel PCA). Diese Methode wird in
\subsecref{ch:MethodenDerDimRed:statistisch:kPCA} genauer behandelt.

%% ==============================
\subsection{Kernel Principal Component Analysis}
\label{ch:MethodenDerDimRed:statistisch:kPCA}
\nomenclature[Z]{kPCA}{Kernel Principal Component Analysis}
Die Kernel Principal Components Analysis \parencite{Scholkopf.1997} ist eine nichtlineare Erweiterung der klassischen Principal Component
Analysis und kann somit theoretisch nichtlineare Zusammenhänge in den Daten erkennen. Um das zu
erreichen, wird sich am sogenannten \newterm{Kernel Trick} bedient, der schon erfolgreich auf
\textit{Support Vector Machines} \parencite{Boser.1992} angewandt wurde. Im Folgenden wird die Grundidee der Kernel PCA und das Konzept
eines Kernels genauer erläutert. Daraufhin wird die konkrete Vorgehensweise vorgestellt und
abschließend werden praktische Probleme besprochen.

\subsubsection{Grundidee}
\label{ch:MethodenDerDimRed:statistisch:kPCA:Grundidee}

Die Grundidee der Kernel PCA ist es, die Daten mithilfe einer Funktion $\bm{\phi}$ in einen
Vektorraum $\mathcal{H}$ abzubilden, der nichtlinear mit dem Ursprungsraum $\mathcal{X}$ verbunden
ist. Der Raum $\mathcal{H}$ wird in diesem Kontext auch Merkmalsraum (engl. \textit{feature space})
genannt und kann potentiell sehr hochdimensional sein. Nach der Transformation wird die
traditionelle Principal Component Analysis durchgeführt, um damit die niedrigdimensionale
Repräsentation $\rvect{y}$ zu erhalten. Dieses Vorgehen erscheint auf den ersten Blick
kontraintuitiv, da durch den \enquote{Umweg} über die Abbildung $\bm{\phi}$ die Dimension zuerst
erhöht wird. Motiviert wird es dadurch, dass die Daten in $\mathcal{X}$ einen
\textit{nichtlinearen} Zusammenhang aufweisen können, während in $\mathcal{H}$ der Zusammenhang
linear und damit für lineare Algorithmen wie die Principal Component Analysis erkennbar ist \parencite[vgl.][26]{ShaweTaylor.2011}.

Der entscheidende Punkt ist jedoch, dass man die Abbildung $\bm{\phi}$ nicht explizit berechnet,
sondern mithilfe eines sogenannten \newterm{Kernels} $\kappa: \mathcal{X} \times \mathcal{X}
	\rightarrow \real$ implizit gegeben hat \parencites[586 -- 588]{Bishop.2006}[583]{Scholkopf.1997}. Der Kernel Trick ist immer dann anwendbar,
wenn der Algorithmus so formuliert werden kann, dass $\bm{\phi}$ ausschließlich als Skalarprodukt
vorkommt. Im Folgenden wird die Vorgehensweise der Kernel PCA kurz erläutert. Allerdings wird für
eine eingehendere Behandlung des Kernel Tricks und anderer Kernel-Methoden auf
\textcite{ShaweTaylor.2011} verwiesen.

\subsubsection{Der Kernel}
\label{ch:MethodenDerDimRed:statistisch:kPCA:KernelFunktion}

Ein Kernel $\kappa$ ist definiert als das Skalarprodukt im Merkmalsraum \parencite[34]{ShaweTaylor.2011}
\begin{equation}
	\label{eq:kPCA:KernelFunction}
	\kappa(\vect{x}_i, \vect{x}_j) = \tr{\bm{\phi}(\vect{x}_i)} \bm{\phi}(\vect{x}_j)
\end{equation}
und ist damit intuitiv gesehen ein \enquote{Orakel} für die Ähnlichkeit der Daten im Merkmalsraum, weil dies ohne Kenntnis der Koordinaten berechnet werden kann \parencite[71]{ShaweTaylor.2011}. In der Praxis häufig anzutreffen sind Gaußsche Kernel (auch: Radiale
Basisfunktionen (RBF))
\begin{equation}
	\kappa(\vect{x}_i, \vect{x}_j) = \exp \left(- \frac{\norm{ \vect{x}_i - \vect{x}_j}^2}{2\sigma}\right) \, ,
\end{equation}
wobei der Parameter $\sigma > 0$ die Bandbreite des Kernels bestimmt \parencite[296]{ShaweTaylor.2011}. Ein zu kleiner Wert von $\sigma$ führt dazu, dass der Kernel die
Ähnlichkeit der Punkte unterschätzt. Analog führt ein zu großer Wert der Bandbreite dazu, dass der
Kernel die Ähnlichkeit überschätzt, d.h. die lokale Struktur wird unter Umständen vernachlässigt.
Des Weiteren sind polynomiale Funktionen \parencite[292]{ShaweTaylor.2011}
\begin{equation}
	\kappa(\vect{x}_i, \vect{x}_j) = (\vect{x}_i \cdot \vect{x}_j)^p
\end{equation}
des Grades $p$ eine von vielen möglichen Kernel-Funktionen.

Wie bereits in \subsecref{ch:MethodenDerDimRed:statistisch:kPCA:Grundidee} erwähnt, muss die
Principal Component Analysis nun in eine Form gebracht werden, in der nur Skalarprodukte von
$\bm{\phi}(\rvect{x})$ vorkommen. Diese Skalarprodukte können dann gemäß
\eqref{eq:kPCA:KernelFunction} durch die Kernel-Funktion ersetzt werden.

\subsubsection{Vorgehensweise}
\label{ch:MethodenDerDimRed:statistisch:kPCA:Vorgehensweise}

Betrachten wir also die Kovarianzmatrix $\mat{C}$ der in den Merkmalsraum projizierten Daten
$\bm{\phi}(\mat{X}) = \tr{\left[ \bm{\phi}(\vect{x}_1), \ldots, \bm{\phi}(\vect{x}_n) \right]}$. Um
die Berechnung zu vereinfachen, wird auch hier analog zur klassischen Principal Component Analysis
die Zentrierung der Daten im Merkmalsraum angenommen. Damit vereinfacht sich die Berechnung der
Kovarianzmatrix zu
\begin{equation}
	\mat{C} = \frac{1}{n} \tr{\bm{\phi}(\mat{X})} \bm{\phi}(\mat{X}) \, .
\end{equation}
Um nun die Principal Component Analysis durchführen zu können, müssten die Eigenvektoren dieser Matrix bestimmt werden. Dies ist jedoch nicht möglich, da die Abbildung $\bm{\phi}$ unbekannt ist. Wie sich herausstellt, ist diese Matrix aber ähnlich zur Kernel-Matrix $\mat{K}$, welche über den Kernel $\kappa$ als
\begin{align}
	\mat{K} & = \left( \kappa(\vect{x}_i, \vect{x}_j) \right)_{i,j = 1}^n \label{eq:kPCA:KernelMatrix_A} \\
	        & = \bm{\phi}(\mat{X}) \tr{\bm{\phi}(\mat{X})} \label{eq:kPCA:KernelMatrix_B}
\end{align}
definiert ist \parencite[68]{ShaweTaylor.2011}. Die Eigenwertszerlegung der zentrierten Kernel-Matrix liefert mit
einer entsprechenden Skalierung die Eigenvektoren der Kovarianzmatrix $\mat{C}$. Konkret wird dazu
die $n \times n$ Kernel-Matrix in einem ersten Schritt mittels
\begin{equation}
	\label{eq:KernelCentering}
	\widetilde{\mat{K}} = \mat{K} - \frac{1}{n} \ones \tr{\ones} \mat{K} - \frac{1}{n} \mat{K} \ones \tr{\ones} + \frac{1}{n^2} (\tr{\ones} \mat{K} \ones) \ones \tr{\ones}
\end{equation}
zentriert \parencite[131]{ShaweTaylor.2011}, wobei $\ones = \tr{(1, \ldots, 1)}$ der Vektor bestehend aus Einsen
ist. Diese Transformation der Kernel-Matrix stellt die Annahme der zentrierten Daten im
Merkmalsraum implizit sicher. Die zentrierte Kernel-Matrix $\widetilde{\mat{K}}$ wird dann im
zweiten Schritt in ihre Eigenvektoren $\vect{v_i}$ und Eigenwerte $\lambda_i$ mit $i = 1, \ldots,
	n$ zerlegt, womit über den Zusammenhang
\begin{equation}
	\label{eq:kPCA:ZusammenhangEigenvektoren}
	\vect{a}_i = \frac{1}{\sqrt{\lambda_i}} \vect{v}_i
\end{equation}
die Eigenvektoren $\vect{a}_i$ von $\mat{C}$ bestimmt werden können \parencite[142]{ShaweTaylor.2011}. Letztlich kann die Projektion eines Punktes $\vect{x}_i$ auf die
niedrigdimensionale Repräsentation $\rvect{y}_i$ durch
\begin{equation}
	\vect{y}_i = \Bigg( \sum_{j=1}^n \vect{a}_j^{(l)} \kappa(\vect{x}_i, \vect{x}_j) \Bigg)_{l=1}^d
\end{equation}
berechnet werden, wobei $\vect{a}_j^{(l)}$ der $l$-te Eintrag in $\vect{a}_j$ ist \parencite[150]{ShaweTaylor.2011}.

\subsubsection{Praktische Probleme der Kernel PCA}
\label{ch:MethodenDerDimRed:statistisch:kPCA:AuswahlKF}

Ein entscheidender Schwachpunkt der Kernel PCA ist die Selektion der Hyperparameter, d.h. die
Auswahl des Kernels und die Wahl der Parameter für den Kernel. Problematisch ist, dass der Kernel
oft sensitiv gegenüber seiner Parameter ist. Im Falle eines Gaußschen Kernels muss ein Parameter
$\sigma$ gewählt werden, der die Bandbreite des Kernels bestimmt. Die Performance auf einem
Datensatz hängt davon ab, wie passend diese Bandbreite gewählt wurde. Allgemein umgeht man das
Problem der manuellen Auswahl eines Kernels und seiner Parameter für Kernel-Methoden mittels einer
Gittersuche und kombiniert dies mit Kreuzvalidierung. Beispielsweise wird ein Qualitätskriterium
festgelegt und die Methode mit einer vorab definierten Auswahl an Werten der Hyperparameter auf
einer Teilmenge des Datensatzes trainiert und mit einer anderen Teilmenge evaluiert. Die Parameter
der Methode, die hinsichtlich des Qualitätskriteriums am besten abgeschnitten hat, werden dann für
das finale Modell übernommen und auf dem vollen Trainingsdatensatz trainiert. Dies funktioniert für
Klassifikatoren und Regressoren wie z.B. die Support Vector Machine gut, da die Wahl der
Qualitätskriterien nicht problematisch ist. Für die Dimensionsreduktion ist die Wahl des
Qualitätskriteriums nicht trivial, wie wir später in
\subsecref{ch:Vergleich:sec:Methodik:subsec:Qualitaetskriterien} sehen werden. Eine Möglichkeit
wäre die quadratische Abweichung zwischen der ursprünglichen Repräsentation eines Punktes und der
Rekonstruktion aus seiner niedrigdimensionalen Repräsentation. Die Kernel PCA stellt allerdings
entgegen der klassischen Principal Component Analysis keine exakte inverse Transformation zur
Verfügung, weswegen auf Approximationen zurückgegriffen werden muss. Dieses Problem entspricht dem
Finden des sogenannten Urbildes (engl. \textit{pre-image}) in Kernel-Methoden \parencite{Kwok.2004}. Mithilfe dieser Approximation könnte eine Kreuzvalidierung über
Rekonstruktionsfehler durchgeführt werden \parencite[siehe z.B.][]{Alam.2014}. In dieser Arbeit wird auf die Approximation des Urbildes
verzichtet und stattdessen Qualitätskriterien der Dimensionsreduktion für eine Gittersuche
verwendet. Das Vorgehen wird später in \secref{ch:Vergleich:sec:ParameterwahlTrainingsdetails}
genauer erläutert.

Ein weiterer Nachteil der Kernel Principal Component Analysis ist, dass die Größe der Kernel-Matrix
quadratisch in der Stichprobengröße $n$ ansteigt. Dadurch kann diese Matrix auf großen Datensätzen
möglicherweise nicht vollständig im Hauptspeicher zwischengespeichert werden. Die ohnehin teure
Operation der Eigenwertszerlegung einer Matrix wird dadurch zusätzlich zum Problem. Für große
Datensätze wurden daher inkrementelle Versionen der Kernel Principal Component Analysis entwickelt \parencite[siehe z.B.][]{Hallgren.2018}.

%% ==============================
\subsection{Locally Linear Embedding}
\label{ch:MethodenDerDimRed:statistisch:LLE}
\nomenclature[Z]{LLE}{Locally Linear Embedding}
Locally Linear Embedding (LLE), entwickelt von \textcite{Roweis.2000}, nutzt lokale Eigenschaften der Daten aus und bezieht damit gleichzeitig globale Eigenschaften mit ein.\rewrite{erklären wie es global eigenschaften miteinbezieht} Um dies zu erreichen, geht LLE in zwei Schritten vor: Zuerst wird die optimale lineare Rekonstruktion durch die Nachbarn eines Datenpunktes bestimmt. Im zweiten Schritt werden dann geometrische Eigenschaften dieser Rekonstruktion ausgenutzt, durch die die niedrigdimensionale Repräsentation abgeleitet werden kann.

\subsubsection{Lineare Rekonstruktion durch die Nachbarn}
\label{ch:MethodenDerDimRed:statistisch:LLE:LineareRekonstruktion}
LLE nimmt an, dass die ursprünglichen Daten auf einer $d$-dimensionalen glatten Mannigfaltigkeit im $\real^D$ eingebettet sind und dass diese Mannigfaltigkeit auf kleinen Stücken (engl. \textit{patches}) approximativ linear ist. Daher kann ein hochdimensionaler Vektor $\vect{x}_i$ mit $i = 1,\ldots,n$ durch eine Linearkombination seiner Nachbarn
ausgedrückt werden \parencite[2323]{Roweis.2000}. Die Koeffizienten dieser Linearkombination sind die sogenannten
Rekonstruktionsgewichte $W_{ij}$, welche durch Minimierung der Zielfunktion
\begin{equation}
	\label{eq:LLE:Zielfunktion}
	\sum_{i = 1}^n \norm[\Big]{ \vect{x}_i - \sum_{j = 1}^n W_{ij}\vect{x}_j}^2
\end{equation}
gefunden werden. Die Minimierung erfolgt unter den folgenden zwei Nebenbedingungen \parencite[2]{Roweis.2000}. Erstens müssen sich alle Zeilen der Gewichtsmatrix $\mat{W} \in \real^{n
		\times n}$ zu Eins summieren
\begin{equation}
	\label{eq:LLE:ErsteNebenbedingung}
	\sum_{j = 1}^nW_{ij} = 1 \, , \quad \forall i = 1, \ldots, n
\end{equation}
und zweitens sollen Datenpunkte, die keine Nachbarn sind, keinen Einfluss auf die Rekonstruktion haben, das heißt
\begin{equation}
	\label{eq:LLE:ZweiteNebenbedingung}
	W_{ij} = 0 \, , \quad \text{falls } \vect{x}_j \text{ kein Nachbar von } \vect{x}_i \text{ ist.}
\end{equation}
Die erste Nebenbedingung (\eqref{eq:LLE:ErsteNebenbedingung}) macht die Gewichtsmatrix invariant gegenüber Translationen: Werden alle Datenpunkte um den Wert $\alpha \in \real$ verschoben, so ist $W_{ij}$ wegen $\sum_j W_{ij}\alpha = \alpha$ immer noch eine optimale Lösung \parencite[8]{Cayton.2005}:
\begin{equation}
	\sum_{i = 1}^n \norm[\Big]{ \vect{x}_i + \alpha - \sum_{j = 1}^n W_{ij}(\vect{x}_j + \alpha)}^2 = \sum_{i = 1}^n \norm[\Big]{ \vect{x}_i - \sum_{j = 1}^n W_{ij}\vect{x}_j}^2 \, .
\end{equation}
Des Weiteren sind die Gewichtsmatrizen, die dieses Optimierungsproblem lösen, invariant gegenüber Rotationen und Skalierungen. Damit begründen sie das in \subsecref{ch:MethodenDerDimRed:statistisch:LLE:FindenDerRepr} erläuterte Finden der niedrigdimensionalen Repräsentation mithilfe derselben Rekonstruktionsgewichte.

% Das restringierte Optimierungsproblem kann mittels des Lagrange-Ansatzes gelöst werden, womit man auf eine geschlossene Form der Rekonstruktionsgewichte für einen Vektor $\vect{x}_i$ kommt \parencites[2325 -- 2326]{Roweis.2000}[3]{Ghojogh.2020}:
% \begin{equation}
% 	\vect{W}_i = \frac{\sum_k C_{jk}^{-1}}{\sum_{lm} C_{lm}^{-1}} \, ,
% \end{equation}
% wobei $\mat{C}$ die lokale Kovarianzmatrix ist. 
\subsubsection{Finden der niedrigdimensionalen Repräsentation}
\label{ch:MethodenDerDimRed:statistisch:LLE:FindenDerRepr}
Wurden die optimalen Rekonstruktionsgewichte $W_{ij}$, die die Zielfunktion aus
\eqref{eq:LLE:Zielfunktion} unter den genannten Nebenbedingungen minimieren, gefunden, so kann in einem zweiten Schritt die niedrigdimensionale Repräsentation $\vect{y}$ hergeleitet werden. Dazu werden die Eigenschaften der Gewichtsmatrix $\mat{W}$ ausgenutzt. Nimmt man wie eingangs erwähnt die Einbettung der Daten auf einer $d$-Mannigfaltigkeit an, dann gibt es eine Abbildung eines Punktes $\vect{x}$ auf die niedrigdimensionale Repräsentation $\vect{y}$. Diese Abbildung besteht aus einer Translation, Rotation und Skalierung -- genau die Eigenschaften gegenüber den die Gewichtsmatrix invariant ist. Man benutzt also dieselben Gewichte $W_{ij}$, die auch für die Rekonstruktion der ursprünglichen Repräsentation genutzt wurden, und löst mit fixem $W_{ij}$ das folgende Optimierungsproblem, um die latente Repräsentation zu finden \parencite[2324]{Roweis.2000}:
\begin{equation}
	\label{eq:LLE:ZielfunktionY}
	\min_{\mat{Y}}\, \sum_{i = 1}^n \norm[\Big]{\vect{y}_i - \sum_{j = 1}^n W_{ij}\vect{y}_j}^2 \, .
\end{equation}
\eqref{eq:LLE:ZielfunktionY} kann äquivalent umgeformt werden zu \parencite[4]{Ghojogh.2020}
\begin{equation}
	\min_{\mat{Y}}\, \Spur(\tr{\mat{Y}} \mat{M} \mat{Y}) \, ,
\end{equation}
wobei $\mat{M} = \tr{(\identity_n - \mat{W})} (\identity_n - \mat{W})$ und $\Spur(\,\cdot\,)$ die Summe der Diagonalelemente einer Matrix bezeichnet. Jedoch ist dieses Optimierungsproblem unterbestimmt, weswegen man für $\mat{Y}$ eine Einheits-Kovarianzmatrix und einen spaltenweisen Mittelwert von Null fordert \parencite[11]{Saul.2000}. Diese Restriktionen sind erlaubt, da die Zielfunktion in
\eqref{eq:LLE:ZielfunktionY} invariant gegenüber Rotationen und Skalierungen ist \parencite[2326]{Roweis.2000}.

Wie \textcite[3 -- 4]{Ghojogh.2020} zeigen, wird das Problem über die Eigenwertzerlegung der
dünnbesetzten Matrix $\mat{M}$ gelöst: Die Spalten von $\mat{Y}$ entsprechen den unteren $d$
Eigenvektoren von $\mat{M}$, wobei das triviale Eigenvektor-Eigenwert-Paar $(\ones, 0)$ ausgelassen
wird. Die ebenen genannten Restriktionen, die das Optimierungsproblem bis auf eine Rotation
eindeutig machen, lösen die Schwachstelle einer trivialen Lösung in dieser Zielfunktion jedoch
nicht vollständig. Es kann dazu kommen, dass fast alle niedrigdimensionalen Punkte $\vect{y}_i$ nah
um den Ursprung liegen und einige \enquote{Stränge} vom Ursprung aus weggehen, um die Restriktion
der Einheits-Kovarianzmatrix zu erfüllen. Die Restriktionen sind also nicht stark genug, um
triviale Lösungen vollständig zu umgehen \parencite[vgl.][23]{vanderMaaten.2009}.

%% ==============================
%% ==============================
\section{Machine Learning Methoden}
\label{ch:MethodenDerDimRed:modern}
%% ==============================
Nachdem in \secref{ch:MethodenDerDimRed:statistisch} einige statistischen Methoden der
Dimensionsreduktion näher betrachtet wurden, werden nun zwei ausgewählte Machine Learning Methoden,
nämlich der \newterm{Autoencoder} in seiner klassischen Form in
\subsecref{ch:MethodenDerDimRed:ML:AE} und eine Modifizierung in Form des \newterm{Contractive
	Autoencoder} (CAE) in \subsecref{ch:MethodenDerDimRed:ML:CAE} vorgestellt.

\subsection{Autoencoder}
\label{ch:MethodenDerDimRed:ML:AE}
\nomenclature[Z]{AE}{Autoencoder}

Der Autoencoder ist ein neuronales Netzwerk, das die Rekonstruktion des Inputvektors $\vect{x}$ aus
den Trainingsdaten lernt. Um das sinnvoll zu erreichen, besitzt der Autoencoder eine wie in
\figref{fig:SchematischerAutoencoder} gezeigte symmetrische Architektur.

\input{\figures/autoencoder-architecture.tex}

Dabei kann der Encoder und Decoder unterschiedliche Architekturen annehmen -- die eines neuronalen
Netzes ist allerdings die häufigste und auch hier verwendete Variante. Diese besteht in der
klassischen Form aus einer ungeraden Anzahl an vollvernetzten Schichten mit einer
\newterm{Bottleneck}-Schicht in der Mitte \parencite[2]{Bank.2020}. Diese Schicht wird als \textit{Bottleneck} bezeichnet, da sie nur aus $d < D$
Neuronen besteht und damit die \enquote{kleinste} Stelle im Netzwerk darstellt, durch die der
Inputvektor kodiert werden muss. In diesem Fall wird der Autoencoder als \newterm{unterbestimmt}
(engl. \textit{undercomplete}) bezeichnet \parencite[503]{Goodfellow.2016}.
%  Es gibt demnach auch \newterm{überbestimmte} Autoencoder mit $k > D$
% 	Neuronen in der mittleren Schicht. Diese Variante reduziert die Dimension nicht, sondern erhöht sie \parencite[504]{Goodfellow.2016}. 
Damit erzielt die Bottleneck-Schicht die gewünschte Dimensionsreduktion des Inputvektors \parencites[502]{Goodfellow.2016}[2]{Bank.2020}.\rewrite{umschreiben}

Die Nichtlinearität eines Autoencoders wird durch die Auswahl der Aktivierungsfunktionen bestimmt.
Werden ausschließlich lineare Funktionen verwendet, resultiert dies in einem linearen Autoencoder,
welcher ähnlich zur Principal Component Analysis ist. Dies wird in
\subsecref{ch:Vergleich:sec:Resultate:PCA_AE} noch genauer betrachtet. In den meisten Fällen werden
jedoch \textit{Sigmoid}-Aktivierungsfunktionen eingesetzt, um einen nichtlinearen Zusammenhang zu
lernen und damit die Flexibilität der Architektur auszunutzen \parencite[4]{Charte.2018}.

\subsubsection{Mathematische Formulierung}
\label{ch:MethodenDerDimRed:ML:AE:MathematischeFormulierung}
Mathematisch gesehen versucht der Autoencoder zwei Funktionen zu lernen. Zum einen den sogenannten \newterm{Encoder} $f: \real^D \rightarrow \real^d$, der den Inputvektor in die niedrigdimensionale Repräsentation $\rvect{y} = f(\rvect{x})$ codiert und zum anderen den \newterm{Decoder} $g: \real^d \rightarrow \real^D$, der diese Repräsentation wieder in den Inputvektor dekodiert. Ein einfacher dreischichtiger vollvernetzter Autoencoder mittels
\begin{gather}
	\label{eq:dreischichtigerAE}
	f(\vect{x}) = s(\mat{W}_1 \vect{x} + \vect{b}_1) = \vect{y} \\
	g(f(\vect{x})) = s(\mat{W}_2 \vect{y} + \vect{b}_2) = \estNormal{\vect{x}}
\end{gather}
formalisiert werden, wobei $s$ Aktivierungsfunktion bezeichnet, $\mat{W}_1 \in \real^{d \times D}$ und $\mat{W}_2 \in \real^{d \times D}$ die Gewichte und $\vect{b}_i$ mit $i = 1,2$ die Bias-Vektoren des Netzwerks sind. Dies entspricht je einer affinen Transformation gefolgt von einer Nichtlinearität. Die Aktivierungsfunktion wird später bei der Wahl der Architektur eines Autoencoders noch genauer erläutert.
Das Ziel des Autoencoders ist es, den Inputvektor zu rekonstruieren
\begin{equation}
	\estNormal{\rvect{x}} = g(f(\rvect{x})) \approx \rvect{x} \, ,
\end{equation}
wobei $\estNormal{\vect{x}}$ die Rekonstruktion des Autoencoders bezeichnet.
Dazu minimiert der Autoencoder einen \newterm{Rekonstruktionsfehler} $L(\vect{x}, \vect{\estNormal{x}})$, der eine Ungleichheit zwischen dem Input- und Outputvektor bestraft. Hier wird meistens die quadratische Abweichung der beiden Vektoren verwendet, das heißt
\begin{equation}
	\label{eq:MSE_loss}
	L(\vect{x}, \vect{\estNormal{x}}) = L(\vect{x}, g(f(\vect{x}))) = \norm[\big]{\vect{x} - g(f(\vect{x}))}^2
\end{equation}
\parencite[507]{Goodfellow.2016}. Die Zielfunktion $\mathcal{J}_{\text{AE}}$, die der Autoencoder
minimiert, ist dann beispielsweise der Mittelwert über die Fehlerfunktion in den Trainingsdaten:
\begin{equation}
	\label{eq:AE_objectiveFunction}
	\mathcal{J}_{\text{AE}} = \frac{1}{n} \sum_{i = 1}^n L(\vect{x}_i, \estNormal{\vect{x}}_i) \, .
\end{equation}
Die Gefahr dabei ist, dass der Autoencoder bei zu großer Kapazität einfach die Identitätsfunktion
ohne sinnvolle Repräsentation lernt. Dies ist zum Beispiel der Fall, wenn zu viele Schichten
eingesetzt werden. Aus diesem
Grund muss ein Autoencoder oftmals regularisiert werden, worauf im Folgenden in
\subsecref{ch:MethodenDerDimRed:ML:AE:Regularisierung} eingegangen wird.

\subsubsection{Regularisierung}
\label{ch:MethodenDerDimRed:ML:AE:Regularisierung}
Um eine sinnvolle Repräsentation $\rvect{y}$ zu erhalten, muss der Autoencoder oftmals
regularisiert, das heißt eingeschränkt, werden. Ein erstes Beispiel hierfür ist der in
\figref{fig:SchematischerAutoencoder} gezeigte unterbestimmte Autoencoder, mit dem eine $d$-dimensionale
Repräsentation in der Mitte forciert wird. Besitzt der Autoencoder jedoch eine zu große Kapazität, kann der Autoencoder trotz des Bottlenecks
eine uninformative niedrigdimensionale Repräsentation lernen. Neben dieser \textit{impliziten} Regularisierung, gibt es deswegen noch einige
weitere \textit{explizite} Regularisierungsmethoden, die zusätzlich zum Rekonstruktionsfehler einen
Bestrafungsterm $\Omega(\bm{\theta})$ zur Zielfunktion $\mathcal{J}$ hinzufügen. Hierbei bezeichnet $\bm{\theta}$ die Parameter im Netzwerk, d.h. die Gewichte und Biases (siehe \eqref{eq:dreischichtigerAE}). Die Zielfunktion lautet dann
\begin{equation}
	\mathcal{J}_{\text{AE+reg}} = \frac{1}{n} \sum_{i = 1}^{n}  L(\vect{x}_i, \estNormal{\vect{x}}_i) + \lambda \Omega(\bm{\theta}) \, ,
\end{equation}
wobei $\lambda \geq 0$ ein \textit{Hyperparameter} ist, der die Stärke der Regularisierung kontrolliert. Damit muss der Autoencoder zwischen zwei entgegengesetzten Kräften balancieren: Einerseits muss der Autoencoder lernen, den Inputvektor zu rekonstruieren um den Rekonstruktionsfehler gering zu halten. Andererseits darf der Bestrafungsterm nicht zu groß werden, was triviale Lösungen mit uninformativem Bottleneck ausschließen soll. Insgesamt wird sich dadurch also eine informativere niedrigdimensionale Repräsentation erhofft \parencite[516]{Goodfellow.2016}.

Eine Umsetzung dessen ist der \newterm{Sparse Autoencoder}, der eine dünnbesetzte (engl.
\textit{sparse}) Repräsentation $\vect{y}$ erzwingt, d.h. nur wenige Einträge von $\vect{y}$ sind
ungleich Null. Eine weitere oft eingesetzte Regularisierung für neuronale Netze ist das sogenannte
\newterm{weight decay}, das die Größe der Gewichte im Netzwerk bestraft. Der Regularisierungsterm
entspricht bspw. im Falle eines dreischichtigen Autoencoders (siehe \eqref{eq:dreischichtigerAE})
\begin{equation}
	\label{eq:WeightDecay}
	\Omega(\bm{\theta}) = \norm{\mat{W}_1}^2_F + \norm{\mat{W}_2}^2_F \, ,
\end{equation}
wobei $\norm{\,\cdot\,}_F$ die Frobeniusnorm bezeichnet \parencite[1]{Kunin.2019}. Diese Regularisierung wird später im Vergleich von Autoencodern zur
Principal Component Analysis in \subsecref{ch:Vergleich:sec:Resultate:PCA_AE} noch von Bedeutung
sein. Für das Lernen von Mannigfaltigkeiten besonders interessant ist jedoch der
\newterm{Contractive Autoencoder} \parencite{Rifai.2011}, der im \subsecref{ch:MethodenDerDimRed:ML:CAE} genauer behandelt wird.

\subsubsection{Wahl der Architektur}
\label{ch:MethodenDerDimRed:ML:AE:WahlArchitektur}

Die vielen Freiheitsgrade in der Wahl der Architektur machen die Autoencoder flexibel, sind
zugleich aber auch ein Nachteil dieser Methode. Im Folgenden werden die zu treffenden
Entscheidungen bei der Wahl der Architektur eines Autoencoders kurz erläutert.

Erstens muss zwischen vollvernetzten und faltenden (engl. \textit{Convolutional}) Schichten
entschieden werden. Letztere können eingesetzt werden, wenn der Input des Autoencoders eine
gitterartige Topologie, wie z.B. ein Bild aufweist \parencite[330]{Goodfellow.2016}.\footnote{In der Mathematik ist eine diskrete Faltung zweier
	Funktionen $v$ und $u$ definiert als
	\begin{equation*}
		(v * u)(t) = \sum_{\tau = -\infty}^{+\infty} v(t)u(t - \tau) \, .
	\end{equation*}
	Intuitiv gesehen wird dabei die Funktion $u$ (genannt: Kernel) gespiegelt und von links nach rechts über die Funktion $v$ \enquote{geschoben} und dabei multipliziert. Die Faltungsoperation in neuronalen Netzen kann in einer ähnlichen Weise interpretiert werden. Jedoch wird dabei das Spiegeln des Kernels teilweise weggelassen \parencite[333]{Goodfellow.2016}. Zudem ist der Kernel auf zweidimensionalen Gittern, wie es bspw. bei
	Bildern der Fall ist, meist ebenfalls zweidimensional.} Der Autoencoder wird dann als Convolutional
Autoencoder (ConvAE) bezeichnet. Eine detaillierte Einführung in Convolutional Neural Networks ist
in \textcite[330 -- 372]{Goodfellow.2016} zu finden. In den anderen Fällen können vollvernetzte
Schichten eingesetzt werden, d.h. jedes Neuron hat eine Verbindung zu jedem Neuron aus der
Folgeschicht. In Convolutional Neural Networks ist es üblich, dass nach einer faltenden Schicht
eine sogenannte \textit{Pooling}-Schicht \parencite[339 -- 345]{Goodfellow.2016} kommt, die die Größe des Inputs zwar verkleinert, dies aber
\textit{ohne} trainierbare Gewichte tut. In Autoencodern soll die Inputgröße im Encoder ebenfalls
sukzessive verkleinert werden, allerdings wollen wir lernen, wie man sie verkleinert. Daher wird
hier anstelle von Pooling-Schichten die Schrittgröße (engl. \textit{Stride}) in der faltenden
Schicht auf einen Wert größer Eins gesetzt. Dies bedeutet, dass bei der Faltungsoperation Pixel
ausgelassen werden und dadurch die Größe des Inputs verkleinert wird.

Zweitens muss die Anzahl der Schichten $m$ festgelegt werden, welche für die Symmetrie des Encoders
und Decoders ungerade sein sollte (aber nicht muss). Häufig wählt man hier $m = 3$ oder $m = 5$, da
bei größerem $m$ Schwierigkeiten bei der Konvergenz des Autoencoders auftreten. Dies liegt an der
hohen Anzahl an Verbindungen (Gewichte) im neuronalen Netz, da jedes Neuron mit allen Neuronen aus
der nachfolgenden Schicht verbunden ist. Bei hochdimensionalen Datensätzen besitzt die
Input-Schicht zudem viele Neuronen. Dies führt dazu, dass bereits simple dreischichtige Autoencoder
viele Gewichte haben, wie das folgende Beispiel zeigt. Beträgt die extrinsische Dimension bspw.
1000 und wählt man $m=3$ und hat die Bottleneck-Schicht zehn Neuronen, dann hat dieser simple
Autoencoder bereits 20 000 trainierbare Gewichte.\footnote{Genau genommen hat der Encoder $1000
		\cdot 10 = 10000$ Gewichte und zehn Biases. Der Decoder hat 10000 Gewichte und 1000 Biases. Der
	Autoencoder hat also insgesamt 20 110 zu trainierende Freiheitsgrade.}

Drittens muss die Anzahl der Neuronen in den versteckten Schichten festgelegt werden. Die Anzahl
der Neuronen in der ersten und letzten Schicht entspricht der Größe des Inputvektors, d.h. der
extrinsischen Dimension $D$. Erhöht man die Anzahl der Neuronen in einer versteckten Schicht, so
erhöht sich die Kapazität des Autoencoders. Daher muss dieser Freiheitsgrad mit Bedacht gewählt
werden, da es sonst ebenfalls zu Schwierigkeiten in der Optimierung kommen kann.

Viertens muss die Aktivierungsfunktion festgelegt werden. Dabei könnte theoretisch nach jeder
Schicht eine andere Aktivierungsfunktion eingesetzt werden. Hier wird jedoch angenommen, dass
dieselbe Funktion für alle Schichten verwendet wird. Wie bereits am Anfang dieses Unterabschnitts
erwähnt wurde, bestimmt die Aktivierungsfunktion die Nichtlinearität des Autoencoders. Die wohl
populärsten Aktivierungsfunktionen sind die Sigmoid-Funktion
\begin{equation}
	\label{eq:Sigmoid}
	\operatorname{sigmoid}(x) = \frac{1}{1 + \exp (-x)} \, ,
\end{equation}
der \textit{Tangens Hyperbolicus}
\begin{equation}
	\label{eq:tanh}
	\operatorname{tanh}(x) = \frac{\exp(x) - \exp(-x)}{\exp(x) + \exp(-x)}
\end{equation}
und die \textit{Rectified Linear Units}
\begin{equation}
	\label{eq:ReLU}
	\operatorname{ReLU}(x) = \max\{0, x\} \, ,
\end{equation}
wobei $\exp(\, \cdot \,)$ die Exponentialfunktion darstellt \parencites[191 -- 195]{Goodfellow.2016}[4]{Charte.2018}.
\begin{figure}[h]
	\centering
	\includegraphics{activations.pdf}
	\caption[Drei weitverbreitete Aktivierungsfunktionen.]{Drei weitverbreitete Aktivierungsfunktionen. \captiona Die Sigmoid-Funktion \captionb der Tangens Hyperbolicus und \captionc die Rectified Linear Units. Eigene Darstellung (angelehnt an \textcite[4]{Charte.2018})}
	\label{fig:activations}
\end{figure}
Die Aktivierungsfunktionen sind grafisch in \figref{fig:activations} dargestellt.
Obwohl ReLU (\eqref{eq:ReLU}) die standardmäßig empfohlene Aktivierungsfunktion ist \parencite[195]{Goodfellow.2016}, wird für Autoencoder die Sigmoid-Funktion (\eqref{eq:Sigmoid})
empfohlen. Dies liegt daran, dass die \textit{Rectified Linear Units} alle negativen Inputs auf
Null projiziert und damit die Rekonstruktion erschweren könnte \parencite[4]{Charte.2018}.
% \subsubsection{Verhältnis zur Principal Component Analysis}
% \label{ch:MethodenDerDimRed:ML:AE:VerhaeltnisPCA}

%Um den Zusammenhang zwischen der PCA und dem Autoencoder zu verdeutlichen,
% wird im Folgenden ein einfacher linearer Autoencoder mit drei Schichten betrachtet. Der Encoder
% kann somit als
% \begin{equation}
% 	f(\vect{x}) = \mat{W}_1 \vect{x} + \vect{b}_1 = \vect{y}
% \end{equation}
% und der Decoder als
% \begin{equation}
% 	g(\vect{y}) = \mat{W}_2 \vect{y} + \vect{b}_2 = \estNormal{\vect{x}}
% \end{equation}
% formuliert werden, wobei $\mat{W}_1 \in \real^{d \times D}$ und $\mat{W}_2 \in \real^{D \times d}$ die Gewichtsmatrizen und $\vect{b}_i$ die Bias-Vektoren des Autoencoders sind ($i = 1, 2$). Liegt eine Stichprobe in der Datenmatrix $\mat{X} \in \real^{n \times D}$ vor und minimiert der Autoencoder den mittleren quadratischen Fehler (\eqref{eq:MSE_loss}), dann löst dies\rewrite{das ganze hier bringt mir irgendwie nichts}
% \begin{equation}
% 	\min \norm{ \mat{X} - \tr{ \left( \mat{W}_2( \mat{W}_1 \tr{\mat{X}} + \vect{b}_1 \tr{\ones_n}) + \vect{b}_2 \tr{\ones_n}  \right) } }^2_F
% \end{equation}\rewrite{oben die Gewichtsmatrizen transponieren, dann muss ich nicht die Daten transponieren}
% wobei $\ones_D = \tr{(1, \ldots, 1)} \in \real^D$ der Vektor mit Einsen und $\norm{\,\cdot\,}_F$ die Frobenius-Norm ist \parencite[3]{Plaut.2018}.
\subsection{Contractive Autoencoder}
\label{ch:MethodenDerDimRed:ML:CAE}
\nomenclature[Z]{CAE}{Contractive Autoencoder}

Der Contractive Autoencoder \parencite{Rifai.2011} stellt eine Variante des eben vorgestellten klassischen Autoencoders
(\subsecref{ch:MethodenDerDimRed:ML:AE}) dar. Ähnlich wie der Sparse Autoencoder wird die
Zielfunktion des Autoencoders lediglich um einen zusätzlichen Regularisierungsterm erweitert.
Dieser zusätzliche Term in der Zielfunktion führt zu einer kontrahierenden Eigenschaft: Ähnliche
Punkte werden auf eine kleinere Nachbarschaft auf der Mannigfaltigkeit abgebildet und damit
\enquote{zusammengezogen}. Dies gilt aber nur \textit{lokal}, das heißt unähnliche Punkte können
immer noch weit weg auf der Mannigfaltigkeit liegen \parencite[521]{Goodfellow.2016}. Dazu wird wieder in einem ersten Schritt die Grundidee erläutert und
im zweiten Schritt die konkrete Berechnung des Regularisierungsterms vorgestellt.

\subsubsection{Grundidee}
\label{ch:MethodenDerDimRed:CAE:Grundidee}
Die Idee ist
es, die \textit{Sensitivität} des Autoencoders für den Input zu bestrafen. Das bedeutet, dass bei
kleinen Änderungen des Inputs $\vect{x}$ die Kodierung $f(\vect{x})$ ebenfalls nur minimal geändert
werden soll. Dies entspricht einer kleinen ersten Ableitung und kann über die (quadrierte)
Frobeniusnorm der Jacobi-Matrix $\mat{J}$ des Encoders $f$ erzielt werden \parencites[2]{Rifai.2011}[521]{Goodfellow.2016}. Der Regularisierungsterm lautet damit
\begin{equation}
	\label{eq:CAE-Regularisierung}
	\Omega(\vect{y}) = \norm[\big]{\mat{J}_f(\vect{x})}_F^2 =  \norm[\Big]{ \frac{\partial f(\vect{x})}{\partial \vect{x}} }^2_F \, ,
\end{equation}
was der Summe über die quadrierten Einträge der Jacobi-Matrix entspricht. Intuitiv gesehen werden so lokale Nachbarschaften auf \textit{kleinere} Nachbarschaften in der niedrigdimensionalen Repräsentation abgebildet, das heißt der Autoencoder wirkt kontrahierend.
Der Regularisierungsterm ist, wie wir in \subsecref{ch:MethodenDerDimRed:ML:CAE:BerechnungRegTerm} sehen werden, für einen dreischichtigen Autoencoder leicht zu berechnen, nicht aber für einen tiefen Autoencoder. Dies stellt ein praktisches Problem dar, welches allerdings über ein schichtenweises Vortrainieren teilweise umgangen werden werden kann \parencite[vgl.][522]{Goodfellow.2016}.

\subsubsection{Berechnung des Regularisierungsterms}
\label{ch:MethodenDerDimRed:ML:CAE:BerechnungRegTerm}
Der Regularisierungsterm ist wie eben erwähnt für einen Autoencoder mit nur einer versteckten Schicht, d.h. für einen dreischichtigen Autoencoder, leicht zu berechnen. Bei der mathematischen Formulierung der Autoencoder wurde bereits in \eqref{eq:dreischichtigerAE} die explizite Form des Encoders und Decoders eingeführt. Nimmt man nun an, dass die Gewichte verbunden sind, d.h. $\mat{W}_2 = \tr{\mat{W}_1}$ und wird für die Aktivierungsfunkion $s$ die Sigmoid-Funktion (\eqref{eq:Sigmoid})
verwendet, so kann die Frobeniusnorm der Jacobi-Matrix mittels
\begin{equation}
	\label{eq:CAE-loss-simple}
	\norm[\big]{\mat{J}_f(\vect{x})}_F^2 = \sum_{i = 1}^d \left( y_i (1 - y_i)\right)^2 \sum_{j = 1}^D (\mat{W}_1^2)_{ij}
\end{equation}
effizient berechnet werden \parencite[4]{Rifai.2011}. Für einen Autoencoder mit mehr als einer versteckten Schicht wird die
Berechnung teuer, was das Trainieren eines tiefen Contractive Autoencoders ohne Approximationen wie
beispielsweise das von \textcite{Bengio.2006} erläuterte gierige schichtenweise Vortrainieren
(engl. \textit{greedy layer-wise pretraining}) unpraktikabel macht. Bei diesem Vorgehen wird ein
tiefer Autoencoder schichtenweise durch untiefe Autoencoder (drei Schichten) vortrainiert und im
letzten Schritt erfolgt eine Feinabstimmung des kompletten Netzwerks \parencite[522]{Goodfellow.2016}. Dieses Vorgehen kann analog auch für klassische Autoencoder verwendet
werden, um eine gute Initialisierung der Gewichte zu erreichen und damit suboptimale lokale Minima
der Zielfunktion zu umgehen \parencite[509]{Goodfellow.2016}.